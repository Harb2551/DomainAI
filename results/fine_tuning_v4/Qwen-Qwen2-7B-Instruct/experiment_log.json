{
  "experiment_id": ".-local_models-Qwen-Qwen2-7B-Instruct_20250712_144441",
  "model_name": "./local_models/Qwen-Qwen2-7B-Instruct",
  "timestamp": "2025-07-12T14:44:41.067205",
  "config": {
    "lora_config": {
      "r": 16,
      "lora_alpha": 32,
      "target_modules": "{'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj'}",
      "lora_dropout": 0.1,
      "bias": "none",
      "task_type": "CAUSAL_LM"
    },
    "training_args": {
      "per_device_train_batch_size": 8,
      "per_device_eval_batch_size": 8,
      "gradient_accumulation_steps": 4,
      "effective_batch_size": 32,
      "num_train_epochs": 3,
      "learning_rate": 0.0002,
      "fp16": true,
      "gradient_checkpointing": false,
      "max_grad_norm": 1.0,
      "optimizer": "adamw_torch",
      "warmup_steps": 10,
      "eval_strategy": "epoch",
      "save_strategy": "epoch",
      "logging_steps": 1
    },
    "estimated_total_steps": 396
  },
  "metrics": {
    "training_successful": true,
    "final_train_loss": 1.559492009922974,
    "train_runtime_seconds": 640.2172,
    "train_runtime_minutes": 10.670286666666668,
    "train_samples_per_second": 19.868,
    "train_steps_per_second": 0.623,
    "epoch": 3.0,
    "global_step": 399,
    "total_flos": 1.38904670306304e+17,
    "training_progression": {
      "initial_train_loss": 5.0329,
      "final_train_loss": 1.2232,
      "loss_improvement": 3.8096999999999994,
      "best_eval_loss": 1.6115559339523315,
      "final_eval_loss": 1.6418570280075073,
      "peak_learning_rate": 0.0002,
      "final_learning_rate": 5.141388174807198e-07
    },
    "final_gpu_memory": {
      "allocated_gb": 14.697765827178955,
      "reserved_gb": 34.951171875,
      "max_allocated_gb": 34.05824327468872
    }
  },
  "system_info": {
    "gpu_available": true,
    "gpu_count": 1,
    "gpu_info": [],
    "cuda_version": "12.6",
    "torch_version": "2.7.1+cu126",
    "python_version": "3.9.21",
    "cpu_count": 256,
    "memory_total_gb": 1006.93,
    "memory_available_gb": 984.63
  },
  "training_history": [
    {
      "loss": 5.0329,
      "grad_norm": 3.860342502593994,
      "learning_rate": 0.0,
      "epoch": 0.007547169811320755,
      "step": 1
    },
    {
      "loss": 4.9391,
      "grad_norm": 3.738835334777832,
      "learning_rate": 2e-05,
      "epoch": 0.01509433962264151,
      "step": 2
    },
    {
      "loss": 4.9332,
      "grad_norm": 3.643690824508667,
      "learning_rate": 4e-05,
      "epoch": 0.022641509433962263,
      "step": 3
    },
    {
      "loss": 4.6225,
      "grad_norm": 2.944845199584961,
      "learning_rate": 6e-05,
      "epoch": 0.03018867924528302,
      "step": 4
    },
    {
      "loss": 4.3827,
      "grad_norm": 2.462632894515991,
      "learning_rate": 8e-05,
      "epoch": 0.03773584905660377,
      "step": 5
    },
    {
      "loss": 4.1209,
      "grad_norm": 2.0048253536224365,
      "learning_rate": 0.0001,
      "epoch": 0.045283018867924525,
      "step": 6
    },
    {
      "loss": 3.7031,
      "grad_norm": 2.1053595542907715,
      "learning_rate": 0.00012,
      "epoch": 0.052830188679245285,
      "step": 7
    },
    {
      "loss": 3.5036,
      "grad_norm": 2.293117046356201,
      "learning_rate": 0.00014,
      "epoch": 0.06037735849056604,
      "step": 8
    },
    {
      "loss": 3.2177,
      "grad_norm": 2.3620035648345947,
      "learning_rate": 0.00016,
      "epoch": 0.06792452830188679,
      "step": 9
    },
    {
      "loss": 2.9595,
      "grad_norm": 1.6721397638320923,
      "learning_rate": 0.00018,
      "epoch": 0.07547169811320754,
      "step": 10
    },
    {
      "loss": 2.8052,
      "grad_norm": 1.7787712812423706,
      "learning_rate": 0.0002,
      "epoch": 0.0830188679245283,
      "step": 11
    },
    {
      "loss": 2.7228,
      "grad_norm": 1.4407950639724731,
      "learning_rate": 0.0001994858611825193,
      "epoch": 0.09056603773584905,
      "step": 12
    },
    {
      "loss": 2.5974,
      "grad_norm": 1.4683599472045898,
      "learning_rate": 0.00019897172236503857,
      "epoch": 0.09811320754716982,
      "step": 13
    },
    {
      "loss": 2.5611,
      "grad_norm": 1.3307054042816162,
      "learning_rate": 0.00019845758354755785,
      "epoch": 0.10566037735849057,
      "step": 14
    },
    {
      "loss": 2.4978,
      "grad_norm": 1.1658051013946533,
      "learning_rate": 0.00019794344473007713,
      "epoch": 0.11320754716981132,
      "step": 15
    },
    {
      "loss": 2.506,
      "grad_norm": 7.835478782653809,
      "learning_rate": 0.0001974293059125964,
      "epoch": 0.12075471698113208,
      "step": 16
    },
    {
      "loss": 2.3705,
      "grad_norm": 1.1599923372268677,
      "learning_rate": 0.0001969151670951157,
      "epoch": 0.12830188679245283,
      "step": 17
    },
    {
      "loss": 2.3379,
      "grad_norm": 1.1726197004318237,
      "learning_rate": 0.00019640102827763497,
      "epoch": 0.13584905660377358,
      "step": 18
    },
    {
      "loss": 2.3522,
      "grad_norm": 1.112272024154663,
      "learning_rate": 0.00019588688946015425,
      "epoch": 0.14339622641509434,
      "step": 19
    },
    {
      "loss": 2.3404,
      "grad_norm": 1.1767780780792236,
      "learning_rate": 0.00019537275064267353,
      "epoch": 0.1509433962264151,
      "step": 20
    },
    {
      "loss": 2.2815,
      "grad_norm": 1.065456748008728,
      "learning_rate": 0.00019485861182519281,
      "epoch": 0.15849056603773584,
      "step": 21
    },
    {
      "loss": 2.3141,
      "grad_norm": 1.055454969406128,
      "learning_rate": 0.0001943444730077121,
      "epoch": 0.1660377358490566,
      "step": 22
    },
    {
      "loss": 2.2333,
      "grad_norm": 0.9802432656288147,
      "learning_rate": 0.00019383033419023138,
      "epoch": 0.17358490566037735,
      "step": 23
    },
    {
      "loss": 2.2134,
      "grad_norm": 1.0827741622924805,
      "learning_rate": 0.00019331619537275066,
      "epoch": 0.1811320754716981,
      "step": 24
    },
    {
      "loss": 2.2026,
      "grad_norm": 1.100482702255249,
      "learning_rate": 0.00019280205655526994,
      "epoch": 0.18867924528301888,
      "step": 25
    },
    {
      "loss": 2.1468,
      "grad_norm": 1.0810513496398926,
      "learning_rate": 0.0001922879177377892,
      "epoch": 0.19622641509433963,
      "step": 26
    },
    {
      "loss": 2.1824,
      "grad_norm": 1.0592222213745117,
      "learning_rate": 0.0001917737789203085,
      "epoch": 0.2037735849056604,
      "step": 27
    },
    {
      "loss": 2.1905,
      "grad_norm": 1.005165457725525,
      "learning_rate": 0.00019125964010282778,
      "epoch": 0.21132075471698114,
      "step": 28
    },
    {
      "loss": 2.132,
      "grad_norm": 0.9683828949928284,
      "learning_rate": 0.00019074550128534706,
      "epoch": 0.2188679245283019,
      "step": 29
    },
    {
      "loss": 2.1756,
      "grad_norm": 0.9882816076278687,
      "learning_rate": 0.00019023136246786634,
      "epoch": 0.22641509433962265,
      "step": 30
    },
    {
      "loss": 2.0566,
      "grad_norm": 1.0772621631622314,
      "learning_rate": 0.00018971722365038562,
      "epoch": 0.2339622641509434,
      "step": 31
    },
    {
      "loss": 2.0817,
      "grad_norm": 1.1454118490219116,
      "learning_rate": 0.0001892030848329049,
      "epoch": 0.24150943396226415,
      "step": 32
    },
    {
      "loss": 2.0986,
      "grad_norm": 1.0740714073181152,
      "learning_rate": 0.00018868894601542418,
      "epoch": 0.2490566037735849,
      "step": 33
    },
    {
      "loss": 2.0646,
      "grad_norm": 1.0904600620269775,
      "learning_rate": 0.00018817480719794346,
      "epoch": 0.25660377358490566,
      "step": 34
    },
    {
      "loss": 2.043,
      "grad_norm": 1.0998578071594238,
      "learning_rate": 0.0001876606683804627,
      "epoch": 0.2641509433962264,
      "step": 35
    },
    {
      "loss": 2.0644,
      "grad_norm": 1.0544381141662598,
      "learning_rate": 0.00018714652956298202,
      "epoch": 0.27169811320754716,
      "step": 36
    },
    {
      "loss": 2.0242,
      "grad_norm": 1.086978793144226,
      "learning_rate": 0.0001866323907455013,
      "epoch": 0.2792452830188679,
      "step": 37
    },
    {
      "loss": 2.0005,
      "grad_norm": 1.1246404647827148,
      "learning_rate": 0.00018611825192802058,
      "epoch": 0.28679245283018867,
      "step": 38
    },
    {
      "loss": 2.0089,
      "grad_norm": 1.1378132104873657,
      "learning_rate": 0.00018560411311053984,
      "epoch": 0.2943396226415094,
      "step": 39
    },
    {
      "loss": 1.9568,
      "grad_norm": 1.1735295057296753,
      "learning_rate": 0.00018508997429305914,
      "epoch": 0.3018867924528302,
      "step": 40
    },
    {
      "loss": 1.9431,
      "grad_norm": 1.1704128980636597,
      "learning_rate": 0.00018457583547557842,
      "epoch": 0.30943396226415093,
      "step": 41
    },
    {
      "loss": 2.1816,
      "grad_norm": 1.6406382322311401,
      "learning_rate": 0.0001840616966580977,
      "epoch": 0.3169811320754717,
      "step": 42
    },
    {
      "loss": 1.9309,
      "grad_norm": 1.3083394765853882,
      "learning_rate": 0.00018354755784061696,
      "epoch": 0.32452830188679244,
      "step": 43
    },
    {
      "loss": 1.9459,
      "grad_norm": 1.3545260429382324,
      "learning_rate": 0.00018303341902313626,
      "epoch": 0.3320754716981132,
      "step": 44
    },
    {
      "loss": 1.9506,
      "grad_norm": 1.216874122619629,
      "learning_rate": 0.00018251928020565555,
      "epoch": 0.33962264150943394,
      "step": 45
    },
    {
      "loss": 1.9141,
      "grad_norm": 1.241613745689392,
      "learning_rate": 0.00018200514138817483,
      "epoch": 0.3471698113207547,
      "step": 46
    },
    {
      "loss": 1.9711,
      "grad_norm": 1.212300419807434,
      "learning_rate": 0.00018149100257069408,
      "epoch": 0.35471698113207545,
      "step": 47
    },
    {
      "loss": 1.9563,
      "grad_norm": 1.226071834564209,
      "learning_rate": 0.00018097686375321336,
      "epoch": 0.3622641509433962,
      "step": 48
    },
    {
      "loss": 1.8167,
      "grad_norm": 1.1912261247634888,
      "learning_rate": 0.00018046272493573267,
      "epoch": 0.36981132075471695,
      "step": 49
    },
    {
      "loss": 1.8467,
      "grad_norm": 1.2229106426239014,
      "learning_rate": 0.00017994858611825195,
      "epoch": 0.37735849056603776,
      "step": 50
    },
    {
      "loss": 1.8562,
      "grad_norm": 1.2194249629974365,
      "learning_rate": 0.0001794344473007712,
      "epoch": 0.3849056603773585,
      "step": 51
    },
    {
      "loss": 1.912,
      "grad_norm": 1.3119806051254272,
      "learning_rate": 0.00017892030848329048,
      "epoch": 0.39245283018867927,
      "step": 52
    },
    {
      "loss": 1.93,
      "grad_norm": 1.2134848833084106,
      "learning_rate": 0.0001784061696658098,
      "epoch": 0.4,
      "step": 53
    },
    {
      "loss": 1.8123,
      "grad_norm": 1.2746127843856812,
      "learning_rate": 0.00017789203084832907,
      "epoch": 0.4075471698113208,
      "step": 54
    },
    {
      "loss": 1.8783,
      "grad_norm": 1.3731259107589722,
      "learning_rate": 0.00017737789203084832,
      "epoch": 0.41509433962264153,
      "step": 55
    },
    {
      "loss": 1.8828,
      "grad_norm": 1.3207807540893555,
      "learning_rate": 0.0001768637532133676,
      "epoch": 0.4226415094339623,
      "step": 56
    },
    {
      "loss": 1.77,
      "grad_norm": 1.2730411291122437,
      "learning_rate": 0.0001763496143958869,
      "epoch": 0.43018867924528303,
      "step": 57
    },
    {
      "loss": 1.9067,
      "grad_norm": 1.216477870941162,
      "learning_rate": 0.0001758354755784062,
      "epoch": 0.4377358490566038,
      "step": 58
    },
    {
      "loss": 1.8469,
      "grad_norm": 1.2094615697860718,
      "learning_rate": 0.00017532133676092547,
      "epoch": 0.44528301886792454,
      "step": 59
    },
    {
      "loss": 1.8833,
      "grad_norm": 1.1970871686935425,
      "learning_rate": 0.00017480719794344473,
      "epoch": 0.4528301886792453,
      "step": 60
    },
    {
      "loss": 1.8384,
      "grad_norm": 1.1633546352386475,
      "learning_rate": 0.000174293059125964,
      "epoch": 0.46037735849056605,
      "step": 61
    },
    {
      "loss": 1.9383,
      "grad_norm": 1.2098664045333862,
      "learning_rate": 0.0001737789203084833,
      "epoch": 0.4679245283018868,
      "step": 62
    },
    {
      "loss": 1.8546,
      "grad_norm": 1.2029192447662354,
      "learning_rate": 0.0001732647814910026,
      "epoch": 0.47547169811320755,
      "step": 63
    },
    {
      "loss": 1.7808,
      "grad_norm": 1.1017385721206665,
      "learning_rate": 0.00017275064267352185,
      "epoch": 0.4830188679245283,
      "step": 64
    },
    {
      "loss": 1.8601,
      "grad_norm": 1.159525752067566,
      "learning_rate": 0.00017223650385604113,
      "epoch": 0.49056603773584906,
      "step": 65
    },
    {
      "loss": 1.8997,
      "grad_norm": 1.1180540323257446,
      "learning_rate": 0.00017172236503856043,
      "epoch": 0.4981132075471698,
      "step": 66
    },
    {
      "loss": 1.8211,
      "grad_norm": 1.2738896608352661,
      "learning_rate": 0.00017120822622107972,
      "epoch": 0.5056603773584906,
      "step": 67
    },
    {
      "loss": 1.8401,
      "grad_norm": 1.14117431640625,
      "learning_rate": 0.00017069408740359897,
      "epoch": 0.5132075471698113,
      "step": 68
    },
    {
      "loss": 1.7387,
      "grad_norm": 1.2149277925491333,
      "learning_rate": 0.00017017994858611825,
      "epoch": 0.5207547169811321,
      "step": 69
    },
    {
      "loss": 1.8159,
      "grad_norm": 1.2462879419326782,
      "learning_rate": 0.00016966580976863753,
      "epoch": 0.5283018867924528,
      "step": 70
    },
    {
      "loss": 1.8698,
      "grad_norm": 1.389411449432373,
      "learning_rate": 0.00016915167095115684,
      "epoch": 0.5358490566037736,
      "step": 71
    },
    {
      "loss": 1.8344,
      "grad_norm": 1.3221955299377441,
      "learning_rate": 0.0001686375321336761,
      "epoch": 0.5433962264150943,
      "step": 72
    },
    {
      "loss": 1.8346,
      "grad_norm": 1.2062667608261108,
      "learning_rate": 0.00016812339331619537,
      "epoch": 0.5509433962264151,
      "step": 73
    },
    {
      "loss": 1.7308,
      "grad_norm": 1.1752957105636597,
      "learning_rate": 0.00016760925449871465,
      "epoch": 0.5584905660377358,
      "step": 74
    },
    {
      "loss": 1.6873,
      "grad_norm": 1.2332817316055298,
      "learning_rate": 0.00016709511568123396,
      "epoch": 0.5660377358490566,
      "step": 75
    },
    {
      "loss": 1.7865,
      "grad_norm": 1.1439805030822754,
      "learning_rate": 0.0001665809768637532,
      "epoch": 0.5735849056603773,
      "step": 76
    },
    {
      "loss": 1.7561,
      "grad_norm": 1.207938551902771,
      "learning_rate": 0.0001660668380462725,
      "epoch": 0.5811320754716981,
      "step": 77
    },
    {
      "loss": 1.7096,
      "grad_norm": 1.3074496984481812,
      "learning_rate": 0.00016555269922879177,
      "epoch": 0.5886792452830188,
      "step": 78
    },
    {
      "loss": 1.782,
      "grad_norm": 1.278027892112732,
      "learning_rate": 0.00016503856041131108,
      "epoch": 0.5962264150943396,
      "step": 79
    },
    {
      "loss": 1.7547,
      "grad_norm": 1.2191702127456665,
      "learning_rate": 0.00016452442159383033,
      "epoch": 0.6037735849056604,
      "step": 80
    },
    {
      "loss": 1.7774,
      "grad_norm": 1.255632758140564,
      "learning_rate": 0.00016401028277634961,
      "epoch": 0.6113207547169811,
      "step": 81
    },
    {
      "loss": 1.787,
      "grad_norm": 1.2352951765060425,
      "learning_rate": 0.0001634961439588689,
      "epoch": 0.6188679245283019,
      "step": 82
    },
    {
      "loss": 1.6972,
      "grad_norm": 1.1567593812942505,
      "learning_rate": 0.00016298200514138818,
      "epoch": 0.6264150943396226,
      "step": 83
    },
    {
      "loss": 1.6706,
      "grad_norm": 1.1801073551177979,
      "learning_rate": 0.00016246786632390746,
      "epoch": 0.6339622641509434,
      "step": 84
    },
    {
      "loss": 1.7708,
      "grad_norm": 1.1823405027389526,
      "learning_rate": 0.00016195372750642674,
      "epoch": 0.6415094339622641,
      "step": 85
    },
    {
      "loss": 1.9249,
      "grad_norm": 1.2456964254379272,
      "learning_rate": 0.00016143958868894602,
      "epoch": 0.6490566037735849,
      "step": 86
    },
    {
      "loss": 1.7468,
      "grad_norm": 1.226069450378418,
      "learning_rate": 0.0001609254498714653,
      "epoch": 0.6566037735849056,
      "step": 87
    },
    {
      "loss": 1.7375,
      "grad_norm": 1.20032799243927,
      "learning_rate": 0.0001604113110539846,
      "epoch": 0.6641509433962264,
      "step": 88
    },
    {
      "loss": 1.7824,
      "grad_norm": 1.2559722661972046,
      "learning_rate": 0.00015989717223650386,
      "epoch": 0.6716981132075471,
      "step": 89
    },
    {
      "loss": 1.7481,
      "grad_norm": 1.172574520111084,
      "learning_rate": 0.00015938303341902314,
      "epoch": 0.6792452830188679,
      "step": 90
    },
    {
      "loss": 1.7442,
      "grad_norm": 1.2171663045883179,
      "learning_rate": 0.00015886889460154242,
      "epoch": 0.6867924528301886,
      "step": 91
    },
    {
      "loss": 1.768,
      "grad_norm": 1.2091825008392334,
      "learning_rate": 0.00015835475578406173,
      "epoch": 0.6943396226415094,
      "step": 92
    },
    {
      "loss": 1.7437,
      "grad_norm": 1.1450062990188599,
      "learning_rate": 0.00015784061696658098,
      "epoch": 0.7018867924528301,
      "step": 93
    },
    {
      "loss": 1.7746,
      "grad_norm": 1.1820974349975586,
      "learning_rate": 0.00015732647814910026,
      "epoch": 0.7094339622641509,
      "step": 94
    },
    {
      "loss": 1.7019,
      "grad_norm": 1.2866746187210083,
      "learning_rate": 0.00015681233933161954,
      "epoch": 0.7169811320754716,
      "step": 95
    },
    {
      "loss": 1.744,
      "grad_norm": 1.226144552230835,
      "learning_rate": 0.00015629820051413882,
      "epoch": 0.7245283018867924,
      "step": 96
    },
    {
      "loss": 1.8121,
      "grad_norm": 1.192991852760315,
      "learning_rate": 0.0001557840616966581,
      "epoch": 0.7320754716981132,
      "step": 97
    },
    {
      "loss": 1.6809,
      "grad_norm": 1.2180848121643066,
      "learning_rate": 0.00015526992287917738,
      "epoch": 0.7396226415094339,
      "step": 98
    },
    {
      "loss": 1.6336,
      "grad_norm": 1.1730735301971436,
      "learning_rate": 0.00015475578406169666,
      "epoch": 0.7471698113207547,
      "step": 99
    },
    {
      "loss": 1.6995,
      "grad_norm": 1.18894624710083,
      "learning_rate": 0.00015424164524421594,
      "epoch": 0.7547169811320755,
      "step": 100
    },
    {
      "loss": 1.6117,
      "grad_norm": 1.1996971368789673,
      "learning_rate": 0.00015372750642673522,
      "epoch": 0.7622641509433963,
      "step": 101
    },
    {
      "loss": 1.7393,
      "grad_norm": 1.2368738651275635,
      "learning_rate": 0.0001532133676092545,
      "epoch": 0.769811320754717,
      "step": 102
    },
    {
      "loss": 1.7733,
      "grad_norm": 1.2250248193740845,
      "learning_rate": 0.00015269922879177378,
      "epoch": 0.7773584905660378,
      "step": 103
    },
    {
      "loss": 1.756,
      "grad_norm": 1.2310513257980347,
      "learning_rate": 0.00015218508997429307,
      "epoch": 0.7849056603773585,
      "step": 104
    },
    {
      "loss": 1.6819,
      "grad_norm": 1.2089784145355225,
      "learning_rate": 0.00015167095115681235,
      "epoch": 0.7924528301886793,
      "step": 105
    },
    {
      "loss": 1.8412,
      "grad_norm": 1.2401329278945923,
      "learning_rate": 0.00015115681233933163,
      "epoch": 0.8,
      "step": 106
    },
    {
      "loss": 1.8036,
      "grad_norm": 1.2456320524215698,
      "learning_rate": 0.0001506426735218509,
      "epoch": 0.8075471698113208,
      "step": 107
    },
    {
      "loss": 1.6911,
      "grad_norm": 1.2128560543060303,
      "learning_rate": 0.0001501285347043702,
      "epoch": 0.8150943396226416,
      "step": 108
    },
    {
      "loss": 1.7414,
      "grad_norm": 1.1808284521102905,
      "learning_rate": 0.00014961439588688947,
      "epoch": 0.8226415094339623,
      "step": 109
    },
    {
      "loss": 1.6919,
      "grad_norm": 1.2400094270706177,
      "learning_rate": 0.00014910025706940875,
      "epoch": 0.8301886792452831,
      "step": 110
    },
    {
      "loss": 1.7725,
      "grad_norm": 1.244788408279419,
      "learning_rate": 0.00014858611825192803,
      "epoch": 0.8377358490566038,
      "step": 111
    },
    {
      "loss": 1.6749,
      "grad_norm": 1.216129183769226,
      "learning_rate": 0.0001480719794344473,
      "epoch": 0.8452830188679246,
      "step": 112
    },
    {
      "loss": 1.6725,
      "grad_norm": 1.156552791595459,
      "learning_rate": 0.0001475578406169666,
      "epoch": 0.8528301886792453,
      "step": 113
    },
    {
      "loss": 1.6696,
      "grad_norm": 1.209347128868103,
      "learning_rate": 0.00014704370179948587,
      "epoch": 0.8603773584905661,
      "step": 114
    },
    {
      "loss": 1.6836,
      "grad_norm": 1.2561078071594238,
      "learning_rate": 0.00014652956298200515,
      "epoch": 0.8679245283018868,
      "step": 115
    },
    {
      "loss": 1.7203,
      "grad_norm": 1.2200998067855835,
      "learning_rate": 0.00014601542416452443,
      "epoch": 0.8754716981132076,
      "step": 116
    },
    {
      "loss": 1.6253,
      "grad_norm": 1.1617294549942017,
      "learning_rate": 0.0001455012853470437,
      "epoch": 0.8830188679245283,
      "step": 117
    },
    {
      "loss": 1.704,
      "grad_norm": 1.2382394075393677,
      "learning_rate": 0.000144987146529563,
      "epoch": 0.8905660377358491,
      "step": 118
    },
    {
      "loss": 1.6925,
      "grad_norm": 1.288994550704956,
      "learning_rate": 0.00014447300771208227,
      "epoch": 0.8981132075471698,
      "step": 119
    },
    {
      "loss": 1.6345,
      "grad_norm": 1.2480014562606812,
      "learning_rate": 0.00014395886889460155,
      "epoch": 0.9056603773584906,
      "step": 120
    },
    {
      "loss": 1.7041,
      "grad_norm": 1.2059645652770996,
      "learning_rate": 0.00014344473007712083,
      "epoch": 0.9132075471698113,
      "step": 121
    },
    {
      "loss": 1.6651,
      "grad_norm": 1.2603318691253662,
      "learning_rate": 0.0001429305912596401,
      "epoch": 0.9207547169811321,
      "step": 122
    },
    {
      "loss": 1.6248,
      "grad_norm": 1.2325612306594849,
      "learning_rate": 0.0001424164524421594,
      "epoch": 0.9283018867924528,
      "step": 123
    },
    {
      "loss": 1.726,
      "grad_norm": 1.224183201789856,
      "learning_rate": 0.00014190231362467867,
      "epoch": 0.9358490566037736,
      "step": 124
    },
    {
      "loss": 1.689,
      "grad_norm": 1.186186671257019,
      "learning_rate": 0.00014138817480719795,
      "epoch": 0.9433962264150944,
      "step": 125
    },
    {
      "loss": 1.642,
      "grad_norm": 1.1622978448867798,
      "learning_rate": 0.00014087403598971724,
      "epoch": 0.9509433962264151,
      "step": 126
    },
    {
      "loss": 1.581,
      "grad_norm": 1.2638009786605835,
      "learning_rate": 0.00014035989717223652,
      "epoch": 0.9584905660377359,
      "step": 127
    },
    {
      "loss": 1.6571,
      "grad_norm": 1.2909373044967651,
      "learning_rate": 0.0001398457583547558,
      "epoch": 0.9660377358490566,
      "step": 128
    },
    {
      "loss": 1.6625,
      "grad_norm": 1.209190011024475,
      "learning_rate": 0.00013933161953727508,
      "epoch": 0.9735849056603774,
      "step": 129
    },
    {
      "loss": 1.6208,
      "grad_norm": 1.1803642511367798,
      "learning_rate": 0.00013881748071979436,
      "epoch": 0.9811320754716981,
      "step": 130
    },
    {
      "loss": 1.5855,
      "grad_norm": 1.1989173889160156,
      "learning_rate": 0.0001383033419023136,
      "epoch": 0.9886792452830189,
      "step": 131
    },
    {
      "loss": 1.7047,
      "grad_norm": 1.1780502796173096,
      "learning_rate": 0.00013778920308483292,
      "epoch": 0.9962264150943396,
      "step": 132
    },
    {
      "loss": 1.7635,
      "grad_norm": 1.7488844394683838,
      "learning_rate": 0.0001372750642673522,
      "epoch": 1.0,
      "step": 133
    },
    {
      "eval_loss": 1.7017474174499512,
      "eval_runtime": 10.8945,
      "eval_samples_per_second": 48.648,
      "eval_steps_per_second": 6.15,
      "epoch": 1.0,
      "step": 133
    },
    {
      "loss": 1.4564,
      "grad_norm": 1.1450450420379639,
      "learning_rate": 0.00013676092544987148,
      "epoch": 1.0075471698113208,
      "step": 134
    },
    {
      "loss": 1.495,
      "grad_norm": 1.1412309408187866,
      "learning_rate": 0.00013624678663239073,
      "epoch": 1.0150943396226415,
      "step": 135
    },
    {
      "loss": 1.3822,
      "grad_norm": 1.1330032348632812,
      "learning_rate": 0.00013573264781491004,
      "epoch": 1.0226415094339623,
      "step": 136
    },
    {
      "loss": 1.4785,
      "grad_norm": 1.2176276445388794,
      "learning_rate": 0.00013521850899742932,
      "epoch": 1.030188679245283,
      "step": 137
    },
    {
      "loss": 1.4635,
      "grad_norm": 1.2104731798171997,
      "learning_rate": 0.0001347043701799486,
      "epoch": 1.0377358490566038,
      "step": 138
    },
    {
      "loss": 1.4378,
      "grad_norm": 1.2101114988327026,
      "learning_rate": 0.00013419023136246785,
      "epoch": 1.0452830188679245,
      "step": 139
    },
    {
      "loss": 1.414,
      "grad_norm": 1.2938369512557983,
      "learning_rate": 0.00013367609254498716,
      "epoch": 1.0528301886792453,
      "step": 140
    },
    {
      "loss": 1.4511,
      "grad_norm": 1.326425313949585,
      "learning_rate": 0.00013316195372750644,
      "epoch": 1.060377358490566,
      "step": 141
    },
    {
      "loss": 1.5321,
      "grad_norm": 1.4793202877044678,
      "learning_rate": 0.00013264781491002572,
      "epoch": 1.0679245283018868,
      "step": 142
    },
    {
      "loss": 1.4999,
      "grad_norm": 1.4197380542755127,
      "learning_rate": 0.00013213367609254498,
      "epoch": 1.0754716981132075,
      "step": 143
    },
    {
      "loss": 1.4832,
      "grad_norm": 1.4069544076919556,
      "learning_rate": 0.00013161953727506426,
      "epoch": 1.0830188679245283,
      "step": 144
    },
    {
      "loss": 1.4649,
      "grad_norm": 1.333242654800415,
      "learning_rate": 0.00013110539845758356,
      "epoch": 1.090566037735849,
      "step": 145
    },
    {
      "loss": 1.5012,
      "grad_norm": 1.4140286445617676,
      "learning_rate": 0.00013059125964010284,
      "epoch": 1.0981132075471698,
      "step": 146
    },
    {
      "loss": 1.5474,
      "grad_norm": 1.3501648902893066,
      "learning_rate": 0.00013007712082262213,
      "epoch": 1.1056603773584905,
      "step": 147
    },
    {
      "loss": 1.3567,
      "grad_norm": 1.3034641742706299,
      "learning_rate": 0.00012956298200514138,
      "epoch": 1.1132075471698113,
      "step": 148
    },
    {
      "loss": 1.3884,
      "grad_norm": 1.3540289402008057,
      "learning_rate": 0.00012904884318766069,
      "epoch": 1.120754716981132,
      "step": 149
    },
    {
      "loss": 1.5243,
      "grad_norm": 1.3642233610153198,
      "learning_rate": 0.00012853470437017997,
      "epoch": 1.1283018867924528,
      "step": 150
    },
    {
      "loss": 1.5185,
      "grad_norm": 1.3909785747528076,
      "learning_rate": 0.00012802056555269925,
      "epoch": 1.1358490566037736,
      "step": 151
    },
    {
      "loss": 1.4813,
      "grad_norm": 1.331985592842102,
      "learning_rate": 0.0001275064267352185,
      "epoch": 1.1433962264150943,
      "step": 152
    },
    {
      "loss": 1.4717,
      "grad_norm": 1.2682887315750122,
      "learning_rate": 0.00012699228791773778,
      "epoch": 1.150943396226415,
      "step": 153
    },
    {
      "loss": 1.4837,
      "grad_norm": 1.3724137544631958,
      "learning_rate": 0.0001264781491002571,
      "epoch": 1.1584905660377358,
      "step": 154
    },
    {
      "loss": 1.4121,
      "grad_norm": 1.2991222143173218,
      "learning_rate": 0.00012596401028277637,
      "epoch": 1.1660377358490566,
      "step": 155
    },
    {
      "loss": 1.41,
      "grad_norm": 1.3272463083267212,
      "learning_rate": 0.00012544987146529562,
      "epoch": 1.1735849056603773,
      "step": 156
    },
    {
      "loss": 1.399,
      "grad_norm": 1.3815639019012451,
      "learning_rate": 0.0001249357326478149,
      "epoch": 1.181132075471698,
      "step": 157
    },
    {
      "loss": 1.5061,
      "grad_norm": 1.3101471662521362,
      "learning_rate": 0.0001244215938303342,
      "epoch": 1.1886792452830188,
      "step": 158
    },
    {
      "loss": 1.4094,
      "grad_norm": 1.3652316331863403,
      "learning_rate": 0.0001239074550128535,
      "epoch": 1.1962264150943396,
      "step": 159
    },
    {
      "loss": 1.3866,
      "grad_norm": 1.3830440044403076,
      "learning_rate": 0.00012339331619537274,
      "epoch": 1.2037735849056603,
      "step": 160
    },
    {
      "loss": 1.4459,
      "grad_norm": 1.4946104288101196,
      "learning_rate": 0.00012287917737789202,
      "epoch": 1.211320754716981,
      "step": 161
    },
    {
      "loss": 1.4853,
      "grad_norm": 1.4620217084884644,
      "learning_rate": 0.00012236503856041133,
      "epoch": 1.2188679245283018,
      "step": 162
    },
    {
      "loss": 1.5242,
      "grad_norm": 1.53518807888031,
      "learning_rate": 0.0001218508997429306,
      "epoch": 1.2264150943396226,
      "step": 163
    },
    {
      "loss": 1.476,
      "grad_norm": 1.4435303211212158,
      "learning_rate": 0.00012133676092544988,
      "epoch": 1.2339622641509433,
      "step": 164
    },
    {
      "loss": 1.441,
      "grad_norm": 1.5178064107894897,
      "learning_rate": 0.00012082262210796915,
      "epoch": 1.241509433962264,
      "step": 165
    },
    {
      "loss": 1.47,
      "grad_norm": 1.4220784902572632,
      "learning_rate": 0.00012030848329048843,
      "epoch": 1.2490566037735849,
      "step": 166
    },
    {
      "loss": 1.502,
      "grad_norm": 1.4713746309280396,
      "learning_rate": 0.00011979434447300772,
      "epoch": 1.2566037735849056,
      "step": 167
    },
    {
      "loss": 1.4858,
      "grad_norm": 1.3731850385665894,
      "learning_rate": 0.000119280205655527,
      "epoch": 1.2641509433962264,
      "step": 168
    },
    {
      "loss": 1.4495,
      "grad_norm": 1.4068212509155273,
      "learning_rate": 0.00011876606683804628,
      "epoch": 1.271698113207547,
      "step": 169
    },
    {
      "loss": 1.4483,
      "grad_norm": 1.4740170240402222,
      "learning_rate": 0.00011825192802056555,
      "epoch": 1.2792452830188679,
      "step": 170
    },
    {
      "loss": 1.463,
      "grad_norm": 1.365977168083191,
      "learning_rate": 0.00011773778920308484,
      "epoch": 1.2867924528301886,
      "step": 171
    },
    {
      "loss": 1.5039,
      "grad_norm": 1.4337670803070068,
      "learning_rate": 0.00011722365038560412,
      "epoch": 1.2943396226415094,
      "step": 172
    },
    {
      "loss": 1.3726,
      "grad_norm": 1.3507415056228638,
      "learning_rate": 0.0001167095115681234,
      "epoch": 1.3018867924528301,
      "step": 173
    },
    {
      "loss": 1.4624,
      "grad_norm": 1.4803252220153809,
      "learning_rate": 0.00011619537275064267,
      "epoch": 1.3094339622641509,
      "step": 174
    },
    {
      "loss": 1.4489,
      "grad_norm": 1.4385240077972412,
      "learning_rate": 0.00011568123393316196,
      "epoch": 1.3169811320754716,
      "step": 175
    },
    {
      "loss": 1.4343,
      "grad_norm": 1.3419321775436401,
      "learning_rate": 0.00011516709511568124,
      "epoch": 1.3245283018867924,
      "step": 176
    },
    {
      "loss": 1.49,
      "grad_norm": 1.3933900594711304,
      "learning_rate": 0.00011465295629820053,
      "epoch": 1.3320754716981131,
      "step": 177
    },
    {
      "loss": 1.4239,
      "grad_norm": 1.371646761894226,
      "learning_rate": 0.00011413881748071979,
      "epoch": 1.3396226415094339,
      "step": 178
    },
    {
      "loss": 1.4266,
      "grad_norm": 1.4456239938735962,
      "learning_rate": 0.00011362467866323907,
      "epoch": 1.3471698113207546,
      "step": 179
    },
    {
      "loss": 1.4177,
      "grad_norm": 1.4059605598449707,
      "learning_rate": 0.00011311053984575837,
      "epoch": 1.3547169811320754,
      "step": 180
    },
    {
      "loss": 1.4249,
      "grad_norm": 1.4042613506317139,
      "learning_rate": 0.00011259640102827765,
      "epoch": 1.3622641509433961,
      "step": 181
    },
    {
      "loss": 1.506,
      "grad_norm": 1.5113691091537476,
      "learning_rate": 0.00011208226221079691,
      "epoch": 1.369811320754717,
      "step": 182
    },
    {
      "loss": 1.3969,
      "grad_norm": 1.5095642805099487,
      "learning_rate": 0.0001115681233933162,
      "epoch": 1.3773584905660377,
      "step": 183
    },
    {
      "loss": 1.4853,
      "grad_norm": 1.4634581804275513,
      "learning_rate": 0.00011105398457583549,
      "epoch": 1.3849056603773584,
      "step": 184
    },
    {
      "loss": 1.4625,
      "grad_norm": 1.405017375946045,
      "learning_rate": 0.00011053984575835477,
      "epoch": 1.3924528301886792,
      "step": 185
    },
    {
      "loss": 1.5063,
      "grad_norm": 1.4036991596221924,
      "learning_rate": 0.00011002570694087404,
      "epoch": 1.4,
      "step": 186
    },
    {
      "loss": 1.506,
      "grad_norm": 1.4899381399154663,
      "learning_rate": 0.00010951156812339332,
      "epoch": 1.4075471698113207,
      "step": 187
    },
    {
      "loss": 1.472,
      "grad_norm": 1.4880892038345337,
      "learning_rate": 0.0001089974293059126,
      "epoch": 1.4150943396226414,
      "step": 188
    },
    {
      "loss": 1.4038,
      "grad_norm": 1.4392297267913818,
      "learning_rate": 0.00010848329048843189,
      "epoch": 1.4226415094339622,
      "step": 189
    },
    {
      "loss": 1.4862,
      "grad_norm": 1.4701873064041138,
      "learning_rate": 0.00010796915167095116,
      "epoch": 1.430188679245283,
      "step": 190
    },
    {
      "loss": 1.402,
      "grad_norm": 1.40514075756073,
      "learning_rate": 0.00010745501285347044,
      "epoch": 1.4377358490566037,
      "step": 191
    },
    {
      "loss": 1.4256,
      "grad_norm": 1.4720193147659302,
      "learning_rate": 0.00010694087403598972,
      "epoch": 1.4452830188679244,
      "step": 192
    },
    {
      "loss": 1.414,
      "grad_norm": 1.4868065118789673,
      "learning_rate": 0.00010642673521850901,
      "epoch": 1.4528301886792452,
      "step": 193
    },
    {
      "loss": 1.5038,
      "grad_norm": 1.4978389739990234,
      "learning_rate": 0.00010591259640102828,
      "epoch": 1.460377358490566,
      "step": 194
    },
    {
      "loss": 1.3754,
      "grad_norm": 1.4954416751861572,
      "learning_rate": 0.00010539845758354756,
      "epoch": 1.4679245283018867,
      "step": 195
    },
    {
      "loss": 1.3716,
      "grad_norm": 1.5288251638412476,
      "learning_rate": 0.00010488431876606684,
      "epoch": 1.4754716981132074,
      "step": 196
    },
    {
      "loss": 1.4447,
      "grad_norm": 1.536712646484375,
      "learning_rate": 0.00010437017994858613,
      "epoch": 1.4830188679245282,
      "step": 197
    },
    {
      "loss": 1.4888,
      "grad_norm": 1.5443283319473267,
      "learning_rate": 0.00010385604113110541,
      "epoch": 1.490566037735849,
      "step": 198
    },
    {
      "loss": 1.4229,
      "grad_norm": 1.4798563718795776,
      "learning_rate": 0.00010334190231362468,
      "epoch": 1.4981132075471697,
      "step": 199
    },
    {
      "loss": 1.4445,
      "grad_norm": 1.4807049036026,
      "learning_rate": 0.00010282776349614396,
      "epoch": 1.5056603773584905,
      "step": 200
    },
    {
      "loss": 1.3425,
      "grad_norm": 1.3915996551513672,
      "learning_rate": 0.00010231362467866323,
      "epoch": 1.5132075471698112,
      "step": 201
    },
    {
      "loss": 1.456,
      "grad_norm": 1.4922332763671875,
      "learning_rate": 0.00010179948586118254,
      "epoch": 1.520754716981132,
      "step": 202
    },
    {
      "loss": 1.411,
      "grad_norm": 1.4404346942901611,
      "learning_rate": 0.0001012853470437018,
      "epoch": 1.5283018867924527,
      "step": 203
    },
    {
      "loss": 1.4176,
      "grad_norm": 1.4737262725830078,
      "learning_rate": 0.00010077120822622108,
      "epoch": 1.5358490566037735,
      "step": 204
    },
    {
      "loss": 1.4509,
      "grad_norm": 1.4965441226959229,
      "learning_rate": 0.00010025706940874035,
      "epoch": 1.5433962264150942,
      "step": 205
    },
    {
      "loss": 1.3695,
      "grad_norm": 1.4265236854553223,
      "learning_rate": 9.974293059125965e-05,
      "epoch": 1.550943396226415,
      "step": 206
    },
    {
      "loss": 1.4119,
      "grad_norm": 1.4799100160598755,
      "learning_rate": 9.922879177377893e-05,
      "epoch": 1.5584905660377357,
      "step": 207
    },
    {
      "loss": 1.4002,
      "grad_norm": 1.4812660217285156,
      "learning_rate": 9.87146529562982e-05,
      "epoch": 1.5660377358490565,
      "step": 208
    },
    {
      "loss": 1.3733,
      "grad_norm": 1.5196747779846191,
      "learning_rate": 9.820051413881749e-05,
      "epoch": 1.5735849056603772,
      "step": 209
    },
    {
      "loss": 1.4587,
      "grad_norm": 1.517755150794983,
      "learning_rate": 9.768637532133677e-05,
      "epoch": 1.581132075471698,
      "step": 210
    },
    {
      "loss": 1.4506,
      "grad_norm": 1.5056591033935547,
      "learning_rate": 9.717223650385605e-05,
      "epoch": 1.5886792452830187,
      "step": 211
    },
    {
      "loss": 1.5106,
      "grad_norm": 1.4946039915084839,
      "learning_rate": 9.665809768637533e-05,
      "epoch": 1.5962264150943395,
      "step": 212
    },
    {
      "loss": 1.4031,
      "grad_norm": 1.4506813287734985,
      "learning_rate": 9.61439588688946e-05,
      "epoch": 1.6037735849056602,
      "step": 213
    },
    {
      "loss": 1.4781,
      "grad_norm": 1.4484288692474365,
      "learning_rate": 9.562982005141389e-05,
      "epoch": 1.611320754716981,
      "step": 214
    },
    {
      "loss": 1.3607,
      "grad_norm": 1.5573108196258545,
      "learning_rate": 9.511568123393317e-05,
      "epoch": 1.6188679245283017,
      "step": 215
    },
    {
      "loss": 1.4526,
      "grad_norm": 1.5667742490768433,
      "learning_rate": 9.460154241645245e-05,
      "epoch": 1.6264150943396225,
      "step": 216
    },
    {
      "loss": 1.4202,
      "grad_norm": 1.3869796991348267,
      "learning_rate": 9.408740359897173e-05,
      "epoch": 1.6339622641509433,
      "step": 217
    },
    {
      "loss": 1.3869,
      "grad_norm": 1.4309173822402954,
      "learning_rate": 9.357326478149101e-05,
      "epoch": 1.641509433962264,
      "step": 218
    },
    {
      "loss": 1.5106,
      "grad_norm": 1.4560495615005493,
      "learning_rate": 9.305912596401029e-05,
      "epoch": 1.6490566037735848,
      "step": 219
    },
    {
      "loss": 1.4253,
      "grad_norm": 1.4957280158996582,
      "learning_rate": 9.254498714652957e-05,
      "epoch": 1.6566037735849055,
      "step": 220
    },
    {
      "loss": 1.4314,
      "grad_norm": 1.5000766515731812,
      "learning_rate": 9.203084832904885e-05,
      "epoch": 1.6641509433962263,
      "step": 221
    },
    {
      "loss": 1.4376,
      "grad_norm": 1.4631658792495728,
      "learning_rate": 9.151670951156813e-05,
      "epoch": 1.671698113207547,
      "step": 222
    },
    {
      "loss": 1.3641,
      "grad_norm": 1.4469780921936035,
      "learning_rate": 9.100257069408741e-05,
      "epoch": 1.6792452830188678,
      "step": 223
    },
    {
      "loss": 1.4306,
      "grad_norm": 1.4556012153625488,
      "learning_rate": 9.048843187660668e-05,
      "epoch": 1.6867924528301885,
      "step": 224
    },
    {
      "loss": 1.47,
      "grad_norm": 1.493470549583435,
      "learning_rate": 8.997429305912597e-05,
      "epoch": 1.6943396226415093,
      "step": 225
    },
    {
      "loss": 1.4193,
      "grad_norm": 1.5036957263946533,
      "learning_rate": 8.946015424164524e-05,
      "epoch": 1.70188679245283,
      "step": 226
    },
    {
      "loss": 1.4393,
      "grad_norm": 1.5293830633163452,
      "learning_rate": 8.894601542416453e-05,
      "epoch": 1.7094339622641508,
      "step": 227
    },
    {
      "loss": 1.4152,
      "grad_norm": 1.4847708940505981,
      "learning_rate": 8.84318766066838e-05,
      "epoch": 1.7169811320754715,
      "step": 228
    },
    {
      "loss": 1.3807,
      "grad_norm": 1.4974397420883179,
      "learning_rate": 8.79177377892031e-05,
      "epoch": 1.7245283018867923,
      "step": 229
    },
    {
      "loss": 1.4031,
      "grad_norm": 1.450381875038147,
      "learning_rate": 8.740359897172236e-05,
      "epoch": 1.732075471698113,
      "step": 230
    },
    {
      "loss": 1.4315,
      "grad_norm": 1.4852321147918701,
      "learning_rate": 8.688946015424166e-05,
      "epoch": 1.7396226415094338,
      "step": 231
    },
    {
      "loss": 1.446,
      "grad_norm": 1.5360887050628662,
      "learning_rate": 8.637532133676092e-05,
      "epoch": 1.7471698113207546,
      "step": 232
    },
    {
      "loss": 1.4185,
      "grad_norm": 1.4593687057495117,
      "learning_rate": 8.586118251928022e-05,
      "epoch": 1.7547169811320755,
      "step": 233
    },
    {
      "loss": 1.4028,
      "grad_norm": 1.4818612337112427,
      "learning_rate": 8.534704370179948e-05,
      "epoch": 1.7622641509433963,
      "step": 234
    },
    {
      "loss": 1.4589,
      "grad_norm": 1.5037764310836792,
      "learning_rate": 8.483290488431876e-05,
      "epoch": 1.769811320754717,
      "step": 235
    },
    {
      "loss": 1.3782,
      "grad_norm": 1.4872180223464966,
      "learning_rate": 8.431876606683805e-05,
      "epoch": 1.7773584905660378,
      "step": 236
    },
    {
      "loss": 1.4442,
      "grad_norm": 1.5550490617752075,
      "learning_rate": 8.380462724935733e-05,
      "epoch": 1.7849056603773585,
      "step": 237
    },
    {
      "loss": 1.3873,
      "grad_norm": 1.5572818517684937,
      "learning_rate": 8.32904884318766e-05,
      "epoch": 1.7924528301886793,
      "step": 238
    },
    {
      "loss": 1.4936,
      "grad_norm": 1.457452416419983,
      "learning_rate": 8.277634961439589e-05,
      "epoch": 1.8,
      "step": 239
    },
    {
      "loss": 1.4311,
      "grad_norm": 1.4299843311309814,
      "learning_rate": 8.226221079691517e-05,
      "epoch": 1.8075471698113208,
      "step": 240
    },
    {
      "loss": 1.3966,
      "grad_norm": 1.4990639686584473,
      "learning_rate": 8.174807197943445e-05,
      "epoch": 1.8150943396226416,
      "step": 241
    },
    {
      "loss": 1.4416,
      "grad_norm": 1.4516873359680176,
      "learning_rate": 8.123393316195373e-05,
      "epoch": 1.8226415094339623,
      "step": 242
    },
    {
      "loss": 1.3964,
      "grad_norm": 1.4251058101654053,
      "learning_rate": 8.071979434447301e-05,
      "epoch": 1.830188679245283,
      "step": 243
    },
    {
      "loss": 1.4962,
      "grad_norm": 1.487265944480896,
      "learning_rate": 8.02056555269923e-05,
      "epoch": 1.8377358490566038,
      "step": 244
    },
    {
      "loss": 1.4205,
      "grad_norm": 1.4967126846313477,
      "learning_rate": 7.969151670951157e-05,
      "epoch": 1.8452830188679246,
      "step": 245
    },
    {
      "loss": 1.3737,
      "grad_norm": 1.4346139430999756,
      "learning_rate": 7.917737789203086e-05,
      "epoch": 1.8528301886792453,
      "step": 246
    },
    {
      "loss": 1.3703,
      "grad_norm": 1.4335639476776123,
      "learning_rate": 7.866323907455013e-05,
      "epoch": 1.860377358490566,
      "step": 247
    },
    {
      "loss": 1.3352,
      "grad_norm": 1.4612162113189697,
      "learning_rate": 7.814910025706941e-05,
      "epoch": 1.8679245283018868,
      "step": 248
    },
    {
      "loss": 1.45,
      "grad_norm": 1.5152490139007568,
      "learning_rate": 7.763496143958869e-05,
      "epoch": 1.8754716981132076,
      "step": 249
    },
    {
      "loss": 1.4337,
      "grad_norm": 1.5146833658218384,
      "learning_rate": 7.712082262210797e-05,
      "epoch": 1.8830188679245283,
      "step": 250
    },
    {
      "loss": 1.4382,
      "grad_norm": 1.537802815437317,
      "learning_rate": 7.660668380462725e-05,
      "epoch": 1.890566037735849,
      "step": 251
    },
    {
      "loss": 1.4174,
      "grad_norm": 1.528414011001587,
      "learning_rate": 7.609254498714653e-05,
      "epoch": 1.8981132075471698,
      "step": 252
    },
    {
      "loss": 1.3521,
      "grad_norm": 1.5186940431594849,
      "learning_rate": 7.557840616966581e-05,
      "epoch": 1.9056603773584906,
      "step": 253
    },
    {
      "loss": 1.3379,
      "grad_norm": 1.519835352897644,
      "learning_rate": 7.50642673521851e-05,
      "epoch": 1.9132075471698113,
      "step": 254
    },
    {
      "loss": 1.4384,
      "grad_norm": 1.5257344245910645,
      "learning_rate": 7.455012853470437e-05,
      "epoch": 1.920754716981132,
      "step": 255
    },
    {
      "loss": 1.4657,
      "grad_norm": 1.5222282409667969,
      "learning_rate": 7.403598971722365e-05,
      "epoch": 1.9283018867924528,
      "step": 256
    },
    {
      "loss": 1.4534,
      "grad_norm": 1.526237964630127,
      "learning_rate": 7.352185089974293e-05,
      "epoch": 1.9358490566037736,
      "step": 257
    },
    {
      "loss": 1.3524,
      "grad_norm": 1.457645297050476,
      "learning_rate": 7.300771208226222e-05,
      "epoch": 1.9433962264150944,
      "step": 258
    },
    {
      "loss": 1.3771,
      "grad_norm": 1.4623669385910034,
      "learning_rate": 7.24935732647815e-05,
      "epoch": 1.950943396226415,
      "step": 259
    },
    {
      "loss": 1.3682,
      "grad_norm": 1.518298864364624,
      "learning_rate": 7.197943444730078e-05,
      "epoch": 1.9584905660377359,
      "step": 260
    },
    {
      "loss": 1.4057,
      "grad_norm": 1.547526478767395,
      "learning_rate": 7.146529562982006e-05,
      "epoch": 1.9660377358490566,
      "step": 261
    },
    {
      "loss": 1.4541,
      "grad_norm": 1.5345505475997925,
      "learning_rate": 7.095115681233934e-05,
      "epoch": 1.9735849056603774,
      "step": 262
    },
    {
      "loss": 1.4395,
      "grad_norm": 1.4363181591033936,
      "learning_rate": 7.043701799485862e-05,
      "epoch": 1.9811320754716981,
      "step": 263
    },
    {
      "loss": 1.3441,
      "grad_norm": 1.4867011308670044,
      "learning_rate": 6.99228791773779e-05,
      "epoch": 1.9886792452830189,
      "step": 264
    },
    {
      "loss": 1.393,
      "grad_norm": 1.5648738145828247,
      "learning_rate": 6.940874035989718e-05,
      "epoch": 1.9962264150943396,
      "step": 265
    },
    {
      "loss": 1.391,
      "grad_norm": 2.089810848236084,
      "learning_rate": 6.889460154241646e-05,
      "epoch": 2.0,
      "step": 266
    },
    {
      "eval_loss": 1.6115559339523315,
      "eval_runtime": 10.838,
      "eval_samples_per_second": 48.902,
      "eval_steps_per_second": 6.182,
      "epoch": 2.0,
      "step": 266
    },
    {
      "loss": 1.1828,
      "grad_norm": 1.3912994861602783,
      "learning_rate": 6.838046272493574e-05,
      "epoch": 2.0075471698113208,
      "step": 267
    },
    {
      "loss": 1.2089,
      "grad_norm": 1.414544939994812,
      "learning_rate": 6.786632390745502e-05,
      "epoch": 2.0150943396226415,
      "step": 268
    },
    {
      "loss": 1.2491,
      "grad_norm": 1.4445010423660278,
      "learning_rate": 6.73521850899743e-05,
      "epoch": 2.0226415094339623,
      "step": 269
    },
    {
      "loss": 1.2164,
      "grad_norm": 1.4448374509811401,
      "learning_rate": 6.683804627249358e-05,
      "epoch": 2.030188679245283,
      "step": 270
    },
    {
      "loss": 1.2192,
      "grad_norm": 1.4503034353256226,
      "learning_rate": 6.632390745501286e-05,
      "epoch": 2.0377358490566038,
      "step": 271
    },
    {
      "loss": 1.191,
      "grad_norm": 1.4793455600738525,
      "learning_rate": 6.580976863753213e-05,
      "epoch": 2.0452830188679245,
      "step": 272
    },
    {
      "loss": 1.1679,
      "grad_norm": 1.4452569484710693,
      "learning_rate": 6.529562982005142e-05,
      "epoch": 2.0528301886792453,
      "step": 273
    },
    {
      "loss": 1.1854,
      "grad_norm": 1.539241909980774,
      "learning_rate": 6.478149100257069e-05,
      "epoch": 2.060377358490566,
      "step": 274
    },
    {
      "loss": 1.1786,
      "grad_norm": 1.5919784307479858,
      "learning_rate": 6.426735218508998e-05,
      "epoch": 2.0679245283018868,
      "step": 275
    },
    {
      "loss": 1.1801,
      "grad_norm": 1.5084917545318604,
      "learning_rate": 6.375321336760925e-05,
      "epoch": 2.0754716981132075,
      "step": 276
    },
    {
      "loss": 1.2134,
      "grad_norm": 1.5896055698394775,
      "learning_rate": 6.323907455012854e-05,
      "epoch": 2.0830188679245283,
      "step": 277
    },
    {
      "loss": 1.1657,
      "grad_norm": 1.597206950187683,
      "learning_rate": 6.272493573264781e-05,
      "epoch": 2.090566037735849,
      "step": 278
    },
    {
      "loss": 1.205,
      "grad_norm": 1.6169607639312744,
      "learning_rate": 6.22107969151671e-05,
      "epoch": 2.09811320754717,
      "step": 279
    },
    {
      "loss": 1.1872,
      "grad_norm": 1.6751397848129272,
      "learning_rate": 6.169665809768637e-05,
      "epoch": 2.1056603773584905,
      "step": 280
    },
    {
      "loss": 1.1375,
      "grad_norm": 1.7145068645477295,
      "learning_rate": 6.118251928020567e-05,
      "epoch": 2.1132075471698113,
      "step": 281
    },
    {
      "loss": 1.2652,
      "grad_norm": 1.7699044942855835,
      "learning_rate": 6.066838046272494e-05,
      "epoch": 2.120754716981132,
      "step": 282
    },
    {
      "loss": 1.1846,
      "grad_norm": 1.7645443677902222,
      "learning_rate": 6.015424164524421e-05,
      "epoch": 2.128301886792453,
      "step": 283
    },
    {
      "loss": 1.2294,
      "grad_norm": 1.740395426750183,
      "learning_rate": 5.96401028277635e-05,
      "epoch": 2.1358490566037736,
      "step": 284
    },
    {
      "loss": 1.1936,
      "grad_norm": 1.7227061986923218,
      "learning_rate": 5.9125964010282774e-05,
      "epoch": 2.1433962264150943,
      "step": 285
    },
    {
      "loss": 1.1822,
      "grad_norm": 1.8022714853286743,
      "learning_rate": 5.861182519280206e-05,
      "epoch": 2.150943396226415,
      "step": 286
    },
    {
      "loss": 1.15,
      "grad_norm": 1.7713589668273926,
      "learning_rate": 5.8097686375321335e-05,
      "epoch": 2.158490566037736,
      "step": 287
    },
    {
      "loss": 1.1421,
      "grad_norm": 1.767161250114441,
      "learning_rate": 5.758354755784062e-05,
      "epoch": 2.1660377358490566,
      "step": 288
    },
    {
      "loss": 1.2162,
      "grad_norm": 1.780168890953064,
      "learning_rate": 5.7069408740359896e-05,
      "epoch": 2.1735849056603773,
      "step": 289
    },
    {
      "loss": 1.088,
      "grad_norm": 1.7347424030303955,
      "learning_rate": 5.655526992287918e-05,
      "epoch": 2.181132075471698,
      "step": 290
    },
    {
      "loss": 1.2234,
      "grad_norm": 1.7940436601638794,
      "learning_rate": 5.604113110539846e-05,
      "epoch": 2.188679245283019,
      "step": 291
    },
    {
      "loss": 1.2139,
      "grad_norm": 1.8659151792526245,
      "learning_rate": 5.5526992287917744e-05,
      "epoch": 2.1962264150943396,
      "step": 292
    },
    {
      "loss": 1.2453,
      "grad_norm": 1.7813113927841187,
      "learning_rate": 5.501285347043702e-05,
      "epoch": 2.2037735849056603,
      "step": 293
    },
    {
      "loss": 1.2242,
      "grad_norm": 1.7604901790618896,
      "learning_rate": 5.44987146529563e-05,
      "epoch": 2.211320754716981,
      "step": 294
    },
    {
      "loss": 1.1699,
      "grad_norm": 1.769625186920166,
      "learning_rate": 5.398457583547558e-05,
      "epoch": 2.218867924528302,
      "step": 295
    },
    {
      "loss": 1.1435,
      "grad_norm": 1.739170789718628,
      "learning_rate": 5.347043701799486e-05,
      "epoch": 2.2264150943396226,
      "step": 296
    },
    {
      "loss": 1.1921,
      "grad_norm": 1.8786224126815796,
      "learning_rate": 5.295629820051414e-05,
      "epoch": 2.2339622641509433,
      "step": 297
    },
    {
      "loss": 1.1558,
      "grad_norm": 1.7360581159591675,
      "learning_rate": 5.244215938303342e-05,
      "epoch": 2.241509433962264,
      "step": 298
    },
    {
      "loss": 1.159,
      "grad_norm": 1.7528023719787598,
      "learning_rate": 5.192802056555271e-05,
      "epoch": 2.249056603773585,
      "step": 299
    },
    {
      "loss": 1.1514,
      "grad_norm": 1.7539137601852417,
      "learning_rate": 5.141388174807198e-05,
      "epoch": 2.2566037735849056,
      "step": 300
    },
    {
      "loss": 1.1856,
      "grad_norm": 1.7609119415283203,
      "learning_rate": 5.089974293059127e-05,
      "epoch": 2.2641509433962264,
      "step": 301
    },
    {
      "loss": 1.1582,
      "grad_norm": 1.7249377965927124,
      "learning_rate": 5.038560411311054e-05,
      "epoch": 2.271698113207547,
      "step": 302
    },
    {
      "loss": 1.1781,
      "grad_norm": 1.8279874324798584,
      "learning_rate": 4.987146529562982e-05,
      "epoch": 2.279245283018868,
      "step": 303
    },
    {
      "loss": 1.1737,
      "grad_norm": 1.7067161798477173,
      "learning_rate": 4.93573264781491e-05,
      "epoch": 2.2867924528301886,
      "step": 304
    },
    {
      "loss": 1.1788,
      "grad_norm": 1.7092854976654053,
      "learning_rate": 4.8843187660668383e-05,
      "epoch": 2.2943396226415094,
      "step": 305
    },
    {
      "loss": 1.1305,
      "grad_norm": 1.8026684522628784,
      "learning_rate": 4.8329048843187664e-05,
      "epoch": 2.30188679245283,
      "step": 306
    },
    {
      "loss": 1.1495,
      "grad_norm": 1.716173529624939,
      "learning_rate": 4.7814910025706944e-05,
      "epoch": 2.309433962264151,
      "step": 307
    },
    {
      "loss": 1.1752,
      "grad_norm": 1.7893472909927368,
      "learning_rate": 4.7300771208226225e-05,
      "epoch": 2.3169811320754716,
      "step": 308
    },
    {
      "loss": 1.1752,
      "grad_norm": 1.7626622915267944,
      "learning_rate": 4.6786632390745505e-05,
      "epoch": 2.3245283018867924,
      "step": 309
    },
    {
      "loss": 1.1483,
      "grad_norm": 1.8190288543701172,
      "learning_rate": 4.6272493573264786e-05,
      "epoch": 2.332075471698113,
      "step": 310
    },
    {
      "loss": 1.1993,
      "grad_norm": 1.7880665063858032,
      "learning_rate": 4.5758354755784066e-05,
      "epoch": 2.339622641509434,
      "step": 311
    },
    {
      "loss": 1.225,
      "grad_norm": 1.788029432296753,
      "learning_rate": 4.524421593830334e-05,
      "epoch": 2.3471698113207546,
      "step": 312
    },
    {
      "loss": 1.2468,
      "grad_norm": 1.7639864683151245,
      "learning_rate": 4.473007712082262e-05,
      "epoch": 2.3547169811320754,
      "step": 313
    },
    {
      "loss": 1.1995,
      "grad_norm": 1.7678985595703125,
      "learning_rate": 4.42159383033419e-05,
      "epoch": 2.362264150943396,
      "step": 314
    },
    {
      "loss": 1.2302,
      "grad_norm": 1.9127552509307861,
      "learning_rate": 4.370179948586118e-05,
      "epoch": 2.369811320754717,
      "step": 315
    },
    {
      "loss": 1.1985,
      "grad_norm": 1.8176268339157104,
      "learning_rate": 4.318766066838046e-05,
      "epoch": 2.3773584905660377,
      "step": 316
    },
    {
      "loss": 1.1826,
      "grad_norm": 1.7954981327056885,
      "learning_rate": 4.267352185089974e-05,
      "epoch": 2.3849056603773584,
      "step": 317
    },
    {
      "loss": 1.1645,
      "grad_norm": 1.8126698732376099,
      "learning_rate": 4.215938303341902e-05,
      "epoch": 2.392452830188679,
      "step": 318
    },
    {
      "loss": 1.2103,
      "grad_norm": 1.82870352268219,
      "learning_rate": 4.16452442159383e-05,
      "epoch": 2.4,
      "step": 319
    },
    {
      "loss": 1.2291,
      "grad_norm": 1.800594449043274,
      "learning_rate": 4.1131105398457584e-05,
      "epoch": 2.4075471698113207,
      "step": 320
    },
    {
      "loss": 1.1814,
      "grad_norm": 1.8011165857315063,
      "learning_rate": 4.0616966580976864e-05,
      "epoch": 2.4150943396226414,
      "step": 321
    },
    {
      "loss": 1.1334,
      "grad_norm": 1.8189189434051514,
      "learning_rate": 4.010282776349615e-05,
      "epoch": 2.422641509433962,
      "step": 322
    },
    {
      "loss": 1.254,
      "grad_norm": 1.862934947013855,
      "learning_rate": 3.958868894601543e-05,
      "epoch": 2.430188679245283,
      "step": 323
    },
    {
      "loss": 1.2071,
      "grad_norm": 1.8624433279037476,
      "learning_rate": 3.9074550128534705e-05,
      "epoch": 2.4377358490566037,
      "step": 324
    },
    {
      "loss": 1.1837,
      "grad_norm": 1.7971088886260986,
      "learning_rate": 3.8560411311053986e-05,
      "epoch": 2.4452830188679244,
      "step": 325
    },
    {
      "loss": 1.2252,
      "grad_norm": 1.8095810413360596,
      "learning_rate": 3.8046272493573266e-05,
      "epoch": 2.452830188679245,
      "step": 326
    },
    {
      "loss": 1.1392,
      "grad_norm": 1.706066608428955,
      "learning_rate": 3.753213367609255e-05,
      "epoch": 2.460377358490566,
      "step": 327
    },
    {
      "loss": 1.1523,
      "grad_norm": 1.7356079816818237,
      "learning_rate": 3.701799485861183e-05,
      "epoch": 2.4679245283018867,
      "step": 328
    },
    {
      "loss": 1.2065,
      "grad_norm": 1.8184373378753662,
      "learning_rate": 3.650385604113111e-05,
      "epoch": 2.4754716981132074,
      "step": 329
    },
    {
      "loss": 1.1895,
      "grad_norm": 1.8178374767303467,
      "learning_rate": 3.598971722365039e-05,
      "epoch": 2.483018867924528,
      "step": 330
    },
    {
      "loss": 1.1705,
      "grad_norm": 1.6923563480377197,
      "learning_rate": 3.547557840616967e-05,
      "epoch": 2.490566037735849,
      "step": 331
    },
    {
      "loss": 1.1935,
      "grad_norm": 1.8204443454742432,
      "learning_rate": 3.496143958868895e-05,
      "epoch": 2.4981132075471697,
      "step": 332
    },
    {
      "loss": 1.1968,
      "grad_norm": 1.9548351764678955,
      "learning_rate": 3.444730077120823e-05,
      "epoch": 2.5056603773584905,
      "step": 333
    },
    {
      "loss": 1.1795,
      "grad_norm": 1.8017340898513794,
      "learning_rate": 3.393316195372751e-05,
      "epoch": 2.513207547169811,
      "step": 334
    },
    {
      "loss": 1.1181,
      "grad_norm": 1.7527904510498047,
      "learning_rate": 3.341902313624679e-05,
      "epoch": 2.520754716981132,
      "step": 335
    },
    {
      "loss": 1.2008,
      "grad_norm": 1.8164780139923096,
      "learning_rate": 3.2904884318766064e-05,
      "epoch": 2.5283018867924527,
      "step": 336
    },
    {
      "loss": 1.1954,
      "grad_norm": 1.802573800086975,
      "learning_rate": 3.2390745501285345e-05,
      "epoch": 2.5358490566037735,
      "step": 337
    },
    {
      "loss": 1.1728,
      "grad_norm": 1.8414584398269653,
      "learning_rate": 3.1876606683804625e-05,
      "epoch": 2.543396226415094,
      "step": 338
    },
    {
      "loss": 1.1938,
      "grad_norm": 1.841536283493042,
      "learning_rate": 3.1362467866323906e-05,
      "epoch": 2.550943396226415,
      "step": 339
    },
    {
      "loss": 1.1195,
      "grad_norm": 1.7718199491500854,
      "learning_rate": 3.0848329048843186e-05,
      "epoch": 2.5584905660377357,
      "step": 340
    },
    {
      "loss": 1.1518,
      "grad_norm": 1.7840667963027954,
      "learning_rate": 3.033419023136247e-05,
      "epoch": 2.5660377358490565,
      "step": 341
    },
    {
      "loss": 1.2927,
      "grad_norm": 1.8936119079589844,
      "learning_rate": 2.982005141388175e-05,
      "epoch": 2.5735849056603772,
      "step": 342
    },
    {
      "loss": 1.1728,
      "grad_norm": 1.9069455862045288,
      "learning_rate": 2.930591259640103e-05,
      "epoch": 2.581132075471698,
      "step": 343
    },
    {
      "loss": 1.1844,
      "grad_norm": 1.86262845993042,
      "learning_rate": 2.879177377892031e-05,
      "epoch": 2.5886792452830187,
      "step": 344
    },
    {
      "loss": 1.2131,
      "grad_norm": 1.883484959602356,
      "learning_rate": 2.827763496143959e-05,
      "epoch": 2.5962264150943395,
      "step": 345
    },
    {
      "loss": 1.2354,
      "grad_norm": 1.8782117366790771,
      "learning_rate": 2.7763496143958872e-05,
      "epoch": 2.6037735849056602,
      "step": 346
    },
    {
      "loss": 1.2647,
      "grad_norm": 1.8663997650146484,
      "learning_rate": 2.724935732647815e-05,
      "epoch": 2.611320754716981,
      "step": 347
    },
    {
      "loss": 1.2369,
      "grad_norm": 1.8544902801513672,
      "learning_rate": 2.673521850899743e-05,
      "epoch": 2.6188679245283017,
      "step": 348
    },
    {
      "loss": 1.2125,
      "grad_norm": 1.9053699970245361,
      "learning_rate": 2.622107969151671e-05,
      "epoch": 2.6264150943396225,
      "step": 349
    },
    {
      "loss": 1.1233,
      "grad_norm": 1.7682791948318481,
      "learning_rate": 2.570694087403599e-05,
      "epoch": 2.6339622641509433,
      "step": 350
    },
    {
      "loss": 1.1334,
      "grad_norm": 1.7853431701660156,
      "learning_rate": 2.519280205655527e-05,
      "epoch": 2.641509433962264,
      "step": 351
    },
    {
      "loss": 1.1506,
      "grad_norm": 1.7930688858032227,
      "learning_rate": 2.467866323907455e-05,
      "epoch": 2.6490566037735848,
      "step": 352
    },
    {
      "loss": 1.1785,
      "grad_norm": 1.8703558444976807,
      "learning_rate": 2.4164524421593832e-05,
      "epoch": 2.6566037735849055,
      "step": 353
    },
    {
      "loss": 1.1828,
      "grad_norm": 1.8087106943130493,
      "learning_rate": 2.3650385604113112e-05,
      "epoch": 2.6641509433962263,
      "step": 354
    },
    {
      "loss": 1.1702,
      "grad_norm": 1.884810209274292,
      "learning_rate": 2.3136246786632393e-05,
      "epoch": 2.671698113207547,
      "step": 355
    },
    {
      "loss": 1.134,
      "grad_norm": 1.8646249771118164,
      "learning_rate": 2.262210796915167e-05,
      "epoch": 2.6792452830188678,
      "step": 356
    },
    {
      "loss": 1.1482,
      "grad_norm": 1.8337599039077759,
      "learning_rate": 2.210796915167095e-05,
      "epoch": 2.6867924528301885,
      "step": 357
    },
    {
      "loss": 1.2203,
      "grad_norm": 1.8909101486206055,
      "learning_rate": 2.159383033419023e-05,
      "epoch": 2.6943396226415093,
      "step": 358
    },
    {
      "loss": 1.1572,
      "grad_norm": 1.7900224924087524,
      "learning_rate": 2.107969151670951e-05,
      "epoch": 2.70188679245283,
      "step": 359
    },
    {
      "loss": 1.1747,
      "grad_norm": 1.768805742263794,
      "learning_rate": 2.0565552699228792e-05,
      "epoch": 2.709433962264151,
      "step": 360
    },
    {
      "loss": 1.1088,
      "grad_norm": 1.8017759323120117,
      "learning_rate": 2.0051413881748076e-05,
      "epoch": 2.7169811320754715,
      "step": 361
    },
    {
      "loss": 1.2558,
      "grad_norm": 1.9017211198806763,
      "learning_rate": 1.9537275064267353e-05,
      "epoch": 2.7245283018867923,
      "step": 362
    },
    {
      "loss": 1.128,
      "grad_norm": 1.817359209060669,
      "learning_rate": 1.9023136246786633e-05,
      "epoch": 2.732075471698113,
      "step": 363
    },
    {
      "loss": 1.1603,
      "grad_norm": 1.8151612281799316,
      "learning_rate": 1.8508997429305914e-05,
      "epoch": 2.739622641509434,
      "step": 364
    },
    {
      "loss": 1.188,
      "grad_norm": 1.8483037948608398,
      "learning_rate": 1.7994858611825194e-05,
      "epoch": 2.7471698113207546,
      "step": 365
    },
    {
      "loss": 1.209,
      "grad_norm": 1.9165133237838745,
      "learning_rate": 1.7480719794344475e-05,
      "epoch": 2.7547169811320753,
      "step": 366
    },
    {
      "loss": 1.1499,
      "grad_norm": 1.8353382349014282,
      "learning_rate": 1.6966580976863755e-05,
      "epoch": 2.7622641509433965,
      "step": 367
    },
    {
      "loss": 1.13,
      "grad_norm": 1.77681565284729,
      "learning_rate": 1.6452442159383032e-05,
      "epoch": 2.769811320754717,
      "step": 368
    },
    {
      "loss": 1.1105,
      "grad_norm": 1.783952236175537,
      "learning_rate": 1.5938303341902313e-05,
      "epoch": 2.777358490566038,
      "step": 369
    },
    {
      "loss": 1.2236,
      "grad_norm": 1.8255881071090698,
      "learning_rate": 1.5424164524421593e-05,
      "epoch": 2.7849056603773583,
      "step": 370
    },
    {
      "loss": 1.1613,
      "grad_norm": 1.8458627462387085,
      "learning_rate": 1.4910025706940875e-05,
      "epoch": 2.7924528301886795,
      "step": 371
    },
    {
      "loss": 1.2001,
      "grad_norm": 1.8773030042648315,
      "learning_rate": 1.4395886889460156e-05,
      "epoch": 2.8,
      "step": 372
    },
    {
      "loss": 1.193,
      "grad_norm": 1.9307880401611328,
      "learning_rate": 1.3881748071979436e-05,
      "epoch": 2.807547169811321,
      "step": 373
    },
    {
      "loss": 1.2088,
      "grad_norm": 1.8551815748214722,
      "learning_rate": 1.3367609254498715e-05,
      "epoch": 2.8150943396226413,
      "step": 374
    },
    {
      "loss": 1.1453,
      "grad_norm": 1.8039122819900513,
      "learning_rate": 1.2853470437017995e-05,
      "epoch": 2.8226415094339625,
      "step": 375
    },
    {
      "loss": 1.1795,
      "grad_norm": 1.8185582160949707,
      "learning_rate": 1.2339331619537276e-05,
      "epoch": 2.830188679245283,
      "step": 376
    },
    {
      "loss": 1.1957,
      "grad_norm": 1.81277596950531,
      "learning_rate": 1.1825192802056556e-05,
      "epoch": 2.837735849056604,
      "step": 377
    },
    {
      "loss": 1.21,
      "grad_norm": 1.9246152639389038,
      "learning_rate": 1.1311053984575835e-05,
      "epoch": 2.8452830188679243,
      "step": 378
    },
    {
      "loss": 1.1174,
      "grad_norm": 1.8959848880767822,
      "learning_rate": 1.0796915167095115e-05,
      "epoch": 2.8528301886792455,
      "step": 379
    },
    {
      "loss": 1.2169,
      "grad_norm": 1.8289810419082642,
      "learning_rate": 1.0282776349614396e-05,
      "epoch": 2.860377358490566,
      "step": 380
    },
    {
      "loss": 1.1702,
      "grad_norm": 1.812321424484253,
      "learning_rate": 9.768637532133676e-06,
      "epoch": 2.867924528301887,
      "step": 381
    },
    {
      "loss": 1.1334,
      "grad_norm": 1.8101327419281006,
      "learning_rate": 9.254498714652957e-06,
      "epoch": 2.8754716981132074,
      "step": 382
    },
    {
      "loss": 1.2014,
      "grad_norm": 1.813897728919983,
      "learning_rate": 8.740359897172237e-06,
      "epoch": 2.8830188679245285,
      "step": 383
    },
    {
      "loss": 1.1557,
      "grad_norm": 1.783665657043457,
      "learning_rate": 8.226221079691516e-06,
      "epoch": 2.890566037735849,
      "step": 384
    },
    {
      "loss": 1.1064,
      "grad_norm": 1.7805958986282349,
      "learning_rate": 7.712082262210796e-06,
      "epoch": 2.89811320754717,
      "step": 385
    },
    {
      "loss": 1.1605,
      "grad_norm": 1.819898247718811,
      "learning_rate": 7.197943444730078e-06,
      "epoch": 2.9056603773584904,
      "step": 386
    },
    {
      "loss": 1.1655,
      "grad_norm": 1.770599603652954,
      "learning_rate": 6.683804627249357e-06,
      "epoch": 2.9132075471698116,
      "step": 387
    },
    {
      "loss": 1.2126,
      "grad_norm": 1.8346935510635376,
      "learning_rate": 6.169665809768638e-06,
      "epoch": 2.920754716981132,
      "step": 388
    },
    {
      "loss": 1.2321,
      "grad_norm": 1.9071781635284424,
      "learning_rate": 5.6555269922879175e-06,
      "epoch": 2.928301886792453,
      "step": 389
    },
    {
      "loss": 1.1497,
      "grad_norm": 1.8098198175430298,
      "learning_rate": 5.141388174807198e-06,
      "epoch": 2.9358490566037734,
      "step": 390
    },
    {
      "loss": 1.1549,
      "grad_norm": 1.786152958869934,
      "learning_rate": 4.627249357326478e-06,
      "epoch": 2.9433962264150946,
      "step": 391
    },
    {
      "loss": 1.1607,
      "grad_norm": 1.8260661363601685,
      "learning_rate": 4.113110539845758e-06,
      "epoch": 2.950943396226415,
      "step": 392
    },
    {
      "loss": 1.1636,
      "grad_norm": 1.8863811492919922,
      "learning_rate": 3.598971722365039e-06,
      "epoch": 2.958490566037736,
      "step": 393
    },
    {
      "loss": 1.2662,
      "grad_norm": 1.9921797513961792,
      "learning_rate": 3.084832904884319e-06,
      "epoch": 2.9660377358490564,
      "step": 394
    },
    {
      "loss": 1.1846,
      "grad_norm": 1.8196779489517212,
      "learning_rate": 2.570694087403599e-06,
      "epoch": 2.9735849056603776,
      "step": 395
    },
    {
      "loss": 1.0982,
      "grad_norm": 1.8596806526184082,
      "learning_rate": 2.056555269922879e-06,
      "epoch": 2.981132075471698,
      "step": 396
    },
    {
      "loss": 1.1502,
      "grad_norm": 1.8511828184127808,
      "learning_rate": 1.5424164524421595e-06,
      "epoch": 2.988679245283019,
      "step": 397
    },
    {
      "loss": 1.1631,
      "grad_norm": 1.8692783117294312,
      "learning_rate": 1.0282776349614395e-06,
      "epoch": 2.9962264150943394,
      "step": 398
    },
    {
      "loss": 1.2232,
      "grad_norm": 2.6438894271850586,
      "learning_rate": 5.141388174807198e-07,
      "epoch": 3.0,
      "step": 399
    },
    {
      "eval_loss": 1.6418570280075073,
      "eval_runtime": 10.8331,
      "eval_samples_per_second": 48.924,
      "eval_steps_per_second": 6.185,
      "epoch": 3.0,
      "step": 399
    },
    {
      "train_runtime": 640.2172,
      "train_samples_per_second": 19.868,
      "train_steps_per_second": 0.623,
      "total_flos": 1.38904670306304e+17,
      "train_loss": 1.559492009922974,
      "epoch": 3.0,
      "step": 399
    }
  ],
  "dataset_info": {
    "train_size": 4240,
    "val_size": 530,
    "total_size": 4770,
    "train_val_split": "4240/530",
    "data_dir": "./datasets/datasets_v4",
    "sample_train_example": {
      "text_length": 208,
      "keys": [
        "text",
        "labels"
      ]
    }
  },
  "model_stats": {
    "trainable_params_detail": [
      {
        "name": "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight",
        "shape": [
          16,
          3584
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 57344
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight",
        "shape": [
          3584,
          16
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 57344
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight",
        "shape": [
          16,
          3584
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 57344
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight",
        "shape": [
          512,
          16
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 8192
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight",
        "shape": [
          16,
          3584
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 57344
      }
    ],
    "total_trainable_params_verified": 392
  },
  "training_start_time": "2025-07-12T14:45:00.008186",
  "training_end_time": "2025-07-12T14:55:40.501407",
  "total_training_duration": "0:10:40.493221"
}