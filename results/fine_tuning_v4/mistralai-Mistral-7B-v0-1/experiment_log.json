{
  "experiment_id": ".-local_models-mistralai-Mistral-7B-v0.1_20250712_150644",
  "model_name": "./local_models/mistralai-Mistral-7B-v0.1",
  "timestamp": "2025-07-12T15:06:44.498465",
  "config": {
    "lora_config": {
      "r": 16,
      "lora_alpha": 32,
      "target_modules": "{'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj'}",
      "lora_dropout": 0.1,
      "bias": "none",
      "task_type": "CAUSAL_LM"
    },
    "training_args": {
      "per_device_train_batch_size": 8,
      "per_device_eval_batch_size": 8,
      "gradient_accumulation_steps": 4,
      "effective_batch_size": 32,
      "num_train_epochs": 3,
      "learning_rate": 0.0002,
      "fp16": true,
      "gradient_checkpointing": false,
      "max_grad_norm": 1.0,
      "optimizer": "adamw_torch",
      "warmup_steps": 10,
      "eval_strategy": "epoch",
      "save_strategy": "epoch",
      "logging_steps": 1
    },
    "estimated_total_steps": 396
  },
  "metrics": {
    "training_successful": true,
    "final_train_loss": 1.0157097651247393,
    "train_runtime_seconds": 628.767,
    "train_runtime_minutes": 10.479450000000002,
    "train_samples_per_second": 20.23,
    "train_steps_per_second": 0.635,
    "epoch": 3.0,
    "global_step": 399,
    "total_flos": 1.3974698786291712e+17,
    "training_progression": {
      "initial_train_loss": 3.6521,
      "final_train_loss": 0.654,
      "loss_improvement": 2.9981,
      "best_eval_loss": 1.166236162185669,
      "final_eval_loss": 1.2668596506118774,
      "peak_learning_rate": 0.0002,
      "final_learning_rate": 5.141388174807198e-07
    },
    "final_gpu_memory": {
      "allocated_gb": 14.020028591156006,
      "reserved_gb": 32.251953125,
      "max_allocated_gb": 34.05824327468872
    }
  },
  "system_info": {
    "gpu_available": true,
    "gpu_count": 1,
    "gpu_info": [],
    "cuda_version": "12.6",
    "torch_version": "2.7.1+cu126",
    "python_version": "3.9.21",
    "cpu_count": 256,
    "memory_total_gb": 1006.93,
    "memory_available_gb": 983.4
  },
  "training_history": [
    {
      "loss": 3.6521,
      "grad_norm": 7.435201644897461,
      "learning_rate": 0.0,
      "epoch": 0.007547169811320755,
      "step": 1
    },
    {
      "loss": 3.6194,
      "grad_norm": 8.124125480651855,
      "learning_rate": 2e-05,
      "epoch": 0.01509433962264151,
      "step": 2
    },
    {
      "loss": 3.5207,
      "grad_norm": 6.345755577087402,
      "learning_rate": 4e-05,
      "epoch": 0.022641509433962263,
      "step": 3
    },
    {
      "loss": 3.2095,
      "grad_norm": 6.459449291229248,
      "learning_rate": 6e-05,
      "epoch": 0.03018867924528302,
      "step": 4
    },
    {
      "loss": 2.8885,
      "grad_norm": 4.957934856414795,
      "learning_rate": 8e-05,
      "epoch": 0.03773584905660377,
      "step": 5
    },
    {
      "loss": 2.6385,
      "grad_norm": 8.33602523803711,
      "learning_rate": 0.0001,
      "epoch": 0.045283018867924525,
      "step": 6
    },
    {
      "loss": 2.4279,
      "grad_norm": 4.864644527435303,
      "learning_rate": 0.00012,
      "epoch": 0.052830188679245285,
      "step": 7
    },
    {
      "loss": 2.2651,
      "grad_norm": 5.944729328155518,
      "learning_rate": 0.00014,
      "epoch": 0.06037735849056604,
      "step": 8
    },
    {
      "loss": 2.1728,
      "grad_norm": 3.531334400177002,
      "learning_rate": 0.00016,
      "epoch": 0.06792452830188679,
      "step": 9
    },
    {
      "loss": 2.0983,
      "grad_norm": 2.487457513809204,
      "learning_rate": 0.00018,
      "epoch": 0.07547169811320754,
      "step": 10
    },
    {
      "loss": 2.0554,
      "grad_norm": 2.2315566539764404,
      "learning_rate": 0.0002,
      "epoch": 0.0830188679245283,
      "step": 11
    },
    {
      "loss": 1.9931,
      "grad_norm": 2.1583783626556396,
      "learning_rate": 0.0001994858611825193,
      "epoch": 0.09056603773584905,
      "step": 12
    },
    {
      "loss": 1.9104,
      "grad_norm": 1.8806138038635254,
      "learning_rate": 0.00019897172236503857,
      "epoch": 0.09811320754716982,
      "step": 13
    },
    {
      "loss": 1.9174,
      "grad_norm": 1.9429625272750854,
      "learning_rate": 0.00019845758354755785,
      "epoch": 0.10566037735849057,
      "step": 14
    },
    {
      "loss": 1.8337,
      "grad_norm": 1.9935381412506104,
      "learning_rate": 0.00019794344473007713,
      "epoch": 0.11320754716981132,
      "step": 15
    },
    {
      "loss": 1.7952,
      "grad_norm": 1.9907176494598389,
      "learning_rate": 0.0001974293059125964,
      "epoch": 0.12075471698113208,
      "step": 16
    },
    {
      "loss": 1.7426,
      "grad_norm": 2.0335280895233154,
      "learning_rate": 0.0001969151670951157,
      "epoch": 0.12830188679245283,
      "step": 17
    },
    {
      "loss": 1.6886,
      "grad_norm": 2.2617533206939697,
      "learning_rate": 0.00019640102827763497,
      "epoch": 0.13584905660377358,
      "step": 18
    },
    {
      "loss": 1.7555,
      "grad_norm": 2.2018730640411377,
      "learning_rate": 0.00019588688946015425,
      "epoch": 0.14339622641509434,
      "step": 19
    },
    {
      "loss": 1.6602,
      "grad_norm": 2.7246458530426025,
      "learning_rate": 0.00019537275064267353,
      "epoch": 0.1509433962264151,
      "step": 20
    },
    {
      "loss": 1.5923,
      "grad_norm": 3.2666103839874268,
      "learning_rate": 0.00019485861182519281,
      "epoch": 0.15849056603773584,
      "step": 21
    },
    {
      "loss": 1.6924,
      "grad_norm": 5.002918720245361,
      "learning_rate": 0.0001943444730077121,
      "epoch": 0.1660377358490566,
      "step": 22
    },
    {
      "loss": 1.5428,
      "grad_norm": 3.9343461990356445,
      "learning_rate": 0.00019383033419023138,
      "epoch": 0.17358490566037735,
      "step": 23
    },
    {
      "loss": 1.5182,
      "grad_norm": 8.499701499938965,
      "learning_rate": 0.00019331619537275066,
      "epoch": 0.1811320754716981,
      "step": 24
    },
    {
      "loss": 1.5049,
      "grad_norm": 2.188978433609009,
      "learning_rate": 0.00019280205655526994,
      "epoch": 0.18867924528301888,
      "step": 25
    },
    {
      "loss": 1.507,
      "grad_norm": 1.8573849201202393,
      "learning_rate": 0.0001922879177377892,
      "epoch": 0.19622641509433963,
      "step": 26
    },
    {
      "loss": 1.4799,
      "grad_norm": 1.9573835134506226,
      "learning_rate": 0.0001917737789203085,
      "epoch": 0.2037735849056604,
      "step": 27
    },
    {
      "loss": 1.514,
      "grad_norm": 1.869933009147644,
      "learning_rate": 0.00019125964010282778,
      "epoch": 0.21132075471698114,
      "step": 28
    },
    {
      "loss": 1.4527,
      "grad_norm": 1.9094334840774536,
      "learning_rate": 0.00019074550128534706,
      "epoch": 0.2188679245283019,
      "step": 29
    },
    {
      "loss": 1.4972,
      "grad_norm": 1.9717878103256226,
      "learning_rate": 0.00019023136246786634,
      "epoch": 0.22641509433962265,
      "step": 30
    },
    {
      "loss": 1.4412,
      "grad_norm": 1.9156179428100586,
      "learning_rate": 0.00018971722365038562,
      "epoch": 0.2339622641509434,
      "step": 31
    },
    {
      "loss": 1.4166,
      "grad_norm": 1.8315343856811523,
      "learning_rate": 0.0001892030848329049,
      "epoch": 0.24150943396226415,
      "step": 32
    },
    {
      "loss": 1.4716,
      "grad_norm": 1.8015611171722412,
      "learning_rate": 0.00018868894601542418,
      "epoch": 0.2490566037735849,
      "step": 33
    },
    {
      "loss": 1.4211,
      "grad_norm": 1.8081920146942139,
      "learning_rate": 0.00018817480719794346,
      "epoch": 0.25660377358490566,
      "step": 34
    },
    {
      "loss": 1.3981,
      "grad_norm": 1.6802908182144165,
      "learning_rate": 0.0001876606683804627,
      "epoch": 0.2641509433962264,
      "step": 35
    },
    {
      "loss": 1.3734,
      "grad_norm": 1.6720914840698242,
      "learning_rate": 0.00018714652956298202,
      "epoch": 0.27169811320754716,
      "step": 36
    },
    {
      "loss": 1.4162,
      "grad_norm": 1.840710163116455,
      "learning_rate": 0.0001866323907455013,
      "epoch": 0.2792452830188679,
      "step": 37
    },
    {
      "loss": 1.4056,
      "grad_norm": 1.8129185438156128,
      "learning_rate": 0.00018611825192802058,
      "epoch": 0.28679245283018867,
      "step": 38
    },
    {
      "loss": 1.3768,
      "grad_norm": 1.7368439435958862,
      "learning_rate": 0.00018560411311053984,
      "epoch": 0.2943396226415094,
      "step": 39
    },
    {
      "loss": 1.3965,
      "grad_norm": 1.5529299974441528,
      "learning_rate": 0.00018508997429305914,
      "epoch": 0.3018867924528302,
      "step": 40
    },
    {
      "loss": 1.335,
      "grad_norm": 1.8135310411453247,
      "learning_rate": 0.00018457583547557842,
      "epoch": 0.30943396226415093,
      "step": 41
    },
    {
      "loss": 1.4515,
      "grad_norm": 3.552466869354248,
      "learning_rate": 0.0001840616966580977,
      "epoch": 0.3169811320754717,
      "step": 42
    },
    {
      "loss": 1.3847,
      "grad_norm": 2.0484561920166016,
      "learning_rate": 0.00018354755784061696,
      "epoch": 0.32452830188679244,
      "step": 43
    },
    {
      "loss": 1.3296,
      "grad_norm": 1.6068869829177856,
      "learning_rate": 0.00018303341902313626,
      "epoch": 0.3320754716981132,
      "step": 44
    },
    {
      "loss": 1.3483,
      "grad_norm": 1.4650014638900757,
      "learning_rate": 0.00018251928020565555,
      "epoch": 0.33962264150943394,
      "step": 45
    },
    {
      "loss": 1.3571,
      "grad_norm": 1.6353325843811035,
      "learning_rate": 0.00018200514138817483,
      "epoch": 0.3471698113207547,
      "step": 46
    },
    {
      "loss": 1.3448,
      "grad_norm": 1.510746717453003,
      "learning_rate": 0.00018149100257069408,
      "epoch": 0.35471698113207545,
      "step": 47
    },
    {
      "loss": 1.3806,
      "grad_norm": 1.6188594102859497,
      "learning_rate": 0.00018097686375321336,
      "epoch": 0.3622641509433962,
      "step": 48
    },
    {
      "loss": 1.2775,
      "grad_norm": 1.6555263996124268,
      "learning_rate": 0.00018046272493573267,
      "epoch": 0.36981132075471695,
      "step": 49
    },
    {
      "loss": 1.2948,
      "grad_norm": 1.4978678226470947,
      "learning_rate": 0.00017994858611825195,
      "epoch": 0.37735849056603776,
      "step": 50
    },
    {
      "loss": 1.3186,
      "grad_norm": 1.6160175800323486,
      "learning_rate": 0.0001794344473007712,
      "epoch": 0.3849056603773585,
      "step": 51
    },
    {
      "loss": 1.3467,
      "grad_norm": 1.4571813344955444,
      "learning_rate": 0.00017892030848329048,
      "epoch": 0.39245283018867927,
      "step": 52
    },
    {
      "loss": 1.3418,
      "grad_norm": 1.4701759815216064,
      "learning_rate": 0.0001784061696658098,
      "epoch": 0.4,
      "step": 53
    },
    {
      "loss": 1.2523,
      "grad_norm": 1.3786907196044922,
      "learning_rate": 0.00017789203084832907,
      "epoch": 0.4075471698113208,
      "step": 54
    },
    {
      "loss": 1.3129,
      "grad_norm": 1.6111361980438232,
      "learning_rate": 0.00017737789203084832,
      "epoch": 0.41509433962264153,
      "step": 55
    },
    {
      "loss": 1.2871,
      "grad_norm": 1.5038011074066162,
      "learning_rate": 0.0001768637532133676,
      "epoch": 0.4226415094339623,
      "step": 56
    },
    {
      "loss": 1.2148,
      "grad_norm": 1.4673631191253662,
      "learning_rate": 0.0001763496143958869,
      "epoch": 0.43018867924528303,
      "step": 57
    },
    {
      "loss": 1.3232,
      "grad_norm": 1.5974305868148804,
      "learning_rate": 0.0001758354755784062,
      "epoch": 0.4377358490566038,
      "step": 58
    },
    {
      "loss": 1.3125,
      "grad_norm": 1.583763837814331,
      "learning_rate": 0.00017532133676092547,
      "epoch": 0.44528301886792454,
      "step": 59
    },
    {
      "loss": 1.3142,
      "grad_norm": 1.451283574104309,
      "learning_rate": 0.00017480719794344473,
      "epoch": 0.4528301886792453,
      "step": 60
    },
    {
      "loss": 1.2927,
      "grad_norm": 1.4628934860229492,
      "learning_rate": 0.000174293059125964,
      "epoch": 0.46037735849056605,
      "step": 61
    },
    {
      "loss": 1.3833,
      "grad_norm": 1.4975883960723877,
      "learning_rate": 0.0001737789203084833,
      "epoch": 0.4679245283018868,
      "step": 62
    },
    {
      "loss": 1.2797,
      "grad_norm": 1.3556774854660034,
      "learning_rate": 0.0001732647814910026,
      "epoch": 0.47547169811320755,
      "step": 63
    },
    {
      "loss": 1.2928,
      "grad_norm": 1.445556402206421,
      "learning_rate": 0.00017275064267352185,
      "epoch": 0.4830188679245283,
      "step": 64
    },
    {
      "loss": 1.3111,
      "grad_norm": 1.3662333488464355,
      "learning_rate": 0.00017223650385604113,
      "epoch": 0.49056603773584906,
      "step": 65
    },
    {
      "loss": 1.3528,
      "grad_norm": 1.3990422487258911,
      "learning_rate": 0.00017172236503856043,
      "epoch": 0.4981132075471698,
      "step": 66
    },
    {
      "loss": 1.2619,
      "grad_norm": 1.3504568338394165,
      "learning_rate": 0.00017120822622107972,
      "epoch": 0.5056603773584906,
      "step": 67
    },
    {
      "loss": 1.2961,
      "grad_norm": 1.34306800365448,
      "learning_rate": 0.00017069408740359897,
      "epoch": 0.5132075471698113,
      "step": 68
    },
    {
      "loss": 1.261,
      "grad_norm": 1.4960577487945557,
      "learning_rate": 0.00017017994858611825,
      "epoch": 0.5207547169811321,
      "step": 69
    },
    {
      "loss": 1.3116,
      "grad_norm": 1.440544605255127,
      "learning_rate": 0.00016966580976863753,
      "epoch": 0.5283018867924528,
      "step": 70
    },
    {
      "loss": 1.3037,
      "grad_norm": 1.441185474395752,
      "learning_rate": 0.00016915167095115684,
      "epoch": 0.5358490566037736,
      "step": 71
    },
    {
      "loss": 1.2614,
      "grad_norm": 1.4597474336624146,
      "learning_rate": 0.0001686375321336761,
      "epoch": 0.5433962264150943,
      "step": 72
    },
    {
      "loss": 1.3102,
      "grad_norm": 1.3176363706588745,
      "learning_rate": 0.00016812339331619537,
      "epoch": 0.5509433962264151,
      "step": 73
    },
    {
      "loss": 1.2563,
      "grad_norm": 1.2990485429763794,
      "learning_rate": 0.00016760925449871465,
      "epoch": 0.5584905660377358,
      "step": 74
    },
    {
      "loss": 1.2167,
      "grad_norm": 1.3607426881790161,
      "learning_rate": 0.00016709511568123396,
      "epoch": 0.5660377358490566,
      "step": 75
    },
    {
      "loss": 1.2993,
      "grad_norm": 1.3581225872039795,
      "learning_rate": 0.0001665809768637532,
      "epoch": 0.5735849056603773,
      "step": 76
    },
    {
      "loss": 1.2504,
      "grad_norm": 1.4446009397506714,
      "learning_rate": 0.0001660668380462725,
      "epoch": 0.5811320754716981,
      "step": 77
    },
    {
      "loss": 1.2309,
      "grad_norm": 1.5711019039154053,
      "learning_rate": 0.00016555269922879177,
      "epoch": 0.5886792452830188,
      "step": 78
    },
    {
      "loss": 1.2534,
      "grad_norm": 1.4063332080841064,
      "learning_rate": 0.00016503856041131108,
      "epoch": 0.5962264150943396,
      "step": 79
    },
    {
      "loss": 1.243,
      "grad_norm": 1.469097375869751,
      "learning_rate": 0.00016452442159383033,
      "epoch": 0.6037735849056604,
      "step": 80
    },
    {
      "loss": 1.2618,
      "grad_norm": 1.5251253843307495,
      "learning_rate": 0.00016401028277634961,
      "epoch": 0.6113207547169811,
      "step": 81
    },
    {
      "loss": 1.2563,
      "grad_norm": 1.4025542736053467,
      "learning_rate": 0.0001634961439588689,
      "epoch": 0.6188679245283019,
      "step": 82
    },
    {
      "loss": 1.2037,
      "grad_norm": 1.2707829475402832,
      "learning_rate": 0.00016298200514138818,
      "epoch": 0.6264150943396226,
      "step": 83
    },
    {
      "loss": 1.2327,
      "grad_norm": 1.3339635133743286,
      "learning_rate": 0.00016246786632390746,
      "epoch": 0.6339622641509434,
      "step": 84
    },
    {
      "loss": 1.2524,
      "grad_norm": 1.3277238607406616,
      "learning_rate": 0.00016195372750642674,
      "epoch": 0.6415094339622641,
      "step": 85
    },
    {
      "loss": 1.3692,
      "grad_norm": 1.483864426612854,
      "learning_rate": 0.00016143958868894602,
      "epoch": 0.6490566037735849,
      "step": 86
    },
    {
      "loss": 1.2619,
      "grad_norm": 1.39090096950531,
      "learning_rate": 0.0001609254498714653,
      "epoch": 0.6566037735849056,
      "step": 87
    },
    {
      "loss": 1.2246,
      "grad_norm": 1.392425298690796,
      "learning_rate": 0.0001604113110539846,
      "epoch": 0.6641509433962264,
      "step": 88
    },
    {
      "loss": 1.2413,
      "grad_norm": 1.321758508682251,
      "learning_rate": 0.00015989717223650386,
      "epoch": 0.6716981132075471,
      "step": 89
    },
    {
      "loss": 1.272,
      "grad_norm": 1.287583827972412,
      "learning_rate": 0.00015938303341902314,
      "epoch": 0.6792452830188679,
      "step": 90
    },
    {
      "loss": 1.2218,
      "grad_norm": 1.2651888132095337,
      "learning_rate": 0.00015886889460154242,
      "epoch": 0.6867924528301886,
      "step": 91
    },
    {
      "loss": 1.2774,
      "grad_norm": 1.3706294298171997,
      "learning_rate": 0.00015835475578406173,
      "epoch": 0.6943396226415094,
      "step": 92
    },
    {
      "loss": 1.2552,
      "grad_norm": 1.2776412963867188,
      "learning_rate": 0.00015784061696658098,
      "epoch": 0.7018867924528301,
      "step": 93
    },
    {
      "loss": 1.2812,
      "grad_norm": 1.3124593496322632,
      "learning_rate": 0.00015732647814910026,
      "epoch": 0.7094339622641509,
      "step": 94
    },
    {
      "loss": 1.216,
      "grad_norm": 1.2974908351898193,
      "learning_rate": 0.00015681233933161954,
      "epoch": 0.7169811320754716,
      "step": 95
    },
    {
      "loss": 1.251,
      "grad_norm": 1.4020918607711792,
      "learning_rate": 0.00015629820051413882,
      "epoch": 0.7245283018867924,
      "step": 96
    },
    {
      "loss": 1.2479,
      "grad_norm": 1.3173575401306152,
      "learning_rate": 0.0001557840616966581,
      "epoch": 0.7320754716981132,
      "step": 97
    },
    {
      "loss": 1.2129,
      "grad_norm": 1.2942845821380615,
      "learning_rate": 0.00015526992287917738,
      "epoch": 0.7396226415094339,
      "step": 98
    },
    {
      "loss": 1.1436,
      "grad_norm": 1.3502408266067505,
      "learning_rate": 0.00015475578406169666,
      "epoch": 0.7471698113207547,
      "step": 99
    },
    {
      "loss": 1.2445,
      "grad_norm": 1.2907986640930176,
      "learning_rate": 0.00015424164524421594,
      "epoch": 0.7547169811320755,
      "step": 100
    },
    {
      "loss": 1.1188,
      "grad_norm": 1.286708950996399,
      "learning_rate": 0.00015372750642673522,
      "epoch": 0.7622641509433963,
      "step": 101
    },
    {
      "loss": 1.2283,
      "grad_norm": 1.2786113023757935,
      "learning_rate": 0.0001532133676092545,
      "epoch": 0.769811320754717,
      "step": 102
    },
    {
      "loss": 1.2568,
      "grad_norm": 1.320180892944336,
      "learning_rate": 0.00015269922879177378,
      "epoch": 0.7773584905660378,
      "step": 103
    },
    {
      "loss": 1.2337,
      "grad_norm": 1.3927370309829712,
      "learning_rate": 0.00015218508997429307,
      "epoch": 0.7849056603773585,
      "step": 104
    },
    {
      "loss": 1.1902,
      "grad_norm": 1.3315589427947998,
      "learning_rate": 0.00015167095115681235,
      "epoch": 0.7924528301886793,
      "step": 105
    },
    {
      "loss": 1.2957,
      "grad_norm": 1.2645288705825806,
      "learning_rate": 0.00015115681233933163,
      "epoch": 0.8,
      "step": 106
    },
    {
      "loss": 1.2969,
      "grad_norm": 1.2939932346343994,
      "learning_rate": 0.0001506426735218509,
      "epoch": 0.8075471698113208,
      "step": 107
    },
    {
      "loss": 1.2009,
      "grad_norm": 1.207280158996582,
      "learning_rate": 0.0001501285347043702,
      "epoch": 0.8150943396226416,
      "step": 108
    },
    {
      "loss": 1.2297,
      "grad_norm": 1.2352544069290161,
      "learning_rate": 0.00014961439588688947,
      "epoch": 0.8226415094339623,
      "step": 109
    },
    {
      "loss": 1.1827,
      "grad_norm": 1.2434842586517334,
      "learning_rate": 0.00014910025706940875,
      "epoch": 0.8301886792452831,
      "step": 110
    },
    {
      "loss": 1.2453,
      "grad_norm": 1.3384766578674316,
      "learning_rate": 0.00014858611825192803,
      "epoch": 0.8377358490566038,
      "step": 111
    },
    {
      "loss": 1.189,
      "grad_norm": 1.2223204374313354,
      "learning_rate": 0.0001480719794344473,
      "epoch": 0.8452830188679246,
      "step": 112
    },
    {
      "loss": 1.1889,
      "grad_norm": 1.2605167627334595,
      "learning_rate": 0.0001475578406169666,
      "epoch": 0.8528301886792453,
      "step": 113
    },
    {
      "loss": 1.2089,
      "grad_norm": 1.349860668182373,
      "learning_rate": 0.00014704370179948587,
      "epoch": 0.8603773584905661,
      "step": 114
    },
    {
      "loss": 1.1738,
      "grad_norm": 1.2541415691375732,
      "learning_rate": 0.00014652956298200515,
      "epoch": 0.8679245283018868,
      "step": 115
    },
    {
      "loss": 1.2301,
      "grad_norm": 1.3497296571731567,
      "learning_rate": 0.00014601542416452443,
      "epoch": 0.8754716981132076,
      "step": 116
    },
    {
      "loss": 1.1677,
      "grad_norm": 1.2476121187210083,
      "learning_rate": 0.0001455012853470437,
      "epoch": 0.8830188679245283,
      "step": 117
    },
    {
      "loss": 1.2595,
      "grad_norm": 1.371078610420227,
      "learning_rate": 0.000144987146529563,
      "epoch": 0.8905660377358491,
      "step": 118
    },
    {
      "loss": 1.2092,
      "grad_norm": 1.2557255029678345,
      "learning_rate": 0.00014447300771208227,
      "epoch": 0.8981132075471698,
      "step": 119
    },
    {
      "loss": 1.1822,
      "grad_norm": 1.2338021993637085,
      "learning_rate": 0.00014395886889460155,
      "epoch": 0.9056603773584906,
      "step": 120
    },
    {
      "loss": 1.2207,
      "grad_norm": 1.2751542329788208,
      "learning_rate": 0.00014344473007712083,
      "epoch": 0.9132075471698113,
      "step": 121
    },
    {
      "loss": 1.2051,
      "grad_norm": 1.2230724096298218,
      "learning_rate": 0.0001429305912596401,
      "epoch": 0.9207547169811321,
      "step": 122
    },
    {
      "loss": 1.1608,
      "grad_norm": 1.2104297876358032,
      "learning_rate": 0.0001424164524421594,
      "epoch": 0.9283018867924528,
      "step": 123
    },
    {
      "loss": 1.2598,
      "grad_norm": 1.255414605140686,
      "learning_rate": 0.00014190231362467867,
      "epoch": 0.9358490566037736,
      "step": 124
    },
    {
      "loss": 1.2279,
      "grad_norm": 1.2402294874191284,
      "learning_rate": 0.00014138817480719795,
      "epoch": 0.9433962264150944,
      "step": 125
    },
    {
      "loss": 1.175,
      "grad_norm": 1.1767518520355225,
      "learning_rate": 0.00014087403598971724,
      "epoch": 0.9509433962264151,
      "step": 126
    },
    {
      "loss": 1.1321,
      "grad_norm": 1.2749766111373901,
      "learning_rate": 0.00014035989717223652,
      "epoch": 0.9584905660377359,
      "step": 127
    },
    {
      "loss": 1.1965,
      "grad_norm": 1.2466641664505005,
      "learning_rate": 0.0001398457583547558,
      "epoch": 0.9660377358490566,
      "step": 128
    },
    {
      "loss": 1.1794,
      "grad_norm": 1.2216815948486328,
      "learning_rate": 0.00013933161953727508,
      "epoch": 0.9735849056603774,
      "step": 129
    },
    {
      "loss": 1.161,
      "grad_norm": 1.2664538621902466,
      "learning_rate": 0.00013881748071979436,
      "epoch": 0.9811320754716981,
      "step": 130
    },
    {
      "loss": 1.1502,
      "grad_norm": 1.3559129238128662,
      "learning_rate": 0.0001383033419023136,
      "epoch": 0.9886792452830189,
      "step": 131
    },
    {
      "loss": 1.2166,
      "grad_norm": 1.2029565572738647,
      "learning_rate": 0.00013778920308483292,
      "epoch": 0.9962264150943396,
      "step": 132
    },
    {
      "loss": 1.2267,
      "grad_norm": 1.8236706256866455,
      "learning_rate": 0.0001372750642673522,
      "epoch": 1.0,
      "step": 133
    },
    {
      "eval_loss": 1.2073590755462646,
      "eval_runtime": 10.3896,
      "eval_samples_per_second": 51.012,
      "eval_steps_per_second": 6.449,
      "epoch": 1.0,
      "step": 133
    },
    {
      "loss": 0.9652,
      "grad_norm": 1.154698371887207,
      "learning_rate": 0.00013676092544987148,
      "epoch": 1.0075471698113208,
      "step": 134
    },
    {
      "loss": 1.0116,
      "grad_norm": 1.1717909574508667,
      "learning_rate": 0.00013624678663239073,
      "epoch": 1.0150943396226415,
      "step": 135
    },
    {
      "loss": 0.9078,
      "grad_norm": 1.1572117805480957,
      "learning_rate": 0.00013573264781491004,
      "epoch": 1.0226415094339623,
      "step": 136
    },
    {
      "loss": 0.9488,
      "grad_norm": 1.3087576627731323,
      "learning_rate": 0.00013521850899742932,
      "epoch": 1.030188679245283,
      "step": 137
    },
    {
      "loss": 0.9803,
      "grad_norm": 1.3791096210479736,
      "learning_rate": 0.0001347043701799486,
      "epoch": 1.0377358490566038,
      "step": 138
    },
    {
      "loss": 0.931,
      "grad_norm": 1.35535728931427,
      "learning_rate": 0.00013419023136246785,
      "epoch": 1.0452830188679245,
      "step": 139
    },
    {
      "loss": 0.9214,
      "grad_norm": 1.4044268131256104,
      "learning_rate": 0.00013367609254498716,
      "epoch": 1.0528301886792453,
      "step": 140
    },
    {
      "loss": 0.9724,
      "grad_norm": 1.5056873559951782,
      "learning_rate": 0.00013316195372750644,
      "epoch": 1.060377358490566,
      "step": 141
    },
    {
      "loss": 1.0173,
      "grad_norm": 1.5094118118286133,
      "learning_rate": 0.00013264781491002572,
      "epoch": 1.0679245283018868,
      "step": 142
    },
    {
      "loss": 0.9588,
      "grad_norm": 1.394177794456482,
      "learning_rate": 0.00013213367609254498,
      "epoch": 1.0754716981132075,
      "step": 143
    },
    {
      "loss": 0.9327,
      "grad_norm": 1.2678030729293823,
      "learning_rate": 0.00013161953727506426,
      "epoch": 1.0830188679245283,
      "step": 144
    },
    {
      "loss": 0.9483,
      "grad_norm": 1.220996618270874,
      "learning_rate": 0.00013110539845758356,
      "epoch": 1.090566037735849,
      "step": 145
    },
    {
      "loss": 0.9802,
      "grad_norm": 2.452423095703125,
      "learning_rate": 0.00013059125964010284,
      "epoch": 1.0981132075471698,
      "step": 146
    },
    {
      "loss": 1.0272,
      "grad_norm": 1.3901010751724243,
      "learning_rate": 0.00013007712082262213,
      "epoch": 1.1056603773584905,
      "step": 147
    },
    {
      "loss": 0.8995,
      "grad_norm": 1.2688361406326294,
      "learning_rate": 0.00012956298200514138,
      "epoch": 1.1132075471698113,
      "step": 148
    },
    {
      "loss": 0.9241,
      "grad_norm": 1.409746527671814,
      "learning_rate": 0.00012904884318766069,
      "epoch": 1.120754716981132,
      "step": 149
    },
    {
      "loss": 1.0113,
      "grad_norm": 1.3254226446151733,
      "learning_rate": 0.00012853470437017997,
      "epoch": 1.1283018867924528,
      "step": 150
    },
    {
      "loss": 1.0097,
      "grad_norm": 1.320622205734253,
      "learning_rate": 0.00012802056555269925,
      "epoch": 1.1358490566037736,
      "step": 151
    },
    {
      "loss": 0.9693,
      "grad_norm": 1.3825056552886963,
      "learning_rate": 0.0001275064267352185,
      "epoch": 1.1433962264150943,
      "step": 152
    },
    {
      "loss": 0.9703,
      "grad_norm": 1.346821665763855,
      "learning_rate": 0.00012699228791773778,
      "epoch": 1.150943396226415,
      "step": 153
    },
    {
      "loss": 0.9849,
      "grad_norm": 1.3703467845916748,
      "learning_rate": 0.0001264781491002571,
      "epoch": 1.1584905660377358,
      "step": 154
    },
    {
      "loss": 0.9343,
      "grad_norm": 1.3484816551208496,
      "learning_rate": 0.00012596401028277637,
      "epoch": 1.1660377358490566,
      "step": 155
    },
    {
      "loss": 0.908,
      "grad_norm": 1.3737744092941284,
      "learning_rate": 0.00012544987146529562,
      "epoch": 1.1735849056603773,
      "step": 156
    },
    {
      "loss": 0.8971,
      "grad_norm": 1.3809881210327148,
      "learning_rate": 0.0001249357326478149,
      "epoch": 1.181132075471698,
      "step": 157
    },
    {
      "loss": 0.9642,
      "grad_norm": 1.4051077365875244,
      "learning_rate": 0.0001244215938303342,
      "epoch": 1.1886792452830188,
      "step": 158
    },
    {
      "loss": 0.8982,
      "grad_norm": 1.4965720176696777,
      "learning_rate": 0.0001239074550128535,
      "epoch": 1.1962264150943396,
      "step": 159
    },
    {
      "loss": 0.9118,
      "grad_norm": 1.4208757877349854,
      "learning_rate": 0.00012339331619537274,
      "epoch": 1.2037735849056603,
      "step": 160
    },
    {
      "loss": 0.9234,
      "grad_norm": 1.429783582687378,
      "learning_rate": 0.00012287917737789202,
      "epoch": 1.211320754716981,
      "step": 161
    },
    {
      "loss": 0.9588,
      "grad_norm": 1.5420602560043335,
      "learning_rate": 0.00012236503856041133,
      "epoch": 1.2188679245283018,
      "step": 162
    },
    {
      "loss": 0.9787,
      "grad_norm": 1.5045881271362305,
      "learning_rate": 0.0001218508997429306,
      "epoch": 1.2264150943396226,
      "step": 163
    },
    {
      "loss": 0.9976,
      "grad_norm": 1.5010550022125244,
      "learning_rate": 0.00012133676092544988,
      "epoch": 1.2339622641509433,
      "step": 164
    },
    {
      "loss": 0.9783,
      "grad_norm": 1.472108006477356,
      "learning_rate": 0.00012082262210796915,
      "epoch": 1.241509433962264,
      "step": 165
    },
    {
      "loss": 0.9701,
      "grad_norm": 1.3894085884094238,
      "learning_rate": 0.00012030848329048843,
      "epoch": 1.2490566037735849,
      "step": 166
    },
    {
      "loss": 0.952,
      "grad_norm": 1.3692346811294556,
      "learning_rate": 0.00011979434447300772,
      "epoch": 1.2566037735849056,
      "step": 167
    },
    {
      "loss": 0.9858,
      "grad_norm": 1.291438102722168,
      "learning_rate": 0.000119280205655527,
      "epoch": 1.2641509433962264,
      "step": 168
    },
    {
      "loss": 0.8937,
      "grad_norm": 1.3226087093353271,
      "learning_rate": 0.00011876606683804628,
      "epoch": 1.271698113207547,
      "step": 169
    },
    {
      "loss": 0.929,
      "grad_norm": 1.3019211292266846,
      "learning_rate": 0.00011825192802056555,
      "epoch": 1.2792452830188679,
      "step": 170
    },
    {
      "loss": 0.9714,
      "grad_norm": 1.376280426979065,
      "learning_rate": 0.00011773778920308484,
      "epoch": 1.2867924528301886,
      "step": 171
    },
    {
      "loss": 0.9888,
      "grad_norm": 1.3948229551315308,
      "learning_rate": 0.00011722365038560412,
      "epoch": 1.2943396226415094,
      "step": 172
    },
    {
      "loss": 0.8937,
      "grad_norm": 1.3052396774291992,
      "learning_rate": 0.0001167095115681234,
      "epoch": 1.3018867924528301,
      "step": 173
    },
    {
      "loss": 0.9447,
      "grad_norm": 1.3844060897827148,
      "learning_rate": 0.00011619537275064267,
      "epoch": 1.3094339622641509,
      "step": 174
    },
    {
      "loss": 0.9604,
      "grad_norm": 1.3855654001235962,
      "learning_rate": 0.00011568123393316196,
      "epoch": 1.3169811320754716,
      "step": 175
    },
    {
      "loss": 0.9176,
      "grad_norm": 1.3219629526138306,
      "learning_rate": 0.00011516709511568124,
      "epoch": 1.3245283018867924,
      "step": 176
    },
    {
      "loss": 0.9741,
      "grad_norm": 1.428922414779663,
      "learning_rate": 0.00011465295629820053,
      "epoch": 1.3320754716981131,
      "step": 177
    },
    {
      "loss": 0.9197,
      "grad_norm": 1.3363828659057617,
      "learning_rate": 0.00011413881748071979,
      "epoch": 1.3396226415094339,
      "step": 178
    },
    {
      "loss": 0.9225,
      "grad_norm": 1.4080677032470703,
      "learning_rate": 0.00011362467866323907,
      "epoch": 1.3471698113207546,
      "step": 179
    },
    {
      "loss": 0.9252,
      "grad_norm": 1.4284861087799072,
      "learning_rate": 0.00011311053984575837,
      "epoch": 1.3547169811320754,
      "step": 180
    },
    {
      "loss": 0.9104,
      "grad_norm": 1.370656132698059,
      "learning_rate": 0.00011259640102827765,
      "epoch": 1.3622641509433961,
      "step": 181
    },
    {
      "loss": 0.9616,
      "grad_norm": 1.433681607246399,
      "learning_rate": 0.00011208226221079691,
      "epoch": 1.369811320754717,
      "step": 182
    },
    {
      "loss": 0.92,
      "grad_norm": 1.4083348512649536,
      "learning_rate": 0.0001115681233933162,
      "epoch": 1.3773584905660377,
      "step": 183
    },
    {
      "loss": 0.9542,
      "grad_norm": 1.4018793106079102,
      "learning_rate": 0.00011105398457583549,
      "epoch": 1.3849056603773584,
      "step": 184
    },
    {
      "loss": 0.967,
      "grad_norm": 1.4201881885528564,
      "learning_rate": 0.00011053984575835477,
      "epoch": 1.3924528301886792,
      "step": 185
    },
    {
      "loss": 0.9759,
      "grad_norm": 1.4880133867263794,
      "learning_rate": 0.00011002570694087404,
      "epoch": 1.4,
      "step": 186
    },
    {
      "loss": 1.0016,
      "grad_norm": 1.4786145687103271,
      "learning_rate": 0.00010951156812339332,
      "epoch": 1.4075471698113207,
      "step": 187
    },
    {
      "loss": 0.9605,
      "grad_norm": 1.4807101488113403,
      "learning_rate": 0.0001089974293059126,
      "epoch": 1.4150943396226414,
      "step": 188
    },
    {
      "loss": 0.9386,
      "grad_norm": 1.3971935510635376,
      "learning_rate": 0.00010848329048843189,
      "epoch": 1.4226415094339622,
      "step": 189
    },
    {
      "loss": 0.9842,
      "grad_norm": 1.400838851928711,
      "learning_rate": 0.00010796915167095116,
      "epoch": 1.430188679245283,
      "step": 190
    },
    {
      "loss": 0.8897,
      "grad_norm": 1.2702217102050781,
      "learning_rate": 0.00010745501285347044,
      "epoch": 1.4377358490566037,
      "step": 191
    },
    {
      "loss": 0.954,
      "grad_norm": 1.4276446104049683,
      "learning_rate": 0.00010694087403598972,
      "epoch": 1.4452830188679244,
      "step": 192
    },
    {
      "loss": 0.9296,
      "grad_norm": 1.3542993068695068,
      "learning_rate": 0.00010642673521850901,
      "epoch": 1.4528301886792452,
      "step": 193
    },
    {
      "loss": 0.9531,
      "grad_norm": 1.370550274848938,
      "learning_rate": 0.00010591259640102828,
      "epoch": 1.460377358490566,
      "step": 194
    },
    {
      "loss": 0.921,
      "grad_norm": 1.4417227506637573,
      "learning_rate": 0.00010539845758354756,
      "epoch": 1.4679245283018867,
      "step": 195
    },
    {
      "loss": 0.8905,
      "grad_norm": 1.3752578496932983,
      "learning_rate": 0.00010488431876606684,
      "epoch": 1.4754716981132074,
      "step": 196
    },
    {
      "loss": 0.9326,
      "grad_norm": 1.3999663591384888,
      "learning_rate": 0.00010437017994858613,
      "epoch": 1.4830188679245282,
      "step": 197
    },
    {
      "loss": 0.9854,
      "grad_norm": 1.4549118280410767,
      "learning_rate": 0.00010385604113110541,
      "epoch": 1.490566037735849,
      "step": 198
    },
    {
      "loss": 0.9234,
      "grad_norm": 1.4266388416290283,
      "learning_rate": 0.00010334190231362468,
      "epoch": 1.4981132075471697,
      "step": 199
    },
    {
      "loss": 0.9532,
      "grad_norm": 1.434346318244934,
      "learning_rate": 0.00010282776349614396,
      "epoch": 1.5056603773584905,
      "step": 200
    },
    {
      "loss": 0.9175,
      "grad_norm": 1.2955867052078247,
      "learning_rate": 0.00010231362467866323,
      "epoch": 1.5132075471698112,
      "step": 201
    },
    {
      "loss": 0.9434,
      "grad_norm": 1.3608405590057373,
      "learning_rate": 0.00010179948586118254,
      "epoch": 1.520754716981132,
      "step": 202
    },
    {
      "loss": 0.9507,
      "grad_norm": 1.4005091190338135,
      "learning_rate": 0.0001012853470437018,
      "epoch": 1.5283018867924527,
      "step": 203
    },
    {
      "loss": 0.9291,
      "grad_norm": 1.3356488943099976,
      "learning_rate": 0.00010077120822622108,
      "epoch": 1.5358490566037735,
      "step": 204
    },
    {
      "loss": 0.9556,
      "grad_norm": 1.4144678115844727,
      "learning_rate": 0.00010025706940874035,
      "epoch": 1.5433962264150942,
      "step": 205
    },
    {
      "loss": 0.885,
      "grad_norm": 1.3452003002166748,
      "learning_rate": 9.974293059125965e-05,
      "epoch": 1.550943396226415,
      "step": 206
    },
    {
      "loss": 0.923,
      "grad_norm": 1.386170744895935,
      "learning_rate": 9.922879177377893e-05,
      "epoch": 1.5584905660377357,
      "step": 207
    },
    {
      "loss": 0.9474,
      "grad_norm": 1.4096752405166626,
      "learning_rate": 9.87146529562982e-05,
      "epoch": 1.5660377358490565,
      "step": 208
    },
    {
      "loss": 0.9276,
      "grad_norm": 1.4205842018127441,
      "learning_rate": 9.820051413881749e-05,
      "epoch": 1.5735849056603772,
      "step": 209
    },
    {
      "loss": 0.935,
      "grad_norm": 1.399619221687317,
      "learning_rate": 9.768637532133677e-05,
      "epoch": 1.581132075471698,
      "step": 210
    },
    {
      "loss": 0.971,
      "grad_norm": 1.4472142457962036,
      "learning_rate": 9.717223650385605e-05,
      "epoch": 1.5886792452830187,
      "step": 211
    },
    {
      "loss": 0.9993,
      "grad_norm": 1.494049072265625,
      "learning_rate": 9.665809768637533e-05,
      "epoch": 1.5962264150943395,
      "step": 212
    },
    {
      "loss": 0.9211,
      "grad_norm": 1.4027680158615112,
      "learning_rate": 9.61439588688946e-05,
      "epoch": 1.6037735849056602,
      "step": 213
    },
    {
      "loss": 0.9706,
      "grad_norm": 1.380792498588562,
      "learning_rate": 9.562982005141389e-05,
      "epoch": 1.611320754716981,
      "step": 214
    },
    {
      "loss": 0.8755,
      "grad_norm": 1.3655508756637573,
      "learning_rate": 9.511568123393317e-05,
      "epoch": 1.6188679245283017,
      "step": 215
    },
    {
      "loss": 0.9441,
      "grad_norm": 1.4338641166687012,
      "learning_rate": 9.460154241645245e-05,
      "epoch": 1.6264150943396225,
      "step": 216
    },
    {
      "loss": 0.9411,
      "grad_norm": 1.3969262838363647,
      "learning_rate": 9.408740359897173e-05,
      "epoch": 1.6339622641509433,
      "step": 217
    },
    {
      "loss": 0.8945,
      "grad_norm": 1.4123165607452393,
      "learning_rate": 9.357326478149101e-05,
      "epoch": 1.641509433962264,
      "step": 218
    },
    {
      "loss": 0.9942,
      "grad_norm": 1.3737510442733765,
      "learning_rate": 9.305912596401029e-05,
      "epoch": 1.6490566037735848,
      "step": 219
    },
    {
      "loss": 0.9283,
      "grad_norm": 1.4017468690872192,
      "learning_rate": 9.254498714652957e-05,
      "epoch": 1.6566037735849055,
      "step": 220
    },
    {
      "loss": 0.9344,
      "grad_norm": 1.3863075971603394,
      "learning_rate": 9.203084832904885e-05,
      "epoch": 1.6641509433962263,
      "step": 221
    },
    {
      "loss": 0.9691,
      "grad_norm": 1.420173168182373,
      "learning_rate": 9.151670951156813e-05,
      "epoch": 1.671698113207547,
      "step": 222
    },
    {
      "loss": 0.8927,
      "grad_norm": 1.3245829343795776,
      "learning_rate": 9.100257069408741e-05,
      "epoch": 1.6792452830188678,
      "step": 223
    },
    {
      "loss": 0.9585,
      "grad_norm": 1.4977225065231323,
      "learning_rate": 9.048843187660668e-05,
      "epoch": 1.6867924528301885,
      "step": 224
    },
    {
      "loss": 0.9553,
      "grad_norm": 1.4149212837219238,
      "learning_rate": 8.997429305912597e-05,
      "epoch": 1.6943396226415093,
      "step": 225
    },
    {
      "loss": 0.9339,
      "grad_norm": 1.380434513092041,
      "learning_rate": 8.946015424164524e-05,
      "epoch": 1.70188679245283,
      "step": 226
    },
    {
      "loss": 0.9578,
      "grad_norm": 1.3742483854293823,
      "learning_rate": 8.894601542416453e-05,
      "epoch": 1.7094339622641508,
      "step": 227
    },
    {
      "loss": 0.9255,
      "grad_norm": 1.350992202758789,
      "learning_rate": 8.84318766066838e-05,
      "epoch": 1.7169811320754715,
      "step": 228
    },
    {
      "loss": 0.9373,
      "grad_norm": 1.4448672533035278,
      "learning_rate": 8.79177377892031e-05,
      "epoch": 1.7245283018867923,
      "step": 229
    },
    {
      "loss": 0.9176,
      "grad_norm": 1.4093759059906006,
      "learning_rate": 8.740359897172236e-05,
      "epoch": 1.732075471698113,
      "step": 230
    },
    {
      "loss": 0.9531,
      "grad_norm": 1.4065288305282593,
      "learning_rate": 8.688946015424166e-05,
      "epoch": 1.7396226415094338,
      "step": 231
    },
    {
      "loss": 0.9625,
      "grad_norm": 1.4275342226028442,
      "learning_rate": 8.637532133676092e-05,
      "epoch": 1.7471698113207546,
      "step": 232
    },
    {
      "loss": 0.9212,
      "grad_norm": 1.3427499532699585,
      "learning_rate": 8.586118251928022e-05,
      "epoch": 1.7547169811320755,
      "step": 233
    },
    {
      "loss": 0.9193,
      "grad_norm": 1.372523546218872,
      "learning_rate": 8.534704370179948e-05,
      "epoch": 1.7622641509433963,
      "step": 234
    },
    {
      "loss": 0.9674,
      "grad_norm": 1.461439847946167,
      "learning_rate": 8.483290488431876e-05,
      "epoch": 1.769811320754717,
      "step": 235
    },
    {
      "loss": 0.9165,
      "grad_norm": 1.3825210332870483,
      "learning_rate": 8.431876606683805e-05,
      "epoch": 1.7773584905660378,
      "step": 236
    },
    {
      "loss": 0.9708,
      "grad_norm": 1.4625297784805298,
      "learning_rate": 8.380462724935733e-05,
      "epoch": 1.7849056603773585,
      "step": 237
    },
    {
      "loss": 0.9162,
      "grad_norm": 1.4415326118469238,
      "learning_rate": 8.32904884318766e-05,
      "epoch": 1.7924528301886793,
      "step": 238
    },
    {
      "loss": 0.9667,
      "grad_norm": 1.3645572662353516,
      "learning_rate": 8.277634961439589e-05,
      "epoch": 1.8,
      "step": 239
    },
    {
      "loss": 0.9536,
      "grad_norm": 1.3436810970306396,
      "learning_rate": 8.226221079691517e-05,
      "epoch": 1.8075471698113208,
      "step": 240
    },
    {
      "loss": 0.8894,
      "grad_norm": 1.3423805236816406,
      "learning_rate": 8.174807197943445e-05,
      "epoch": 1.8150943396226416,
      "step": 241
    },
    {
      "loss": 0.9186,
      "grad_norm": 1.3990215063095093,
      "learning_rate": 8.123393316195373e-05,
      "epoch": 1.8226415094339623,
      "step": 242
    },
    {
      "loss": 0.8964,
      "grad_norm": 1.3448008298873901,
      "learning_rate": 8.071979434447301e-05,
      "epoch": 1.830188679245283,
      "step": 243
    },
    {
      "loss": 0.9674,
      "grad_norm": 1.461146593093872,
      "learning_rate": 8.02056555269923e-05,
      "epoch": 1.8377358490566038,
      "step": 244
    },
    {
      "loss": 0.9469,
      "grad_norm": 1.3995120525360107,
      "learning_rate": 7.969151670951157e-05,
      "epoch": 1.8452830188679246,
      "step": 245
    },
    {
      "loss": 0.9102,
      "grad_norm": 1.4167518615722656,
      "learning_rate": 7.917737789203086e-05,
      "epoch": 1.8528301886792453,
      "step": 246
    },
    {
      "loss": 0.8885,
      "grad_norm": 1.3908028602600098,
      "learning_rate": 7.866323907455013e-05,
      "epoch": 1.860377358490566,
      "step": 247
    },
    {
      "loss": 0.8691,
      "grad_norm": 1.411172866821289,
      "learning_rate": 7.814910025706941e-05,
      "epoch": 1.8679245283018868,
      "step": 248
    },
    {
      "loss": 0.9358,
      "grad_norm": 1.4075253009796143,
      "learning_rate": 7.763496143958869e-05,
      "epoch": 1.8754716981132076,
      "step": 249
    },
    {
      "loss": 0.9327,
      "grad_norm": 1.46833074092865,
      "learning_rate": 7.712082262210797e-05,
      "epoch": 1.8830188679245283,
      "step": 250
    },
    {
      "loss": 0.9676,
      "grad_norm": 1.4624409675598145,
      "learning_rate": 7.660668380462725e-05,
      "epoch": 1.890566037735849,
      "step": 251
    },
    {
      "loss": 0.9474,
      "grad_norm": 1.4063626527786255,
      "learning_rate": 7.609254498714653e-05,
      "epoch": 1.8981132075471698,
      "step": 252
    },
    {
      "loss": 0.8576,
      "grad_norm": 1.3420257568359375,
      "learning_rate": 7.557840616966581e-05,
      "epoch": 1.9056603773584906,
      "step": 253
    },
    {
      "loss": 0.9042,
      "grad_norm": 1.438219428062439,
      "learning_rate": 7.50642673521851e-05,
      "epoch": 1.9132075471698113,
      "step": 254
    },
    {
      "loss": 0.9185,
      "grad_norm": 1.385175108909607,
      "learning_rate": 7.455012853470437e-05,
      "epoch": 1.920754716981132,
      "step": 255
    },
    {
      "loss": 0.9598,
      "grad_norm": 1.4097009897232056,
      "learning_rate": 7.403598971722365e-05,
      "epoch": 1.9283018867924528,
      "step": 256
    },
    {
      "loss": 0.9505,
      "grad_norm": 1.3984622955322266,
      "learning_rate": 7.352185089974293e-05,
      "epoch": 1.9358490566037736,
      "step": 257
    },
    {
      "loss": 0.9253,
      "grad_norm": 1.3834450244903564,
      "learning_rate": 7.300771208226222e-05,
      "epoch": 1.9433962264150944,
      "step": 258
    },
    {
      "loss": 0.8839,
      "grad_norm": 1.40491783618927,
      "learning_rate": 7.24935732647815e-05,
      "epoch": 1.950943396226415,
      "step": 259
    },
    {
      "loss": 0.902,
      "grad_norm": 1.3993295431137085,
      "learning_rate": 7.197943444730078e-05,
      "epoch": 1.9584905660377359,
      "step": 260
    },
    {
      "loss": 0.918,
      "grad_norm": 1.4042069911956787,
      "learning_rate": 7.146529562982006e-05,
      "epoch": 1.9660377358490566,
      "step": 261
    },
    {
      "loss": 0.9395,
      "grad_norm": 1.3908528089523315,
      "learning_rate": 7.095115681233934e-05,
      "epoch": 1.9735849056603774,
      "step": 262
    },
    {
      "loss": 0.9545,
      "grad_norm": 1.3816769123077393,
      "learning_rate": 7.043701799485862e-05,
      "epoch": 1.9811320754716981,
      "step": 263
    },
    {
      "loss": 0.8825,
      "grad_norm": 1.4353166818618774,
      "learning_rate": 6.99228791773779e-05,
      "epoch": 1.9886792452830189,
      "step": 264
    },
    {
      "loss": 0.9294,
      "grad_norm": 1.4415419101715088,
      "learning_rate": 6.940874035989718e-05,
      "epoch": 1.9962264150943396,
      "step": 265
    },
    {
      "loss": 0.8936,
      "grad_norm": 1.9358662366867065,
      "learning_rate": 6.889460154241646e-05,
      "epoch": 2.0,
      "step": 266
    },
    {
      "eval_loss": 1.166236162185669,
      "eval_runtime": 10.3455,
      "eval_samples_per_second": 51.23,
      "eval_steps_per_second": 6.476,
      "epoch": 2.0,
      "step": 266
    },
    {
      "loss": 0.6856,
      "grad_norm": 1.2277330160140991,
      "learning_rate": 6.838046272493574e-05,
      "epoch": 2.0075471698113208,
      "step": 267
    },
    {
      "loss": 0.7033,
      "grad_norm": 1.2857894897460938,
      "learning_rate": 6.786632390745502e-05,
      "epoch": 2.0150943396226415,
      "step": 268
    },
    {
      "loss": 0.725,
      "grad_norm": 1.2550148963928223,
      "learning_rate": 6.73521850899743e-05,
      "epoch": 2.0226415094339623,
      "step": 269
    },
    {
      "loss": 0.6784,
      "grad_norm": 1.2952995300292969,
      "learning_rate": 6.683804627249358e-05,
      "epoch": 2.030188679245283,
      "step": 270
    },
    {
      "loss": 0.6991,
      "grad_norm": 1.3573201894760132,
      "learning_rate": 6.632390745501286e-05,
      "epoch": 2.0377358490566038,
      "step": 271
    },
    {
      "loss": 0.66,
      "grad_norm": 1.3730899095535278,
      "learning_rate": 6.580976863753213e-05,
      "epoch": 2.0452830188679245,
      "step": 272
    },
    {
      "loss": 0.6299,
      "grad_norm": 1.4412288665771484,
      "learning_rate": 6.529562982005142e-05,
      "epoch": 2.0528301886792453,
      "step": 273
    },
    {
      "loss": 0.671,
      "grad_norm": 2.2764081954956055,
      "learning_rate": 6.478149100257069e-05,
      "epoch": 2.060377358490566,
      "step": 274
    },
    {
      "loss": 0.6546,
      "grad_norm": 2.100116729736328,
      "learning_rate": 6.426735218508998e-05,
      "epoch": 2.0679245283018868,
      "step": 275
    },
    {
      "loss": 0.6925,
      "grad_norm": 1.9325031042099,
      "learning_rate": 6.375321336760925e-05,
      "epoch": 2.0754716981132075,
      "step": 276
    },
    {
      "loss": 0.688,
      "grad_norm": 1.7723945379257202,
      "learning_rate": 6.323907455012854e-05,
      "epoch": 2.0830188679245283,
      "step": 277
    },
    {
      "loss": 0.6747,
      "grad_norm": 1.5990731716156006,
      "learning_rate": 6.272493573264781e-05,
      "epoch": 2.090566037735849,
      "step": 278
    },
    {
      "loss": 0.6461,
      "grad_norm": 1.4304521083831787,
      "learning_rate": 6.22107969151671e-05,
      "epoch": 2.09811320754717,
      "step": 279
    },
    {
      "loss": 0.6357,
      "grad_norm": 1.4520647525787354,
      "learning_rate": 6.169665809768637e-05,
      "epoch": 2.1056603773584905,
      "step": 280
    },
    {
      "loss": 0.6426,
      "grad_norm": 1.4748873710632324,
      "learning_rate": 6.118251928020567e-05,
      "epoch": 2.1132075471698113,
      "step": 281
    },
    {
      "loss": 0.6788,
      "grad_norm": 1.432215690612793,
      "learning_rate": 6.066838046272494e-05,
      "epoch": 2.120754716981132,
      "step": 282
    },
    {
      "loss": 0.6716,
      "grad_norm": 1.4504095315933228,
      "learning_rate": 6.015424164524421e-05,
      "epoch": 2.128301886792453,
      "step": 283
    },
    {
      "loss": 0.6643,
      "grad_norm": 1.9304344654083252,
      "learning_rate": 5.96401028277635e-05,
      "epoch": 2.1358490566037736,
      "step": 284
    },
    {
      "loss": 0.6631,
      "grad_norm": 1.4605656862258911,
      "learning_rate": 5.9125964010282774e-05,
      "epoch": 2.1433962264150943,
      "step": 285
    },
    {
      "loss": 0.6769,
      "grad_norm": 1.5579078197479248,
      "learning_rate": 5.861182519280206e-05,
      "epoch": 2.150943396226415,
      "step": 286
    },
    {
      "loss": 0.6441,
      "grad_norm": 1.5171663761138916,
      "learning_rate": 5.8097686375321335e-05,
      "epoch": 2.158490566037736,
      "step": 287
    },
    {
      "loss": 0.633,
      "grad_norm": 1.4970982074737549,
      "learning_rate": 5.758354755784062e-05,
      "epoch": 2.1660377358490566,
      "step": 288
    },
    {
      "loss": 0.6884,
      "grad_norm": 1.5944570302963257,
      "learning_rate": 5.7069408740359896e-05,
      "epoch": 2.1735849056603773,
      "step": 289
    },
    {
      "loss": 0.5931,
      "grad_norm": 1.521968960762024,
      "learning_rate": 5.655526992287918e-05,
      "epoch": 2.181132075471698,
      "step": 290
    },
    {
      "loss": 0.6848,
      "grad_norm": 1.7299097776412964,
      "learning_rate": 5.604113110539846e-05,
      "epoch": 2.188679245283019,
      "step": 291
    },
    {
      "loss": 0.6884,
      "grad_norm": 1.747899055480957,
      "learning_rate": 5.5526992287917744e-05,
      "epoch": 2.1962264150943396,
      "step": 292
    },
    {
      "loss": 0.6679,
      "grad_norm": 1.709375023841858,
      "learning_rate": 5.501285347043702e-05,
      "epoch": 2.2037735849056603,
      "step": 293
    },
    {
      "loss": 0.6689,
      "grad_norm": 1.7170000076293945,
      "learning_rate": 5.44987146529563e-05,
      "epoch": 2.211320754716981,
      "step": 294
    },
    {
      "loss": 0.6605,
      "grad_norm": 1.7023842334747314,
      "learning_rate": 5.398457583547558e-05,
      "epoch": 2.218867924528302,
      "step": 295
    },
    {
      "loss": 0.6371,
      "grad_norm": 1.5312681198120117,
      "learning_rate": 5.347043701799486e-05,
      "epoch": 2.2264150943396226,
      "step": 296
    },
    {
      "loss": 0.6529,
      "grad_norm": 1.6275936365127563,
      "learning_rate": 5.295629820051414e-05,
      "epoch": 2.2339622641509433,
      "step": 297
    },
    {
      "loss": 0.639,
      "grad_norm": 1.6025112867355347,
      "learning_rate": 5.244215938303342e-05,
      "epoch": 2.241509433962264,
      "step": 298
    },
    {
      "loss": 0.6527,
      "grad_norm": 1.6133973598480225,
      "learning_rate": 5.192802056555271e-05,
      "epoch": 2.249056603773585,
      "step": 299
    },
    {
      "loss": 0.664,
      "grad_norm": 1.6917140483856201,
      "learning_rate": 5.141388174807198e-05,
      "epoch": 2.2566037735849056,
      "step": 300
    },
    {
      "loss": 0.6773,
      "grad_norm": 1.6249561309814453,
      "learning_rate": 5.089974293059127e-05,
      "epoch": 2.2641509433962264,
      "step": 301
    },
    {
      "loss": 0.6475,
      "grad_norm": 1.5349268913269043,
      "learning_rate": 5.038560411311054e-05,
      "epoch": 2.271698113207547,
      "step": 302
    },
    {
      "loss": 0.645,
      "grad_norm": 1.6419284343719482,
      "learning_rate": 4.987146529562982e-05,
      "epoch": 2.279245283018868,
      "step": 303
    },
    {
      "loss": 0.6399,
      "grad_norm": 1.5791473388671875,
      "learning_rate": 4.93573264781491e-05,
      "epoch": 2.2867924528301886,
      "step": 304
    },
    {
      "loss": 0.6568,
      "grad_norm": 1.6002740859985352,
      "learning_rate": 4.8843187660668383e-05,
      "epoch": 2.2943396226415094,
      "step": 305
    },
    {
      "loss": 0.6093,
      "grad_norm": 1.4587674140930176,
      "learning_rate": 4.8329048843187664e-05,
      "epoch": 2.30188679245283,
      "step": 306
    },
    {
      "loss": 0.6542,
      "grad_norm": 1.5794485807418823,
      "learning_rate": 4.7814910025706944e-05,
      "epoch": 2.309433962264151,
      "step": 307
    },
    {
      "loss": 0.6539,
      "grad_norm": 1.6344192028045654,
      "learning_rate": 4.7300771208226225e-05,
      "epoch": 2.3169811320754716,
      "step": 308
    },
    {
      "loss": 0.6532,
      "grad_norm": 1.5386673212051392,
      "learning_rate": 4.6786632390745505e-05,
      "epoch": 2.3245283018867924,
      "step": 309
    },
    {
      "loss": 0.6453,
      "grad_norm": 1.6136583089828491,
      "learning_rate": 4.6272493573264786e-05,
      "epoch": 2.332075471698113,
      "step": 310
    },
    {
      "loss": 0.6643,
      "grad_norm": 1.6669830083847046,
      "learning_rate": 4.5758354755784066e-05,
      "epoch": 2.339622641509434,
      "step": 311
    },
    {
      "loss": 0.6661,
      "grad_norm": 1.7247599363327026,
      "learning_rate": 4.524421593830334e-05,
      "epoch": 2.3471698113207546,
      "step": 312
    },
    {
      "loss": 0.68,
      "grad_norm": 1.7056859731674194,
      "learning_rate": 4.473007712082262e-05,
      "epoch": 2.3547169811320754,
      "step": 313
    },
    {
      "loss": 0.6654,
      "grad_norm": 1.6922991275787354,
      "learning_rate": 4.42159383033419e-05,
      "epoch": 2.362264150943396,
      "step": 314
    },
    {
      "loss": 0.6785,
      "grad_norm": 1.728803277015686,
      "learning_rate": 4.370179948586118e-05,
      "epoch": 2.369811320754717,
      "step": 315
    },
    {
      "loss": 0.6527,
      "grad_norm": 1.616797924041748,
      "learning_rate": 4.318766066838046e-05,
      "epoch": 2.3773584905660377,
      "step": 316
    },
    {
      "loss": 0.6562,
      "grad_norm": 1.5985995531082153,
      "learning_rate": 4.267352185089974e-05,
      "epoch": 2.3849056603773584,
      "step": 317
    },
    {
      "loss": 0.6681,
      "grad_norm": 1.7021267414093018,
      "learning_rate": 4.215938303341902e-05,
      "epoch": 2.392452830188679,
      "step": 318
    },
    {
      "loss": 0.6694,
      "grad_norm": 1.6962084770202637,
      "learning_rate": 4.16452442159383e-05,
      "epoch": 2.4,
      "step": 319
    },
    {
      "loss": 0.6807,
      "grad_norm": 1.5881836414337158,
      "learning_rate": 4.1131105398457584e-05,
      "epoch": 2.4075471698113207,
      "step": 320
    },
    {
      "loss": 0.675,
      "grad_norm": 1.7253063917160034,
      "learning_rate": 4.0616966580976864e-05,
      "epoch": 2.4150943396226414,
      "step": 321
    },
    {
      "loss": 0.6454,
      "grad_norm": 1.654752254486084,
      "learning_rate": 4.010282776349615e-05,
      "epoch": 2.422641509433962,
      "step": 322
    },
    {
      "loss": 0.6593,
      "grad_norm": 1.685213565826416,
      "learning_rate": 3.958868894601543e-05,
      "epoch": 2.430188679245283,
      "step": 323
    },
    {
      "loss": 0.6315,
      "grad_norm": 1.6529383659362793,
      "learning_rate": 3.9074550128534705e-05,
      "epoch": 2.4377358490566037,
      "step": 324
    },
    {
      "loss": 0.6346,
      "grad_norm": 1.6494354009628296,
      "learning_rate": 3.8560411311053986e-05,
      "epoch": 2.4452830188679244,
      "step": 325
    },
    {
      "loss": 0.6516,
      "grad_norm": 1.5885337591171265,
      "learning_rate": 3.8046272493573266e-05,
      "epoch": 2.452830188679245,
      "step": 326
    },
    {
      "loss": 0.6764,
      "grad_norm": 1.616326928138733,
      "learning_rate": 3.753213367609255e-05,
      "epoch": 2.460377358490566,
      "step": 327
    },
    {
      "loss": 0.6386,
      "grad_norm": 1.559799313545227,
      "learning_rate": 3.701799485861183e-05,
      "epoch": 2.4679245283018867,
      "step": 328
    },
    {
      "loss": 0.6612,
      "grad_norm": 1.6463404893875122,
      "learning_rate": 3.650385604113111e-05,
      "epoch": 2.4754716981132074,
      "step": 329
    },
    {
      "loss": 0.6449,
      "grad_norm": 1.6507012844085693,
      "learning_rate": 3.598971722365039e-05,
      "epoch": 2.483018867924528,
      "step": 330
    },
    {
      "loss": 0.6645,
      "grad_norm": 1.6528139114379883,
      "learning_rate": 3.547557840616967e-05,
      "epoch": 2.490566037735849,
      "step": 331
    },
    {
      "loss": 0.6981,
      "grad_norm": 1.7348213195800781,
      "learning_rate": 3.496143958868895e-05,
      "epoch": 2.4981132075471697,
      "step": 332
    },
    {
      "loss": 0.6758,
      "grad_norm": 1.7416330575942993,
      "learning_rate": 3.444730077120823e-05,
      "epoch": 2.5056603773584905,
      "step": 333
    },
    {
      "loss": 0.6681,
      "grad_norm": 1.6700336933135986,
      "learning_rate": 3.393316195372751e-05,
      "epoch": 2.513207547169811,
      "step": 334
    },
    {
      "loss": 0.6299,
      "grad_norm": 1.5602121353149414,
      "learning_rate": 3.341902313624679e-05,
      "epoch": 2.520754716981132,
      "step": 335
    },
    {
      "loss": 0.6448,
      "grad_norm": 1.6794242858886719,
      "learning_rate": 3.2904884318766064e-05,
      "epoch": 2.5283018867924527,
      "step": 336
    },
    {
      "loss": 0.6328,
      "grad_norm": 1.6314451694488525,
      "learning_rate": 3.2390745501285345e-05,
      "epoch": 2.5358490566037735,
      "step": 337
    },
    {
      "loss": 0.6516,
      "grad_norm": 1.6955198049545288,
      "learning_rate": 3.1876606683804625e-05,
      "epoch": 2.543396226415094,
      "step": 338
    },
    {
      "loss": 0.6628,
      "grad_norm": 1.6558295488357544,
      "learning_rate": 3.1362467866323906e-05,
      "epoch": 2.550943396226415,
      "step": 339
    },
    {
      "loss": 0.6208,
      "grad_norm": 1.6482348442077637,
      "learning_rate": 3.0848329048843186e-05,
      "epoch": 2.5584905660377357,
      "step": 340
    },
    {
      "loss": 0.6274,
      "grad_norm": 1.632947325706482,
      "learning_rate": 3.033419023136247e-05,
      "epoch": 2.5660377358490565,
      "step": 341
    },
    {
      "loss": 0.7014,
      "grad_norm": 1.7651704549789429,
      "learning_rate": 2.982005141388175e-05,
      "epoch": 2.5735849056603772,
      "step": 342
    },
    {
      "loss": 0.6601,
      "grad_norm": 2.0355746746063232,
      "learning_rate": 2.930591259640103e-05,
      "epoch": 2.581132075471698,
      "step": 343
    },
    {
      "loss": 0.6568,
      "grad_norm": 1.702254056930542,
      "learning_rate": 2.879177377892031e-05,
      "epoch": 2.5886792452830187,
      "step": 344
    },
    {
      "loss": 0.691,
      "grad_norm": 1.6798804998397827,
      "learning_rate": 2.827763496143959e-05,
      "epoch": 2.5962264150943395,
      "step": 345
    },
    {
      "loss": 0.692,
      "grad_norm": 1.6784210205078125,
      "learning_rate": 2.7763496143958872e-05,
      "epoch": 2.6037735849056602,
      "step": 346
    },
    {
      "loss": 0.6822,
      "grad_norm": 1.7848566770553589,
      "learning_rate": 2.724935732647815e-05,
      "epoch": 2.611320754716981,
      "step": 347
    },
    {
      "loss": 0.6574,
      "grad_norm": 1.6256685256958008,
      "learning_rate": 2.673521850899743e-05,
      "epoch": 2.6188679245283017,
      "step": 348
    },
    {
      "loss": 0.6491,
      "grad_norm": 1.6476603746414185,
      "learning_rate": 2.622107969151671e-05,
      "epoch": 2.6264150943396225,
      "step": 349
    },
    {
      "loss": 0.638,
      "grad_norm": 1.6192508935928345,
      "learning_rate": 2.570694087403599e-05,
      "epoch": 2.6339622641509433,
      "step": 350
    },
    {
      "loss": 0.6313,
      "grad_norm": 1.6382197141647339,
      "learning_rate": 2.519280205655527e-05,
      "epoch": 2.641509433962264,
      "step": 351
    },
    {
      "loss": 0.6344,
      "grad_norm": 1.6422145366668701,
      "learning_rate": 2.467866323907455e-05,
      "epoch": 2.6490566037735848,
      "step": 352
    },
    {
      "loss": 0.6536,
      "grad_norm": 1.6721845865249634,
      "learning_rate": 2.4164524421593832e-05,
      "epoch": 2.6566037735849055,
      "step": 353
    },
    {
      "loss": 0.6691,
      "grad_norm": 1.6876617670059204,
      "learning_rate": 2.3650385604113112e-05,
      "epoch": 2.6641509433962263,
      "step": 354
    },
    {
      "loss": 0.6336,
      "grad_norm": 1.6655389070510864,
      "learning_rate": 2.3136246786632393e-05,
      "epoch": 2.671698113207547,
      "step": 355
    },
    {
      "loss": 0.6181,
      "grad_norm": 1.6384738683700562,
      "learning_rate": 2.262210796915167e-05,
      "epoch": 2.6792452830188678,
      "step": 356
    },
    {
      "loss": 0.6195,
      "grad_norm": 1.5895315408706665,
      "learning_rate": 2.210796915167095e-05,
      "epoch": 2.6867924528301885,
      "step": 357
    },
    {
      "loss": 0.6727,
      "grad_norm": 1.71238112449646,
      "learning_rate": 2.159383033419023e-05,
      "epoch": 2.6943396226415093,
      "step": 358
    },
    {
      "loss": 0.6353,
      "grad_norm": 1.6276895999908447,
      "learning_rate": 2.107969151670951e-05,
      "epoch": 2.70188679245283,
      "step": 359
    },
    {
      "loss": 0.633,
      "grad_norm": 1.5969487428665161,
      "learning_rate": 2.0565552699228792e-05,
      "epoch": 2.709433962264151,
      "step": 360
    },
    {
      "loss": 0.6093,
      "grad_norm": 1.6160911321640015,
      "learning_rate": 2.0051413881748076e-05,
      "epoch": 2.7169811320754715,
      "step": 361
    },
    {
      "loss": 0.6946,
      "grad_norm": 1.7255879640579224,
      "learning_rate": 1.9537275064267353e-05,
      "epoch": 2.7245283018867923,
      "step": 362
    },
    {
      "loss": 0.6308,
      "grad_norm": 1.6896719932556152,
      "learning_rate": 1.9023136246786633e-05,
      "epoch": 2.732075471698113,
      "step": 363
    },
    {
      "loss": 0.6225,
      "grad_norm": 1.633613109588623,
      "learning_rate": 1.8508997429305914e-05,
      "epoch": 2.739622641509434,
      "step": 364
    },
    {
      "loss": 0.6178,
      "grad_norm": 1.6479034423828125,
      "learning_rate": 1.7994858611825194e-05,
      "epoch": 2.7471698113207546,
      "step": 365
    },
    {
      "loss": 0.6396,
      "grad_norm": 1.7202380895614624,
      "learning_rate": 1.7480719794344475e-05,
      "epoch": 2.7547169811320753,
      "step": 366
    },
    {
      "loss": 0.633,
      "grad_norm": 1.7336405515670776,
      "learning_rate": 1.6966580976863755e-05,
      "epoch": 2.7622641509433965,
      "step": 367
    },
    {
      "loss": 0.6397,
      "grad_norm": 1.6858940124511719,
      "learning_rate": 1.6452442159383032e-05,
      "epoch": 2.769811320754717,
      "step": 368
    },
    {
      "loss": 0.6348,
      "grad_norm": 1.7060155868530273,
      "learning_rate": 1.5938303341902313e-05,
      "epoch": 2.777358490566038,
      "step": 369
    },
    {
      "loss": 0.6609,
      "grad_norm": 1.722499132156372,
      "learning_rate": 1.5424164524421593e-05,
      "epoch": 2.7849056603773583,
      "step": 370
    },
    {
      "loss": 0.6601,
      "grad_norm": 1.7595447301864624,
      "learning_rate": 1.4910025706940875e-05,
      "epoch": 2.7924528301886795,
      "step": 371
    },
    {
      "loss": 0.6572,
      "grad_norm": 1.8120514154434204,
      "learning_rate": 1.4395886889460156e-05,
      "epoch": 2.8,
      "step": 372
    },
    {
      "loss": 0.6771,
      "grad_norm": 1.7734426259994507,
      "learning_rate": 1.3881748071979436e-05,
      "epoch": 2.807547169811321,
      "step": 373
    },
    {
      "loss": 0.6362,
      "grad_norm": 1.7241268157958984,
      "learning_rate": 1.3367609254498715e-05,
      "epoch": 2.8150943396226413,
      "step": 374
    },
    {
      "loss": 0.6259,
      "grad_norm": 1.6719199419021606,
      "learning_rate": 1.2853470437017995e-05,
      "epoch": 2.8226415094339625,
      "step": 375
    },
    {
      "loss": 0.6321,
      "grad_norm": 1.6829948425292969,
      "learning_rate": 1.2339331619537276e-05,
      "epoch": 2.830188679245283,
      "step": 376
    },
    {
      "loss": 0.6242,
      "grad_norm": 1.6960657835006714,
      "learning_rate": 1.1825192802056556e-05,
      "epoch": 2.837735849056604,
      "step": 377
    },
    {
      "loss": 0.6709,
      "grad_norm": 1.7807825803756714,
      "learning_rate": 1.1311053984575835e-05,
      "epoch": 2.8452830188679243,
      "step": 378
    },
    {
      "loss": 0.6198,
      "grad_norm": 1.6908169984817505,
      "learning_rate": 1.0796915167095115e-05,
      "epoch": 2.8528301886792455,
      "step": 379
    },
    {
      "loss": 0.6566,
      "grad_norm": 1.785103678703308,
      "learning_rate": 1.0282776349614396e-05,
      "epoch": 2.860377358490566,
      "step": 380
    },
    {
      "loss": 0.662,
      "grad_norm": 1.6442679166793823,
      "learning_rate": 9.768637532133676e-06,
      "epoch": 2.867924528301887,
      "step": 381
    },
    {
      "loss": 0.6194,
      "grad_norm": 1.676598310470581,
      "learning_rate": 9.254498714652957e-06,
      "epoch": 2.8754716981132074,
      "step": 382
    },
    {
      "loss": 0.6268,
      "grad_norm": 1.6533840894699097,
      "learning_rate": 8.740359897172237e-06,
      "epoch": 2.8830188679245285,
      "step": 383
    },
    {
      "loss": 0.6346,
      "grad_norm": 1.6604454517364502,
      "learning_rate": 8.226221079691516e-06,
      "epoch": 2.890566037735849,
      "step": 384
    },
    {
      "loss": 0.6053,
      "grad_norm": 1.6094533205032349,
      "learning_rate": 7.712082262210796e-06,
      "epoch": 2.89811320754717,
      "step": 385
    },
    {
      "loss": 0.6417,
      "grad_norm": 1.709493637084961,
      "learning_rate": 7.197943444730078e-06,
      "epoch": 2.9056603773584904,
      "step": 386
    },
    {
      "loss": 0.6437,
      "grad_norm": 1.7171164751052856,
      "learning_rate": 6.683804627249357e-06,
      "epoch": 2.9132075471698116,
      "step": 387
    },
    {
      "loss": 0.6666,
      "grad_norm": 1.7245426177978516,
      "learning_rate": 6.169665809768638e-06,
      "epoch": 2.920754716981132,
      "step": 388
    },
    {
      "loss": 0.6664,
      "grad_norm": 1.748738408088684,
      "learning_rate": 5.6555269922879175e-06,
      "epoch": 2.928301886792453,
      "step": 389
    },
    {
      "loss": 0.6138,
      "grad_norm": 1.6603364944458008,
      "learning_rate": 5.141388174807198e-06,
      "epoch": 2.9358490566037734,
      "step": 390
    },
    {
      "loss": 0.6315,
      "grad_norm": 1.6657453775405884,
      "learning_rate": 4.627249357326478e-06,
      "epoch": 2.9433962264150946,
      "step": 391
    },
    {
      "loss": 0.6363,
      "grad_norm": 1.6678462028503418,
      "learning_rate": 4.113110539845758e-06,
      "epoch": 2.950943396226415,
      "step": 392
    },
    {
      "loss": 0.6545,
      "grad_norm": 1.7133716344833374,
      "learning_rate": 3.598971722365039e-06,
      "epoch": 2.958490566037736,
      "step": 393
    },
    {
      "loss": 0.6627,
      "grad_norm": 1.6848467588424683,
      "learning_rate": 3.084832904884319e-06,
      "epoch": 2.9660377358490564,
      "step": 394
    },
    {
      "loss": 0.6583,
      "grad_norm": 1.6903331279754639,
      "learning_rate": 2.570694087403599e-06,
      "epoch": 2.9735849056603776,
      "step": 395
    },
    {
      "loss": 0.6173,
      "grad_norm": 1.733603596687317,
      "learning_rate": 2.056555269922879e-06,
      "epoch": 2.981132075471698,
      "step": 396
    },
    {
      "loss": 0.6306,
      "grad_norm": 1.7681657075881958,
      "learning_rate": 1.5424164524421595e-06,
      "epoch": 2.988679245283019,
      "step": 397
    },
    {
      "loss": 0.6582,
      "grad_norm": 1.7062768936157227,
      "learning_rate": 1.0282776349614395e-06,
      "epoch": 2.9962264150943394,
      "step": 398
    },
    {
      "loss": 0.654,
      "grad_norm": 2.3712854385375977,
      "learning_rate": 5.141388174807198e-07,
      "epoch": 3.0,
      "step": 399
    },
    {
      "eval_loss": 1.2668596506118774,
      "eval_runtime": 10.3535,
      "eval_samples_per_second": 51.19,
      "eval_steps_per_second": 6.471,
      "epoch": 3.0,
      "step": 399
    },
    {
      "train_runtime": 628.767,
      "train_samples_per_second": 20.23,
      "train_steps_per_second": 0.635,
      "total_flos": 1.3974698786291712e+17,
      "train_loss": 1.0157097651247393,
      "epoch": 3.0,
      "step": 399
    }
  ],
  "dataset_info": {
    "train_size": 4240,
    "val_size": 530,
    "total_size": 4770,
    "train_val_split": "4240/530",
    "data_dir": "./datasets/datasets_v4",
    "sample_train_example": {
      "text_length": 208,
      "keys": [
        "text",
        "labels"
      ]
    }
  },
  "model_stats": {
    "trainable_params_detail": [
      {
        "name": "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight",
        "shape": [
          16,
          4096
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight",
        "shape": [
          4096,
          16
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight",
        "shape": [
          16,
          4096
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight",
        "shape": [
          1024,
          16
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 16384
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight",
        "shape": [
          16,
          4096
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      }
    ],
    "total_trainable_params_verified": 448
  },
  "training_start_time": "2025-07-12T15:07:36.350957",
  "training_end_time": "2025-07-12T15:18:05.395701",
  "total_training_duration": "0:10:29.044744"
}