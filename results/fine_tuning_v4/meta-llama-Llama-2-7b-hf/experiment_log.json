{
  "experiment_id": ".-local_models-meta-llama-Llama-2-7b-hf_20250712_145547",
  "model_name": "./local_models/meta-llama-Llama-2-7b-hf",
  "timestamp": "2025-07-12T14:55:47.162599",
  "config": {
    "lora_config": {
      "r": 16,
      "lora_alpha": 32,
      "target_modules": "{'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj'}",
      "lora_dropout": 0.1,
      "bias": "none",
      "task_type": "CAUSAL_LM"
    },
    "training_args": {
      "per_device_train_batch_size": 8,
      "per_device_eval_batch_size": 8,
      "gradient_accumulation_steps": 4,
      "effective_batch_size": 32,
      "num_train_epochs": 3,
      "learning_rate": 0.0002,
      "fp16": true,
      "gradient_checkpointing": false,
      "max_grad_norm": 1.0,
      "optimizer": "adamw_torch",
      "warmup_steps": 10,
      "eval_strategy": "epoch",
      "save_strategy": "epoch",
      "logging_steps": 1
    },
    "estimated_total_steps": 396
  },
  "metrics": {
    "training_successful": true,
    "final_train_loss": 1.1294481911157306,
    "train_runtime_seconds": 600.0073,
    "train_runtime_minutes": 10.000121666666667,
    "train_samples_per_second": 21.2,
    "train_steps_per_second": 0.665,
    "epoch": 3.0,
    "global_step": 399,
    "total_flos": 1.2987481762824192e+17,
    "training_progression": {
      "initial_train_loss": 3.31,
      "final_train_loss": 0.8578,
      "loss_improvement": 2.4522,
      "best_eval_loss": 1.1415289640426636,
      "final_eval_loss": 1.1499392986297607,
      "peak_learning_rate": 0.0002,
      "final_learning_rate": 5.141388174807198e-07
    },
    "final_gpu_memory": {
      "allocated_gb": 13.060555934906006,
      "reserved_gb": 30.77734375,
      "max_allocated_gb": 34.05824327468872
    }
  },
  "system_info": {
    "gpu_available": true,
    "gpu_count": 1,
    "gpu_info": [],
    "cuda_version": "12.6",
    "torch_version": "2.7.1+cu126",
    "python_version": "3.9.21",
    "cpu_count": 256,
    "memory_total_gb": 1006.93,
    "memory_available_gb": 983.36
  },
  "training_history": [
    {
      "loss": 3.31,
      "grad_norm": 1.3442819118499756,
      "learning_rate": 0.0,
      "epoch": 0.007547169811320755,
      "step": 1
    },
    {
      "loss": 3.2476,
      "grad_norm": 1.3761541843414307,
      "learning_rate": 2e-05,
      "epoch": 0.01509433962264151,
      "step": 2
    },
    {
      "loss": 3.3362,
      "grad_norm": 1.4619882106781006,
      "learning_rate": 4e-05,
      "epoch": 0.022641509433962263,
      "step": 3
    },
    {
      "loss": 3.2098,
      "grad_norm": 1.4878789186477661,
      "learning_rate": 6e-05,
      "epoch": 0.03018867924528302,
      "step": 4
    },
    {
      "loss": 3.141,
      "grad_norm": 1.2689467668533325,
      "learning_rate": 8e-05,
      "epoch": 0.03773584905660377,
      "step": 5
    },
    {
      "loss": 3.0169,
      "grad_norm": 1.5307661294937134,
      "learning_rate": 0.0001,
      "epoch": 0.045283018867924525,
      "step": 6
    },
    {
      "loss": 2.8497,
      "grad_norm": 2.1123058795928955,
      "learning_rate": 0.00012,
      "epoch": 0.052830188679245285,
      "step": 7
    },
    {
      "loss": 2.6163,
      "grad_norm": 2.3746376037597656,
      "learning_rate": 0.00014,
      "epoch": 0.06037735849056604,
      "step": 8
    },
    {
      "loss": 2.4338,
      "grad_norm": 1.4399081468582153,
      "learning_rate": 0.00016,
      "epoch": 0.06792452830188679,
      "step": 9
    },
    {
      "loss": 2.2124,
      "grad_norm": 1.401224136352539,
      "learning_rate": 0.00018,
      "epoch": 0.07547169811320754,
      "step": 10
    },
    {
      "loss": 2.1004,
      "grad_norm": 1.3155008554458618,
      "learning_rate": 0.0002,
      "epoch": 0.0830188679245283,
      "step": 11
    },
    {
      "loss": 2.0308,
      "grad_norm": 1.1658704280853271,
      "learning_rate": 0.0001994858611825193,
      "epoch": 0.09056603773584905,
      "step": 12
    },
    {
      "loss": 1.9156,
      "grad_norm": 1.107974648475647,
      "learning_rate": 0.00019897172236503857,
      "epoch": 0.09811320754716982,
      "step": 13
    },
    {
      "loss": 1.8917,
      "grad_norm": 0.8188918828964233,
      "learning_rate": 0.00019845758354755785,
      "epoch": 0.10566037735849057,
      "step": 14
    },
    {
      "loss": 1.836,
      "grad_norm": 0.7684649229049683,
      "learning_rate": 0.00019794344473007713,
      "epoch": 0.11320754716981132,
      "step": 15
    },
    {
      "loss": 1.8215,
      "grad_norm": 0.6803317666053772,
      "learning_rate": 0.0001974293059125964,
      "epoch": 0.12075471698113208,
      "step": 16
    },
    {
      "loss": 1.7681,
      "grad_norm": 0.7970982193946838,
      "learning_rate": 0.0001969151670951157,
      "epoch": 0.12830188679245283,
      "step": 17
    },
    {
      "loss": 1.6317,
      "grad_norm": 0.6614052653312683,
      "learning_rate": 0.00019640102827763497,
      "epoch": 0.13584905660377358,
      "step": 18
    },
    {
      "loss": 1.7128,
      "grad_norm": 0.6312405467033386,
      "learning_rate": 0.00019588688946015425,
      "epoch": 0.14339622641509434,
      "step": 19
    },
    {
      "loss": 1.6897,
      "grad_norm": 0.8068098425865173,
      "learning_rate": 0.00019537275064267353,
      "epoch": 0.1509433962264151,
      "step": 20
    },
    {
      "loss": 1.5883,
      "grad_norm": 0.5760451555252075,
      "learning_rate": 0.00019485861182519281,
      "epoch": 0.15849056603773584,
      "step": 21
    },
    {
      "loss": 1.6785,
      "grad_norm": 0.6545118093490601,
      "learning_rate": 0.0001943444730077121,
      "epoch": 0.1660377358490566,
      "step": 22
    },
    {
      "loss": 1.6004,
      "grad_norm": 0.62018221616745,
      "learning_rate": 0.00019383033419023138,
      "epoch": 0.17358490566037735,
      "step": 23
    },
    {
      "loss": 1.5777,
      "grad_norm": 0.6219264268875122,
      "learning_rate": 0.00019331619537275066,
      "epoch": 0.1811320754716981,
      "step": 24
    },
    {
      "loss": 1.5672,
      "grad_norm": 0.6174509525299072,
      "learning_rate": 0.00019280205655526994,
      "epoch": 0.18867924528301888,
      "step": 25
    },
    {
      "loss": 1.5801,
      "grad_norm": 0.6391956210136414,
      "learning_rate": 0.0001922879177377892,
      "epoch": 0.19622641509433963,
      "step": 26
    },
    {
      "loss": 1.5414,
      "grad_norm": 0.5491505861282349,
      "learning_rate": 0.0001917737789203085,
      "epoch": 0.2037735849056604,
      "step": 27
    },
    {
      "loss": 1.5787,
      "grad_norm": 0.6078590154647827,
      "learning_rate": 0.00019125964010282778,
      "epoch": 0.21132075471698114,
      "step": 28
    },
    {
      "loss": 1.5282,
      "grad_norm": 0.6246992349624634,
      "learning_rate": 0.00019074550128534706,
      "epoch": 0.2188679245283019,
      "step": 29
    },
    {
      "loss": 1.5814,
      "grad_norm": 0.6233237385749817,
      "learning_rate": 0.00019023136246786634,
      "epoch": 0.22641509433962265,
      "step": 30
    },
    {
      "loss": 1.4761,
      "grad_norm": 0.5879965424537659,
      "learning_rate": 0.00018971722365038562,
      "epoch": 0.2339622641509434,
      "step": 31
    },
    {
      "loss": 1.4751,
      "grad_norm": 0.6147722005844116,
      "learning_rate": 0.0001892030848329049,
      "epoch": 0.24150943396226415,
      "step": 32
    },
    {
      "loss": 1.5317,
      "grad_norm": 0.7439630031585693,
      "learning_rate": 0.00018868894601542418,
      "epoch": 0.2490566037735849,
      "step": 33
    },
    {
      "loss": 1.495,
      "grad_norm": 0.6000377535820007,
      "learning_rate": 0.00018817480719794346,
      "epoch": 0.25660377358490566,
      "step": 34
    },
    {
      "loss": 1.4709,
      "grad_norm": 0.6003340482711792,
      "learning_rate": 0.0001876606683804627,
      "epoch": 0.2641509433962264,
      "step": 35
    },
    {
      "loss": 1.4677,
      "grad_norm": 0.566846489906311,
      "learning_rate": 0.00018714652956298202,
      "epoch": 0.27169811320754716,
      "step": 36
    },
    {
      "loss": 1.4601,
      "grad_norm": 0.6136858463287354,
      "learning_rate": 0.0001866323907455013,
      "epoch": 0.2792452830188679,
      "step": 37
    },
    {
      "loss": 1.4627,
      "grad_norm": 0.6099345684051514,
      "learning_rate": 0.00018611825192802058,
      "epoch": 0.28679245283018867,
      "step": 38
    },
    {
      "loss": 1.4227,
      "grad_norm": 0.6157953143119812,
      "learning_rate": 0.00018560411311053984,
      "epoch": 0.2943396226415094,
      "step": 39
    },
    {
      "loss": 1.4386,
      "grad_norm": 0.6295158863067627,
      "learning_rate": 0.00018508997429305914,
      "epoch": 0.3018867924528302,
      "step": 40
    },
    {
      "loss": 1.3891,
      "grad_norm": 0.6674092411994934,
      "learning_rate": 0.00018457583547557842,
      "epoch": 0.30943396226415093,
      "step": 41
    },
    {
      "loss": 1.4699,
      "grad_norm": 0.7217402458190918,
      "learning_rate": 0.0001840616966580977,
      "epoch": 0.3169811320754717,
      "step": 42
    },
    {
      "loss": 1.4337,
      "grad_norm": 0.7565615773200989,
      "learning_rate": 0.00018354755784061696,
      "epoch": 0.32452830188679244,
      "step": 43
    },
    {
      "loss": 1.4154,
      "grad_norm": 0.6482926607131958,
      "learning_rate": 0.00018303341902313626,
      "epoch": 0.3320754716981132,
      "step": 44
    },
    {
      "loss": 1.4297,
      "grad_norm": 0.6145392656326294,
      "learning_rate": 0.00018251928020565555,
      "epoch": 0.33962264150943394,
      "step": 45
    },
    {
      "loss": 1.4108,
      "grad_norm": 0.6238316893577576,
      "learning_rate": 0.00018200514138817483,
      "epoch": 0.3471698113207547,
      "step": 46
    },
    {
      "loss": 1.3911,
      "grad_norm": 0.6677616238594055,
      "learning_rate": 0.00018149100257069408,
      "epoch": 0.35471698113207545,
      "step": 47
    },
    {
      "loss": 1.4307,
      "grad_norm": 0.6148428916931152,
      "learning_rate": 0.00018097686375321336,
      "epoch": 0.3622641509433962,
      "step": 48
    },
    {
      "loss": 1.3236,
      "grad_norm": 0.6156731843948364,
      "learning_rate": 0.00018046272493573267,
      "epoch": 0.36981132075471695,
      "step": 49
    },
    {
      "loss": 1.3635,
      "grad_norm": 0.6445578932762146,
      "learning_rate": 0.00017994858611825195,
      "epoch": 0.37735849056603776,
      "step": 50
    },
    {
      "loss": 1.3743,
      "grad_norm": 0.6736583113670349,
      "learning_rate": 0.0001794344473007712,
      "epoch": 0.3849056603773585,
      "step": 51
    },
    {
      "loss": 1.3962,
      "grad_norm": 0.7075239419937134,
      "learning_rate": 0.00017892030848329048,
      "epoch": 0.39245283018867927,
      "step": 52
    },
    {
      "loss": 1.3963,
      "grad_norm": 0.6754744648933411,
      "learning_rate": 0.0001784061696658098,
      "epoch": 0.4,
      "step": 53
    },
    {
      "loss": 1.3099,
      "grad_norm": 0.65644371509552,
      "learning_rate": 0.00017789203084832907,
      "epoch": 0.4075471698113208,
      "step": 54
    },
    {
      "loss": 1.3498,
      "grad_norm": 0.6644658446311951,
      "learning_rate": 0.00017737789203084832,
      "epoch": 0.41509433962264153,
      "step": 55
    },
    {
      "loss": 1.3385,
      "grad_norm": 0.6146548390388489,
      "learning_rate": 0.0001768637532133676,
      "epoch": 0.4226415094339623,
      "step": 56
    },
    {
      "loss": 1.2684,
      "grad_norm": 0.6228744983673096,
      "learning_rate": 0.0001763496143958869,
      "epoch": 0.43018867924528303,
      "step": 57
    },
    {
      "loss": 1.3641,
      "grad_norm": 0.6160013675689697,
      "learning_rate": 0.0001758354755784062,
      "epoch": 0.4377358490566038,
      "step": 58
    },
    {
      "loss": 1.3327,
      "grad_norm": 0.6456766128540039,
      "learning_rate": 0.00017532133676092547,
      "epoch": 0.44528301886792454,
      "step": 59
    },
    {
      "loss": 1.3322,
      "grad_norm": 0.6617422699928284,
      "learning_rate": 0.00017480719794344473,
      "epoch": 0.4528301886792453,
      "step": 60
    },
    {
      "loss": 1.3199,
      "grad_norm": 0.6632983684539795,
      "learning_rate": 0.000174293059125964,
      "epoch": 0.46037735849056605,
      "step": 61
    },
    {
      "loss": 1.433,
      "grad_norm": 0.7067658305168152,
      "learning_rate": 0.0001737789203084833,
      "epoch": 0.4679245283018868,
      "step": 62
    },
    {
      "loss": 1.3418,
      "grad_norm": 0.6643065214157104,
      "learning_rate": 0.0001732647814910026,
      "epoch": 0.47547169811320755,
      "step": 63
    },
    {
      "loss": 1.3333,
      "grad_norm": 0.6593829989433289,
      "learning_rate": 0.00017275064267352185,
      "epoch": 0.4830188679245283,
      "step": 64
    },
    {
      "loss": 1.3338,
      "grad_norm": 0.6247743368148804,
      "learning_rate": 0.00017223650385604113,
      "epoch": 0.49056603773584906,
      "step": 65
    },
    {
      "loss": 1.3824,
      "grad_norm": 0.6526659727096558,
      "learning_rate": 0.00017172236503856043,
      "epoch": 0.4981132075471698,
      "step": 66
    },
    {
      "loss": 1.311,
      "grad_norm": 0.729470431804657,
      "learning_rate": 0.00017120822622107972,
      "epoch": 0.5056603773584906,
      "step": 67
    },
    {
      "loss": 1.3613,
      "grad_norm": 0.642131507396698,
      "learning_rate": 0.00017069408740359897,
      "epoch": 0.5132075471698113,
      "step": 68
    },
    {
      "loss": 1.2732,
      "grad_norm": 0.7066127061843872,
      "learning_rate": 0.00017017994858611825,
      "epoch": 0.5207547169811321,
      "step": 69
    },
    {
      "loss": 1.3298,
      "grad_norm": 0.6685572266578674,
      "learning_rate": 0.00016966580976863753,
      "epoch": 0.5283018867924528,
      "step": 70
    },
    {
      "loss": 1.34,
      "grad_norm": 0.7505302429199219,
      "learning_rate": 0.00016915167095115684,
      "epoch": 0.5358490566037736,
      "step": 71
    },
    {
      "loss": 1.3083,
      "grad_norm": 0.7266237735748291,
      "learning_rate": 0.0001686375321336761,
      "epoch": 0.5433962264150943,
      "step": 72
    },
    {
      "loss": 1.3338,
      "grad_norm": 0.6476438641548157,
      "learning_rate": 0.00016812339331619537,
      "epoch": 0.5509433962264151,
      "step": 73
    },
    {
      "loss": 1.2566,
      "grad_norm": 0.61875319480896,
      "learning_rate": 0.00016760925449871465,
      "epoch": 0.5584905660377358,
      "step": 74
    },
    {
      "loss": 1.24,
      "grad_norm": 0.645010769367218,
      "learning_rate": 0.00016709511568123396,
      "epoch": 0.5660377358490566,
      "step": 75
    },
    {
      "loss": 1.319,
      "grad_norm": 0.6538770794868469,
      "learning_rate": 0.0001665809768637532,
      "epoch": 0.5735849056603773,
      "step": 76
    },
    {
      "loss": 1.2585,
      "grad_norm": 0.6589723229408264,
      "learning_rate": 0.0001660668380462725,
      "epoch": 0.5811320754716981,
      "step": 77
    },
    {
      "loss": 1.2503,
      "grad_norm": 0.6550537943840027,
      "learning_rate": 0.00016555269922879177,
      "epoch": 0.5886792452830188,
      "step": 78
    },
    {
      "loss": 1.2941,
      "grad_norm": 0.7154865860939026,
      "learning_rate": 0.00016503856041131108,
      "epoch": 0.5962264150943396,
      "step": 79
    },
    {
      "loss": 1.2578,
      "grad_norm": 0.7208206653594971,
      "learning_rate": 0.00016452442159383033,
      "epoch": 0.6037735849056604,
      "step": 80
    },
    {
      "loss": 1.2582,
      "grad_norm": 0.726748526096344,
      "learning_rate": 0.00016401028277634961,
      "epoch": 0.6113207547169811,
      "step": 81
    },
    {
      "loss": 1.2768,
      "grad_norm": 0.6708616614341736,
      "learning_rate": 0.0001634961439588689,
      "epoch": 0.6188679245283019,
      "step": 82
    },
    {
      "loss": 1.2096,
      "grad_norm": 0.6756531596183777,
      "learning_rate": 0.00016298200514138818,
      "epoch": 0.6264150943396226,
      "step": 83
    },
    {
      "loss": 1.2152,
      "grad_norm": 0.6532971858978271,
      "learning_rate": 0.00016246786632390746,
      "epoch": 0.6339622641509434,
      "step": 84
    },
    {
      "loss": 1.2656,
      "grad_norm": 0.6558559536933899,
      "learning_rate": 0.00016195372750642674,
      "epoch": 0.6415094339622641,
      "step": 85
    },
    {
      "loss": 1.3981,
      "grad_norm": 0.711599588394165,
      "learning_rate": 0.00016143958868894602,
      "epoch": 0.6490566037735849,
      "step": 86
    },
    {
      "loss": 1.2688,
      "grad_norm": 0.6745736002922058,
      "learning_rate": 0.0001609254498714653,
      "epoch": 0.6566037735849056,
      "step": 87
    },
    {
      "loss": 1.2439,
      "grad_norm": 0.6841168999671936,
      "learning_rate": 0.0001604113110539846,
      "epoch": 0.6641509433962264,
      "step": 88
    },
    {
      "loss": 1.2648,
      "grad_norm": 0.7168614268302917,
      "learning_rate": 0.00015989717223650386,
      "epoch": 0.6716981132075471,
      "step": 89
    },
    {
      "loss": 1.293,
      "grad_norm": 0.6919727325439453,
      "learning_rate": 0.00015938303341902314,
      "epoch": 0.6792452830188679,
      "step": 90
    },
    {
      "loss": 1.2283,
      "grad_norm": 0.6930951476097107,
      "learning_rate": 0.00015886889460154242,
      "epoch": 0.6867924528301886,
      "step": 91
    },
    {
      "loss": 1.275,
      "grad_norm": 0.691543459892273,
      "learning_rate": 0.00015835475578406173,
      "epoch": 0.6943396226415094,
      "step": 92
    },
    {
      "loss": 1.2486,
      "grad_norm": 0.7136455774307251,
      "learning_rate": 0.00015784061696658098,
      "epoch": 0.7018867924528301,
      "step": 93
    },
    {
      "loss": 1.2981,
      "grad_norm": 0.699851930141449,
      "learning_rate": 0.00015732647814910026,
      "epoch": 0.7094339622641509,
      "step": 94
    },
    {
      "loss": 1.2184,
      "grad_norm": 0.6904928088188171,
      "learning_rate": 0.00015681233933161954,
      "epoch": 0.7169811320754716,
      "step": 95
    },
    {
      "loss": 1.2541,
      "grad_norm": 0.6687981486320496,
      "learning_rate": 0.00015629820051413882,
      "epoch": 0.7245283018867924,
      "step": 96
    },
    {
      "loss": 1.2471,
      "grad_norm": 0.634596586227417,
      "learning_rate": 0.0001557840616966581,
      "epoch": 0.7320754716981132,
      "step": 97
    },
    {
      "loss": 1.2155,
      "grad_norm": 0.6682162284851074,
      "learning_rate": 0.00015526992287917738,
      "epoch": 0.7396226415094339,
      "step": 98
    },
    {
      "loss": 1.1404,
      "grad_norm": 0.6746100783348083,
      "learning_rate": 0.00015475578406169666,
      "epoch": 0.7471698113207547,
      "step": 99
    },
    {
      "loss": 1.2622,
      "grad_norm": 0.6738814115524292,
      "learning_rate": 0.00015424164524421594,
      "epoch": 0.7547169811320755,
      "step": 100
    },
    {
      "loss": 1.1332,
      "grad_norm": 0.6712843775749207,
      "learning_rate": 0.00015372750642673522,
      "epoch": 0.7622641509433963,
      "step": 101
    },
    {
      "loss": 1.2366,
      "grad_norm": 0.718896746635437,
      "learning_rate": 0.0001532133676092545,
      "epoch": 0.769811320754717,
      "step": 102
    },
    {
      "loss": 1.2794,
      "grad_norm": 0.6852566003799438,
      "learning_rate": 0.00015269922879177378,
      "epoch": 0.7773584905660378,
      "step": 103
    },
    {
      "loss": 1.2511,
      "grad_norm": 0.7071084380149841,
      "learning_rate": 0.00015218508997429307,
      "epoch": 0.7849056603773585,
      "step": 104
    },
    {
      "loss": 1.1884,
      "grad_norm": 0.6783962249755859,
      "learning_rate": 0.00015167095115681235,
      "epoch": 0.7924528301886793,
      "step": 105
    },
    {
      "loss": 1.3108,
      "grad_norm": 0.7018897533416748,
      "learning_rate": 0.00015115681233933163,
      "epoch": 0.8,
      "step": 106
    },
    {
      "loss": 1.295,
      "grad_norm": 0.7042877078056335,
      "learning_rate": 0.0001506426735218509,
      "epoch": 0.8075471698113208,
      "step": 107
    },
    {
      "loss": 1.2145,
      "grad_norm": 0.6428989171981812,
      "learning_rate": 0.0001501285347043702,
      "epoch": 0.8150943396226416,
      "step": 108
    },
    {
      "loss": 1.2601,
      "grad_norm": 0.6556801199913025,
      "learning_rate": 0.00014961439588688947,
      "epoch": 0.8226415094339623,
      "step": 109
    },
    {
      "loss": 1.2163,
      "grad_norm": 0.6680077910423279,
      "learning_rate": 0.00014910025706940875,
      "epoch": 0.8301886792452831,
      "step": 110
    },
    {
      "loss": 1.2752,
      "grad_norm": 0.7251431941986084,
      "learning_rate": 0.00014858611825192803,
      "epoch": 0.8377358490566038,
      "step": 111
    },
    {
      "loss": 1.1984,
      "grad_norm": 0.6924125552177429,
      "learning_rate": 0.0001480719794344473,
      "epoch": 0.8452830188679246,
      "step": 112
    },
    {
      "loss": 1.2323,
      "grad_norm": 0.6551395058631897,
      "learning_rate": 0.0001475578406169666,
      "epoch": 0.8528301886792453,
      "step": 113
    },
    {
      "loss": 1.2276,
      "grad_norm": 0.686177670955658,
      "learning_rate": 0.00014704370179948587,
      "epoch": 0.8603773584905661,
      "step": 114
    },
    {
      "loss": 1.1732,
      "grad_norm": 0.6984573602676392,
      "learning_rate": 0.00014652956298200515,
      "epoch": 0.8679245283018868,
      "step": 115
    },
    {
      "loss": 1.2022,
      "grad_norm": 0.6556186079978943,
      "learning_rate": 0.00014601542416452443,
      "epoch": 0.8754716981132076,
      "step": 116
    },
    {
      "loss": 1.1783,
      "grad_norm": 0.6470711827278137,
      "learning_rate": 0.0001455012853470437,
      "epoch": 0.8830188679245283,
      "step": 117
    },
    {
      "loss": 1.2716,
      "grad_norm": 0.7699859142303467,
      "learning_rate": 0.000144987146529563,
      "epoch": 0.8905660377358491,
      "step": 118
    },
    {
      "loss": 1.2177,
      "grad_norm": 0.7098550200462341,
      "learning_rate": 0.00014447300771208227,
      "epoch": 0.8981132075471698,
      "step": 119
    },
    {
      "loss": 1.1932,
      "grad_norm": 0.6919115781784058,
      "learning_rate": 0.00014395886889460155,
      "epoch": 0.9056603773584906,
      "step": 120
    },
    {
      "loss": 1.2202,
      "grad_norm": 0.6998499035835266,
      "learning_rate": 0.00014344473007712083,
      "epoch": 0.9132075471698113,
      "step": 121
    },
    {
      "loss": 1.2063,
      "grad_norm": 0.6944584846496582,
      "learning_rate": 0.0001429305912596401,
      "epoch": 0.9207547169811321,
      "step": 122
    },
    {
      "loss": 1.1581,
      "grad_norm": 0.6663854718208313,
      "learning_rate": 0.0001424164524421594,
      "epoch": 0.9283018867924528,
      "step": 123
    },
    {
      "loss": 1.2595,
      "grad_norm": 0.7074483633041382,
      "learning_rate": 0.00014190231362467867,
      "epoch": 0.9358490566037736,
      "step": 124
    },
    {
      "loss": 1.2325,
      "grad_norm": 0.6846187710762024,
      "learning_rate": 0.00014138817480719795,
      "epoch": 0.9433962264150944,
      "step": 125
    },
    {
      "loss": 1.2141,
      "grad_norm": 0.6635900735855103,
      "learning_rate": 0.00014087403598971724,
      "epoch": 0.9509433962264151,
      "step": 126
    },
    {
      "loss": 1.1485,
      "grad_norm": 0.697465181350708,
      "learning_rate": 0.00014035989717223652,
      "epoch": 0.9584905660377359,
      "step": 127
    },
    {
      "loss": 1.2117,
      "grad_norm": 0.6746033430099487,
      "learning_rate": 0.0001398457583547558,
      "epoch": 0.9660377358490566,
      "step": 128
    },
    {
      "loss": 1.1755,
      "grad_norm": 0.626292884349823,
      "learning_rate": 0.00013933161953727508,
      "epoch": 0.9735849056603774,
      "step": 129
    },
    {
      "loss": 1.1581,
      "grad_norm": 0.6451870203018188,
      "learning_rate": 0.00013881748071979436,
      "epoch": 0.9811320754716981,
      "step": 130
    },
    {
      "loss": 1.1547,
      "grad_norm": 0.6646808385848999,
      "learning_rate": 0.0001383033419023136,
      "epoch": 0.9886792452830189,
      "step": 131
    },
    {
      "loss": 1.2119,
      "grad_norm": 0.6919535398483276,
      "learning_rate": 0.00013778920308483292,
      "epoch": 0.9962264150943396,
      "step": 132
    },
    {
      "loss": 1.232,
      "grad_norm": 0.9528605341911316,
      "learning_rate": 0.0001372750642673522,
      "epoch": 1.0,
      "step": 133
    },
    {
      "eval_loss": 1.2104709148406982,
      "eval_runtime": 10.0269,
      "eval_samples_per_second": 52.858,
      "eval_steps_per_second": 6.682,
      "epoch": 1.0,
      "step": 133
    },
    {
      "loss": 1.0546,
      "grad_norm": 0.6396116018295288,
      "learning_rate": 0.00013676092544987148,
      "epoch": 1.0075471698113208,
      "step": 134
    },
    {
      "loss": 1.1036,
      "grad_norm": 0.6738373637199402,
      "learning_rate": 0.00013624678663239073,
      "epoch": 1.0150943396226415,
      "step": 135
    },
    {
      "loss": 1.0241,
      "grad_norm": 0.6614847779273987,
      "learning_rate": 0.00013573264781491004,
      "epoch": 1.0226415094339623,
      "step": 136
    },
    {
      "loss": 1.0766,
      "grad_norm": 0.684183657169342,
      "learning_rate": 0.00013521850899742932,
      "epoch": 1.030188679245283,
      "step": 137
    },
    {
      "loss": 1.097,
      "grad_norm": 0.7084105014801025,
      "learning_rate": 0.0001347043701799486,
      "epoch": 1.0377358490566038,
      "step": 138
    },
    {
      "loss": 1.0485,
      "grad_norm": 0.7226302027702332,
      "learning_rate": 0.00013419023136246785,
      "epoch": 1.0452830188679245,
      "step": 139
    },
    {
      "loss": 1.0072,
      "grad_norm": 0.7080039978027344,
      "learning_rate": 0.00013367609254498716,
      "epoch": 1.0528301886792453,
      "step": 140
    },
    {
      "loss": 1.0807,
      "grad_norm": 0.7536746859550476,
      "learning_rate": 0.00013316195372750644,
      "epoch": 1.060377358490566,
      "step": 141
    },
    {
      "loss": 1.1002,
      "grad_norm": 0.8588740229606628,
      "learning_rate": 0.00013264781491002572,
      "epoch": 1.0679245283018868,
      "step": 142
    },
    {
      "loss": 1.0872,
      "grad_norm": 0.8338998556137085,
      "learning_rate": 0.00013213367609254498,
      "epoch": 1.0754716981132075,
      "step": 143
    },
    {
      "loss": 1.0231,
      "grad_norm": 0.7158235311508179,
      "learning_rate": 0.00013161953727506426,
      "epoch": 1.0830188679245283,
      "step": 144
    },
    {
      "loss": 1.0624,
      "grad_norm": 0.6830800771713257,
      "learning_rate": 0.00013110539845758356,
      "epoch": 1.090566037735849,
      "step": 145
    },
    {
      "loss": 1.0583,
      "grad_norm": 0.7922537922859192,
      "learning_rate": 0.00013059125964010284,
      "epoch": 1.0981132075471698,
      "step": 146
    },
    {
      "loss": 1.0907,
      "grad_norm": 0.7549559473991394,
      "learning_rate": 0.00013007712082262213,
      "epoch": 1.1056603773584905,
      "step": 147
    },
    {
      "loss": 0.9894,
      "grad_norm": 0.675165593624115,
      "learning_rate": 0.00012956298200514138,
      "epoch": 1.1132075471698113,
      "step": 148
    },
    {
      "loss": 1.0256,
      "grad_norm": 0.7107887268066406,
      "learning_rate": 0.00012904884318766069,
      "epoch": 1.120754716981132,
      "step": 149
    },
    {
      "loss": 1.1034,
      "grad_norm": 0.763831615447998,
      "learning_rate": 0.00012853470437017997,
      "epoch": 1.1283018867924528,
      "step": 150
    },
    {
      "loss": 1.1067,
      "grad_norm": 0.8095376491546631,
      "learning_rate": 0.00012802056555269925,
      "epoch": 1.1358490566037736,
      "step": 151
    },
    {
      "loss": 1.0904,
      "grad_norm": 0.7470231652259827,
      "learning_rate": 0.0001275064267352185,
      "epoch": 1.1433962264150943,
      "step": 152
    },
    {
      "loss": 1.1019,
      "grad_norm": 0.6999379396438599,
      "learning_rate": 0.00012699228791773778,
      "epoch": 1.150943396226415,
      "step": 153
    },
    {
      "loss": 1.0862,
      "grad_norm": 0.7571876645088196,
      "learning_rate": 0.0001264781491002571,
      "epoch": 1.1584905660377358,
      "step": 154
    },
    {
      "loss": 1.0499,
      "grad_norm": 0.7600724101066589,
      "learning_rate": 0.00012596401028277637,
      "epoch": 1.1660377358490566,
      "step": 155
    },
    {
      "loss": 1.0527,
      "grad_norm": 0.709818959236145,
      "learning_rate": 0.00012544987146529562,
      "epoch": 1.1735849056603773,
      "step": 156
    },
    {
      "loss": 1.0473,
      "grad_norm": 0.7405957579612732,
      "learning_rate": 0.0001249357326478149,
      "epoch": 1.181132075471698,
      "step": 157
    },
    {
      "loss": 1.094,
      "grad_norm": 0.7389004826545715,
      "learning_rate": 0.0001244215938303342,
      "epoch": 1.1886792452830188,
      "step": 158
    },
    {
      "loss": 0.979,
      "grad_norm": 0.7285057306289673,
      "learning_rate": 0.0001239074550128535,
      "epoch": 1.1962264150943396,
      "step": 159
    },
    {
      "loss": 1.0267,
      "grad_norm": 0.7582492232322693,
      "learning_rate": 0.00012339331619537274,
      "epoch": 1.2037735849056603,
      "step": 160
    },
    {
      "loss": 1.0489,
      "grad_norm": 0.8453512787818909,
      "learning_rate": 0.00012287917737789202,
      "epoch": 1.211320754716981,
      "step": 161
    },
    {
      "loss": 1.0857,
      "grad_norm": 0.7948401570320129,
      "learning_rate": 0.00012236503856041133,
      "epoch": 1.2188679245283018,
      "step": 162
    },
    {
      "loss": 1.1036,
      "grad_norm": 0.8206292390823364,
      "learning_rate": 0.0001218508997429306,
      "epoch": 1.2264150943396226,
      "step": 163
    },
    {
      "loss": 1.0895,
      "grad_norm": 0.7974334359169006,
      "learning_rate": 0.00012133676092544988,
      "epoch": 1.2339622641509433,
      "step": 164
    },
    {
      "loss": 1.0428,
      "grad_norm": 0.8345683217048645,
      "learning_rate": 0.00012082262210796915,
      "epoch": 1.241509433962264,
      "step": 165
    },
    {
      "loss": 1.0855,
      "grad_norm": 0.7758079767227173,
      "learning_rate": 0.00012030848329048843,
      "epoch": 1.2490566037735849,
      "step": 166
    },
    {
      "loss": 1.0625,
      "grad_norm": 0.8065040111541748,
      "learning_rate": 0.00011979434447300772,
      "epoch": 1.2566037735849056,
      "step": 167
    },
    {
      "loss": 1.0901,
      "grad_norm": 0.7382706999778748,
      "learning_rate": 0.000119280205655527,
      "epoch": 1.2641509433962264,
      "step": 168
    },
    {
      "loss": 1.0151,
      "grad_norm": 0.7448904514312744,
      "learning_rate": 0.00011876606683804628,
      "epoch": 1.271698113207547,
      "step": 169
    },
    {
      "loss": 1.0136,
      "grad_norm": 0.7956605553627014,
      "learning_rate": 0.00011825192802056555,
      "epoch": 1.2792452830188679,
      "step": 170
    },
    {
      "loss": 1.0718,
      "grad_norm": 0.7731323838233948,
      "learning_rate": 0.00011773778920308484,
      "epoch": 1.2867924528301886,
      "step": 171
    },
    {
      "loss": 1.1135,
      "grad_norm": 0.8124710321426392,
      "learning_rate": 0.00011722365038560412,
      "epoch": 1.2943396226415094,
      "step": 172
    },
    {
      "loss": 0.989,
      "grad_norm": 0.7798722386360168,
      "learning_rate": 0.0001167095115681234,
      "epoch": 1.3018867924528301,
      "step": 173
    },
    {
      "loss": 1.0342,
      "grad_norm": 0.7601796388626099,
      "learning_rate": 0.00011619537275064267,
      "epoch": 1.3094339622641509,
      "step": 174
    },
    {
      "loss": 1.0691,
      "grad_norm": 0.8014139533042908,
      "learning_rate": 0.00011568123393316196,
      "epoch": 1.3169811320754716,
      "step": 175
    },
    {
      "loss": 1.0576,
      "grad_norm": 0.7523103952407837,
      "learning_rate": 0.00011516709511568124,
      "epoch": 1.3245283018867924,
      "step": 176
    },
    {
      "loss": 1.0894,
      "grad_norm": 0.8286690711975098,
      "learning_rate": 0.00011465295629820053,
      "epoch": 1.3320754716981131,
      "step": 177
    },
    {
      "loss": 1.0087,
      "grad_norm": 0.76312255859375,
      "learning_rate": 0.00011413881748071979,
      "epoch": 1.3396226415094339,
      "step": 178
    },
    {
      "loss": 1.0118,
      "grad_norm": 0.7807438373565674,
      "learning_rate": 0.00011362467866323907,
      "epoch": 1.3471698113207546,
      "step": 179
    },
    {
      "loss": 1.0214,
      "grad_norm": 0.8300367593765259,
      "learning_rate": 0.00011311053984575837,
      "epoch": 1.3547169811320754,
      "step": 180
    },
    {
      "loss": 1.0359,
      "grad_norm": 0.7834548354148865,
      "learning_rate": 0.00011259640102827765,
      "epoch": 1.3622641509433961,
      "step": 181
    },
    {
      "loss": 1.0672,
      "grad_norm": 0.8409020304679871,
      "learning_rate": 0.00011208226221079691,
      "epoch": 1.369811320754717,
      "step": 182
    },
    {
      "loss": 0.9966,
      "grad_norm": 0.8087120652198792,
      "learning_rate": 0.0001115681233933162,
      "epoch": 1.3773584905660377,
      "step": 183
    },
    {
      "loss": 1.0658,
      "grad_norm": 0.7983148097991943,
      "learning_rate": 0.00011105398457583549,
      "epoch": 1.3849056603773584,
      "step": 184
    },
    {
      "loss": 1.0707,
      "grad_norm": 0.7907383441925049,
      "learning_rate": 0.00011053984575835477,
      "epoch": 1.3924528301886792,
      "step": 185
    },
    {
      "loss": 1.0655,
      "grad_norm": 0.8047699928283691,
      "learning_rate": 0.00011002570694087404,
      "epoch": 1.4,
      "step": 186
    },
    {
      "loss": 1.0981,
      "grad_norm": 0.8645461797714233,
      "learning_rate": 0.00010951156812339332,
      "epoch": 1.4075471698113207,
      "step": 187
    },
    {
      "loss": 1.0836,
      "grad_norm": 0.8417961001396179,
      "learning_rate": 0.0001089974293059126,
      "epoch": 1.4150943396226414,
      "step": 188
    },
    {
      "loss": 1.0083,
      "grad_norm": 0.8021028637886047,
      "learning_rate": 0.00010848329048843189,
      "epoch": 1.4226415094339622,
      "step": 189
    },
    {
      "loss": 1.0683,
      "grad_norm": 0.8052886128425598,
      "learning_rate": 0.00010796915167095116,
      "epoch": 1.430188679245283,
      "step": 190
    },
    {
      "loss": 1.0131,
      "grad_norm": 0.7759863138198853,
      "learning_rate": 0.00010745501285347044,
      "epoch": 1.4377358490566037,
      "step": 191
    },
    {
      "loss": 1.0225,
      "grad_norm": 0.8019853234291077,
      "learning_rate": 0.00010694087403598972,
      "epoch": 1.4452830188679244,
      "step": 192
    },
    {
      "loss": 1.0297,
      "grad_norm": 0.8225907683372498,
      "learning_rate": 0.00010642673521850901,
      "epoch": 1.4528301886792452,
      "step": 193
    },
    {
      "loss": 1.0658,
      "grad_norm": 0.8178576827049255,
      "learning_rate": 0.00010591259640102828,
      "epoch": 1.460377358490566,
      "step": 194
    },
    {
      "loss": 1.0262,
      "grad_norm": 0.8754475116729736,
      "learning_rate": 0.00010539845758354756,
      "epoch": 1.4679245283018867,
      "step": 195
    },
    {
      "loss": 0.9998,
      "grad_norm": 0.8665801882743835,
      "learning_rate": 0.00010488431876606684,
      "epoch": 1.4754716981132074,
      "step": 196
    },
    {
      "loss": 1.0275,
      "grad_norm": 0.8318750858306885,
      "learning_rate": 0.00010437017994858613,
      "epoch": 1.4830188679245282,
      "step": 197
    },
    {
      "loss": 1.0773,
      "grad_norm": 0.8444646596908569,
      "learning_rate": 0.00010385604113110541,
      "epoch": 1.490566037735849,
      "step": 198
    },
    {
      "loss": 1.0157,
      "grad_norm": 0.8378223180770874,
      "learning_rate": 0.00010334190231362468,
      "epoch": 1.4981132075471697,
      "step": 199
    },
    {
      "loss": 1.0608,
      "grad_norm": 0.8353931307792664,
      "learning_rate": 0.00010282776349614396,
      "epoch": 1.5056603773584905,
      "step": 200
    },
    {
      "loss": 0.9978,
      "grad_norm": 0.7949056625366211,
      "learning_rate": 0.00010231362467866323,
      "epoch": 1.5132075471698112,
      "step": 201
    },
    {
      "loss": 1.0414,
      "grad_norm": 0.8003679513931274,
      "learning_rate": 0.00010179948586118254,
      "epoch": 1.520754716981132,
      "step": 202
    },
    {
      "loss": 1.0331,
      "grad_norm": 0.7982467412948608,
      "learning_rate": 0.0001012853470437018,
      "epoch": 1.5283018867924527,
      "step": 203
    },
    {
      "loss": 1.0243,
      "grad_norm": 0.7950766086578369,
      "learning_rate": 0.00010077120822622108,
      "epoch": 1.5358490566037735,
      "step": 204
    },
    {
      "loss": 1.06,
      "grad_norm": 0.8245156407356262,
      "learning_rate": 0.00010025706940874035,
      "epoch": 1.5433962264150942,
      "step": 205
    },
    {
      "loss": 0.974,
      "grad_norm": 0.7956774234771729,
      "learning_rate": 9.974293059125965e-05,
      "epoch": 1.550943396226415,
      "step": 206
    },
    {
      "loss": 1.031,
      "grad_norm": 0.8288906216621399,
      "learning_rate": 9.922879177377893e-05,
      "epoch": 1.5584905660377357,
      "step": 207
    },
    {
      "loss": 1.053,
      "grad_norm": 0.8385576009750366,
      "learning_rate": 9.87146529562982e-05,
      "epoch": 1.5660377358490565,
      "step": 208
    },
    {
      "loss": 1.0037,
      "grad_norm": 0.8331574201583862,
      "learning_rate": 9.820051413881749e-05,
      "epoch": 1.5735849056603772,
      "step": 209
    },
    {
      "loss": 1.0525,
      "grad_norm": 0.8534168004989624,
      "learning_rate": 9.768637532133677e-05,
      "epoch": 1.581132075471698,
      "step": 210
    },
    {
      "loss": 1.0416,
      "grad_norm": 0.8281441926956177,
      "learning_rate": 9.717223650385605e-05,
      "epoch": 1.5886792452830187,
      "step": 211
    },
    {
      "loss": 1.1027,
      "grad_norm": 0.8595896363258362,
      "learning_rate": 9.665809768637533e-05,
      "epoch": 1.5962264150943395,
      "step": 212
    },
    {
      "loss": 1.032,
      "grad_norm": 0.82757169008255,
      "learning_rate": 9.61439588688946e-05,
      "epoch": 1.6037735849056602,
      "step": 213
    },
    {
      "loss": 1.0794,
      "grad_norm": 0.7715693712234497,
      "learning_rate": 9.562982005141389e-05,
      "epoch": 1.611320754716981,
      "step": 214
    },
    {
      "loss": 1.0096,
      "grad_norm": 0.8234891891479492,
      "learning_rate": 9.511568123393317e-05,
      "epoch": 1.6188679245283017,
      "step": 215
    },
    {
      "loss": 1.0267,
      "grad_norm": 0.7888736128807068,
      "learning_rate": 9.460154241645245e-05,
      "epoch": 1.6264150943396225,
      "step": 216
    },
    {
      "loss": 1.06,
      "grad_norm": 0.7851306200027466,
      "learning_rate": 9.408740359897173e-05,
      "epoch": 1.6339622641509433,
      "step": 217
    },
    {
      "loss": 1.0018,
      "grad_norm": 0.8311989307403564,
      "learning_rate": 9.357326478149101e-05,
      "epoch": 1.641509433962264,
      "step": 218
    },
    {
      "loss": 1.0993,
      "grad_norm": 0.7892439961433411,
      "learning_rate": 9.305912596401029e-05,
      "epoch": 1.6490566037735848,
      "step": 219
    },
    {
      "loss": 1.052,
      "grad_norm": 0.8028188943862915,
      "learning_rate": 9.254498714652957e-05,
      "epoch": 1.6566037735849055,
      "step": 220
    },
    {
      "loss": 1.0249,
      "grad_norm": 0.8087449073791504,
      "learning_rate": 9.203084832904885e-05,
      "epoch": 1.6641509433962263,
      "step": 221
    },
    {
      "loss": 1.0303,
      "grad_norm": 0.8331591486930847,
      "learning_rate": 9.151670951156813e-05,
      "epoch": 1.671698113207547,
      "step": 222
    },
    {
      "loss": 0.9931,
      "grad_norm": 0.7665864825248718,
      "learning_rate": 9.100257069408741e-05,
      "epoch": 1.6792452830188678,
      "step": 223
    },
    {
      "loss": 1.0441,
      "grad_norm": 0.8088653087615967,
      "learning_rate": 9.048843187660668e-05,
      "epoch": 1.6867924528301885,
      "step": 224
    },
    {
      "loss": 1.03,
      "grad_norm": 0.8011401891708374,
      "learning_rate": 8.997429305912597e-05,
      "epoch": 1.6943396226415093,
      "step": 225
    },
    {
      "loss": 1.0198,
      "grad_norm": 0.7858319878578186,
      "learning_rate": 8.946015424164524e-05,
      "epoch": 1.70188679245283,
      "step": 226
    },
    {
      "loss": 1.0095,
      "grad_norm": 0.7963500618934631,
      "learning_rate": 8.894601542416453e-05,
      "epoch": 1.7094339622641508,
      "step": 227
    },
    {
      "loss": 1.0097,
      "grad_norm": 0.7987654805183411,
      "learning_rate": 8.84318766066838e-05,
      "epoch": 1.7169811320754715,
      "step": 228
    },
    {
      "loss": 1.0042,
      "grad_norm": 0.8116626143455505,
      "learning_rate": 8.79177377892031e-05,
      "epoch": 1.7245283018867923,
      "step": 229
    },
    {
      "loss": 1.0194,
      "grad_norm": 0.8195555806159973,
      "learning_rate": 8.740359897172236e-05,
      "epoch": 1.732075471698113,
      "step": 230
    },
    {
      "loss": 1.0523,
      "grad_norm": 0.8242316842079163,
      "learning_rate": 8.688946015424166e-05,
      "epoch": 1.7396226415094338,
      "step": 231
    },
    {
      "loss": 1.0529,
      "grad_norm": 0.8446791172027588,
      "learning_rate": 8.637532133676092e-05,
      "epoch": 1.7471698113207546,
      "step": 232
    },
    {
      "loss": 1.0506,
      "grad_norm": 0.8007190823554993,
      "learning_rate": 8.586118251928022e-05,
      "epoch": 1.7547169811320755,
      "step": 233
    },
    {
      "loss": 1.026,
      "grad_norm": 0.7962164878845215,
      "learning_rate": 8.534704370179948e-05,
      "epoch": 1.7622641509433963,
      "step": 234
    },
    {
      "loss": 1.0674,
      "grad_norm": 0.8534480333328247,
      "learning_rate": 8.483290488431876e-05,
      "epoch": 1.769811320754717,
      "step": 235
    },
    {
      "loss": 0.997,
      "grad_norm": 0.7711938619613647,
      "learning_rate": 8.431876606683805e-05,
      "epoch": 1.7773584905660378,
      "step": 236
    },
    {
      "loss": 1.0511,
      "grad_norm": 0.8024222254753113,
      "learning_rate": 8.380462724935733e-05,
      "epoch": 1.7849056603773585,
      "step": 237
    },
    {
      "loss": 1.0004,
      "grad_norm": 0.8355830907821655,
      "learning_rate": 8.32904884318766e-05,
      "epoch": 1.7924528301886793,
      "step": 238
    },
    {
      "loss": 1.0609,
      "grad_norm": 0.8067967295646667,
      "learning_rate": 8.277634961439589e-05,
      "epoch": 1.8,
      "step": 239
    },
    {
      "loss": 1.0717,
      "grad_norm": 0.803025484085083,
      "learning_rate": 8.226221079691517e-05,
      "epoch": 1.8075471698113208,
      "step": 240
    },
    {
      "loss": 1.0083,
      "grad_norm": 0.8218792676925659,
      "learning_rate": 8.174807197943445e-05,
      "epoch": 1.8150943396226416,
      "step": 241
    },
    {
      "loss": 1.0184,
      "grad_norm": 0.7939736247062683,
      "learning_rate": 8.123393316195373e-05,
      "epoch": 1.8226415094339623,
      "step": 242
    },
    {
      "loss": 1.0226,
      "grad_norm": 0.7921652793884277,
      "learning_rate": 8.071979434447301e-05,
      "epoch": 1.830188679245283,
      "step": 243
    },
    {
      "loss": 1.0574,
      "grad_norm": 0.8717647194862366,
      "learning_rate": 8.02056555269923e-05,
      "epoch": 1.8377358490566038,
      "step": 244
    },
    {
      "loss": 1.0366,
      "grad_norm": 0.8383222818374634,
      "learning_rate": 7.969151670951157e-05,
      "epoch": 1.8452830188679246,
      "step": 245
    },
    {
      "loss": 1.0026,
      "grad_norm": 0.7737926840782166,
      "learning_rate": 7.917737789203086e-05,
      "epoch": 1.8528301886792453,
      "step": 246
    },
    {
      "loss": 0.9892,
      "grad_norm": 0.802125096321106,
      "learning_rate": 7.866323907455013e-05,
      "epoch": 1.860377358490566,
      "step": 247
    },
    {
      "loss": 0.9657,
      "grad_norm": 0.8315160274505615,
      "learning_rate": 7.814910025706941e-05,
      "epoch": 1.8679245283018868,
      "step": 248
    },
    {
      "loss": 1.0578,
      "grad_norm": 0.8222597241401672,
      "learning_rate": 7.763496143958869e-05,
      "epoch": 1.8754716981132076,
      "step": 249
    },
    {
      "loss": 1.0516,
      "grad_norm": 0.8555480241775513,
      "learning_rate": 7.712082262210797e-05,
      "epoch": 1.8830188679245283,
      "step": 250
    },
    {
      "loss": 1.0337,
      "grad_norm": 0.8281564712524414,
      "learning_rate": 7.660668380462725e-05,
      "epoch": 1.890566037735849,
      "step": 251
    },
    {
      "loss": 1.039,
      "grad_norm": 0.8222769498825073,
      "learning_rate": 7.609254498714653e-05,
      "epoch": 1.8981132075471698,
      "step": 252
    },
    {
      "loss": 0.988,
      "grad_norm": 0.8707621097564697,
      "learning_rate": 7.557840616966581e-05,
      "epoch": 1.9056603773584906,
      "step": 253
    },
    {
      "loss": 1.0086,
      "grad_norm": 0.9525209665298462,
      "learning_rate": 7.50642673521851e-05,
      "epoch": 1.9132075471698113,
      "step": 254
    },
    {
      "loss": 1.0405,
      "grad_norm": 0.8787140846252441,
      "learning_rate": 7.455012853470437e-05,
      "epoch": 1.920754716981132,
      "step": 255
    },
    {
      "loss": 1.0529,
      "grad_norm": 0.8476085066795349,
      "learning_rate": 7.403598971722365e-05,
      "epoch": 1.9283018867924528,
      "step": 256
    },
    {
      "loss": 1.0434,
      "grad_norm": 0.8469176292419434,
      "learning_rate": 7.352185089974293e-05,
      "epoch": 1.9358490566037736,
      "step": 257
    },
    {
      "loss": 0.9867,
      "grad_norm": 0.8148093223571777,
      "learning_rate": 7.300771208226222e-05,
      "epoch": 1.9433962264150944,
      "step": 258
    },
    {
      "loss": 0.9767,
      "grad_norm": 0.827215313911438,
      "learning_rate": 7.24935732647815e-05,
      "epoch": 1.950943396226415,
      "step": 259
    },
    {
      "loss": 0.9963,
      "grad_norm": 0.8560218214988708,
      "learning_rate": 7.197943444730078e-05,
      "epoch": 1.9584905660377359,
      "step": 260
    },
    {
      "loss": 1.0293,
      "grad_norm": 0.8741632699966431,
      "learning_rate": 7.146529562982006e-05,
      "epoch": 1.9660377358490566,
      "step": 261
    },
    {
      "loss": 1.0349,
      "grad_norm": 0.8597792387008667,
      "learning_rate": 7.095115681233934e-05,
      "epoch": 1.9735849056603774,
      "step": 262
    },
    {
      "loss": 1.0454,
      "grad_norm": 0.8244342803955078,
      "learning_rate": 7.043701799485862e-05,
      "epoch": 1.9811320754716981,
      "step": 263
    },
    {
      "loss": 0.9613,
      "grad_norm": 0.815919041633606,
      "learning_rate": 6.99228791773779e-05,
      "epoch": 1.9886792452830189,
      "step": 264
    },
    {
      "loss": 0.9851,
      "grad_norm": 0.8445442318916321,
      "learning_rate": 6.940874035989718e-05,
      "epoch": 1.9962264150943396,
      "step": 265
    },
    {
      "loss": 0.9861,
      "grad_norm": 1.1340657472610474,
      "learning_rate": 6.889460154241646e-05,
      "epoch": 2.0,
      "step": 266
    },
    {
      "eval_loss": 1.1415289640426636,
      "eval_runtime": 9.9536,
      "eval_samples_per_second": 53.247,
      "eval_steps_per_second": 6.731,
      "epoch": 2.0,
      "step": 266
    },
    {
      "loss": 0.85,
      "grad_norm": 0.8253735899925232,
      "learning_rate": 6.838046272493574e-05,
      "epoch": 2.0075471698113208,
      "step": 267
    },
    {
      "loss": 0.8683,
      "grad_norm": 0.7904548645019531,
      "learning_rate": 6.786632390745502e-05,
      "epoch": 2.0150943396226415,
      "step": 268
    },
    {
      "loss": 0.8825,
      "grad_norm": 0.8012335896492004,
      "learning_rate": 6.73521850899743e-05,
      "epoch": 2.0226415094339623,
      "step": 269
    },
    {
      "loss": 0.8659,
      "grad_norm": 0.8310462832450867,
      "learning_rate": 6.683804627249358e-05,
      "epoch": 2.030188679245283,
      "step": 270
    },
    {
      "loss": 0.9069,
      "grad_norm": 0.837224543094635,
      "learning_rate": 6.632390745501286e-05,
      "epoch": 2.0377358490566038,
      "step": 271
    },
    {
      "loss": 0.8286,
      "grad_norm": 0.8746339082717896,
      "learning_rate": 6.580976863753213e-05,
      "epoch": 2.0452830188679245,
      "step": 272
    },
    {
      "loss": 0.8566,
      "grad_norm": 0.8218390345573425,
      "learning_rate": 6.529562982005142e-05,
      "epoch": 2.0528301886792453,
      "step": 273
    },
    {
      "loss": 0.8504,
      "grad_norm": 0.8893529772758484,
      "learning_rate": 6.478149100257069e-05,
      "epoch": 2.060377358490566,
      "step": 274
    },
    {
      "loss": 0.8372,
      "grad_norm": 0.9012072086334229,
      "learning_rate": 6.426735218508998e-05,
      "epoch": 2.0679245283018868,
      "step": 275
    },
    {
      "loss": 0.8473,
      "grad_norm": 0.89374178647995,
      "learning_rate": 6.375321336760925e-05,
      "epoch": 2.0754716981132075,
      "step": 276
    },
    {
      "loss": 0.8711,
      "grad_norm": 0.967983603477478,
      "learning_rate": 6.323907455012854e-05,
      "epoch": 2.0830188679245283,
      "step": 277
    },
    {
      "loss": 0.8688,
      "grad_norm": 0.9591462016105652,
      "learning_rate": 6.272493573264781e-05,
      "epoch": 2.090566037735849,
      "step": 278
    },
    {
      "loss": 0.8521,
      "grad_norm": 0.935169517993927,
      "learning_rate": 6.22107969151671e-05,
      "epoch": 2.09811320754717,
      "step": 279
    },
    {
      "loss": 0.8561,
      "grad_norm": 0.9473870396614075,
      "learning_rate": 6.169665809768637e-05,
      "epoch": 2.1056603773584905,
      "step": 280
    },
    {
      "loss": 0.8182,
      "grad_norm": 0.9760196208953857,
      "learning_rate": 6.118251928020567e-05,
      "epoch": 2.1132075471698113,
      "step": 281
    },
    {
      "loss": 0.8864,
      "grad_norm": 0.9459804892539978,
      "learning_rate": 6.066838046272494e-05,
      "epoch": 2.120754716981132,
      "step": 282
    },
    {
      "loss": 0.8713,
      "grad_norm": 0.986770749092102,
      "learning_rate": 6.015424164524421e-05,
      "epoch": 2.128301886792453,
      "step": 283
    },
    {
      "loss": 0.8644,
      "grad_norm": 0.9455491304397583,
      "learning_rate": 5.96401028277635e-05,
      "epoch": 2.1358490566037736,
      "step": 284
    },
    {
      "loss": 0.9025,
      "grad_norm": 0.944922149181366,
      "learning_rate": 5.9125964010282774e-05,
      "epoch": 2.1433962264150943,
      "step": 285
    },
    {
      "loss": 0.8878,
      "grad_norm": 0.9705381989479065,
      "learning_rate": 5.861182519280206e-05,
      "epoch": 2.150943396226415,
      "step": 286
    },
    {
      "loss": 0.8818,
      "grad_norm": 0.924077033996582,
      "learning_rate": 5.8097686375321335e-05,
      "epoch": 2.158490566037736,
      "step": 287
    },
    {
      "loss": 0.8259,
      "grad_norm": 0.9142687320709229,
      "learning_rate": 5.758354755784062e-05,
      "epoch": 2.1660377358490566,
      "step": 288
    },
    {
      "loss": 0.9175,
      "grad_norm": 0.9786717295646667,
      "learning_rate": 5.7069408740359896e-05,
      "epoch": 2.1735849056603773,
      "step": 289
    },
    {
      "loss": 0.8065,
      "grad_norm": 0.9139822721481323,
      "learning_rate": 5.655526992287918e-05,
      "epoch": 2.181132075471698,
      "step": 290
    },
    {
      "loss": 0.8814,
      "grad_norm": 0.9560850858688354,
      "learning_rate": 5.604113110539846e-05,
      "epoch": 2.188679245283019,
      "step": 291
    },
    {
      "loss": 0.8725,
      "grad_norm": 0.9670825600624084,
      "learning_rate": 5.5526992287917744e-05,
      "epoch": 2.1962264150943396,
      "step": 292
    },
    {
      "loss": 0.8832,
      "grad_norm": 0.9359447956085205,
      "learning_rate": 5.501285347043702e-05,
      "epoch": 2.2037735849056603,
      "step": 293
    },
    {
      "loss": 0.9028,
      "grad_norm": 0.944583535194397,
      "learning_rate": 5.44987146529563e-05,
      "epoch": 2.211320754716981,
      "step": 294
    },
    {
      "loss": 0.8427,
      "grad_norm": 0.993294358253479,
      "learning_rate": 5.398457583547558e-05,
      "epoch": 2.218867924528302,
      "step": 295
    },
    {
      "loss": 0.8298,
      "grad_norm": 0.9489870071411133,
      "learning_rate": 5.347043701799486e-05,
      "epoch": 2.2264150943396226,
      "step": 296
    },
    {
      "loss": 0.8615,
      "grad_norm": 0.9834899306297302,
      "learning_rate": 5.295629820051414e-05,
      "epoch": 2.2339622641509433,
      "step": 297
    },
    {
      "loss": 0.8445,
      "grad_norm": 0.9892764091491699,
      "learning_rate": 5.244215938303342e-05,
      "epoch": 2.241509433962264,
      "step": 298
    },
    {
      "loss": 0.8635,
      "grad_norm": 1.0248703956604004,
      "learning_rate": 5.192802056555271e-05,
      "epoch": 2.249056603773585,
      "step": 299
    },
    {
      "loss": 0.855,
      "grad_norm": 0.9698135256767273,
      "learning_rate": 5.141388174807198e-05,
      "epoch": 2.2566037735849056,
      "step": 300
    },
    {
      "loss": 0.858,
      "grad_norm": 1.043976068496704,
      "learning_rate": 5.089974293059127e-05,
      "epoch": 2.2641509433962264,
      "step": 301
    },
    {
      "loss": 0.8449,
      "grad_norm": 1.0048283338546753,
      "learning_rate": 5.038560411311054e-05,
      "epoch": 2.271698113207547,
      "step": 302
    },
    {
      "loss": 0.8894,
      "grad_norm": 1.0254307985305786,
      "learning_rate": 4.987146529562982e-05,
      "epoch": 2.279245283018868,
      "step": 303
    },
    {
      "loss": 0.8456,
      "grad_norm": 0.9470877051353455,
      "learning_rate": 4.93573264781491e-05,
      "epoch": 2.2867924528301886,
      "step": 304
    },
    {
      "loss": 0.8857,
      "grad_norm": 1.0086910724639893,
      "learning_rate": 4.8843187660668383e-05,
      "epoch": 2.2943396226415094,
      "step": 305
    },
    {
      "loss": 0.8201,
      "grad_norm": 0.9621313810348511,
      "learning_rate": 4.8329048843187664e-05,
      "epoch": 2.30188679245283,
      "step": 306
    },
    {
      "loss": 0.8435,
      "grad_norm": 0.9544891715049744,
      "learning_rate": 4.7814910025706944e-05,
      "epoch": 2.309433962264151,
      "step": 307
    },
    {
      "loss": 0.9146,
      "grad_norm": 1.0329046249389648,
      "learning_rate": 4.7300771208226225e-05,
      "epoch": 2.3169811320754716,
      "step": 308
    },
    {
      "loss": 0.8889,
      "grad_norm": 0.9626153707504272,
      "learning_rate": 4.6786632390745505e-05,
      "epoch": 2.3245283018867924,
      "step": 309
    },
    {
      "loss": 0.8388,
      "grad_norm": 0.9445528388023376,
      "learning_rate": 4.6272493573264786e-05,
      "epoch": 2.332075471698113,
      "step": 310
    },
    {
      "loss": 0.8728,
      "grad_norm": 0.9812017679214478,
      "learning_rate": 4.5758354755784066e-05,
      "epoch": 2.339622641509434,
      "step": 311
    },
    {
      "loss": 0.8882,
      "grad_norm": 0.9930629134178162,
      "learning_rate": 4.524421593830334e-05,
      "epoch": 2.3471698113207546,
      "step": 312
    },
    {
      "loss": 0.921,
      "grad_norm": 0.9749246835708618,
      "learning_rate": 4.473007712082262e-05,
      "epoch": 2.3547169811320754,
      "step": 313
    },
    {
      "loss": 0.8661,
      "grad_norm": 1.0091021060943604,
      "learning_rate": 4.42159383033419e-05,
      "epoch": 2.362264150943396,
      "step": 314
    },
    {
      "loss": 0.905,
      "grad_norm": 1.0550384521484375,
      "learning_rate": 4.370179948586118e-05,
      "epoch": 2.369811320754717,
      "step": 315
    },
    {
      "loss": 0.8799,
      "grad_norm": 0.9878621101379395,
      "learning_rate": 4.318766066838046e-05,
      "epoch": 2.3773584905660377,
      "step": 316
    },
    {
      "loss": 0.8523,
      "grad_norm": 0.9692268967628479,
      "learning_rate": 4.267352185089974e-05,
      "epoch": 2.3849056603773584,
      "step": 317
    },
    {
      "loss": 0.8599,
      "grad_norm": 0.9764052033424377,
      "learning_rate": 4.215938303341902e-05,
      "epoch": 2.392452830188679,
      "step": 318
    },
    {
      "loss": 0.8883,
      "grad_norm": 1.0091303586959839,
      "learning_rate": 4.16452442159383e-05,
      "epoch": 2.4,
      "step": 319
    },
    {
      "loss": 0.8953,
      "grad_norm": 0.9759610891342163,
      "learning_rate": 4.1131105398457584e-05,
      "epoch": 2.4075471698113207,
      "step": 320
    },
    {
      "loss": 0.8789,
      "grad_norm": 1.0071442127227783,
      "learning_rate": 4.0616966580976864e-05,
      "epoch": 2.4150943396226414,
      "step": 321
    },
    {
      "loss": 0.8166,
      "grad_norm": 0.9901282787322998,
      "learning_rate": 4.010282776349615e-05,
      "epoch": 2.422641509433962,
      "step": 322
    },
    {
      "loss": 0.9171,
      "grad_norm": 1.00879967212677,
      "learning_rate": 3.958868894601543e-05,
      "epoch": 2.430188679245283,
      "step": 323
    },
    {
      "loss": 0.8628,
      "grad_norm": 0.9994575381278992,
      "learning_rate": 3.9074550128534705e-05,
      "epoch": 2.4377358490566037,
      "step": 324
    },
    {
      "loss": 0.8621,
      "grad_norm": 0.9621754884719849,
      "learning_rate": 3.8560411311053986e-05,
      "epoch": 2.4452830188679244,
      "step": 325
    },
    {
      "loss": 0.8834,
      "grad_norm": 1.0105488300323486,
      "learning_rate": 3.8046272493573266e-05,
      "epoch": 2.452830188679245,
      "step": 326
    },
    {
      "loss": 0.8615,
      "grad_norm": 0.9943605661392212,
      "learning_rate": 3.753213367609255e-05,
      "epoch": 2.460377358490566,
      "step": 327
    },
    {
      "loss": 0.8517,
      "grad_norm": 0.9804201722145081,
      "learning_rate": 3.701799485861183e-05,
      "epoch": 2.4679245283018867,
      "step": 328
    },
    {
      "loss": 0.8744,
      "grad_norm": 1.0195302963256836,
      "learning_rate": 3.650385604113111e-05,
      "epoch": 2.4754716981132074,
      "step": 329
    },
    {
      "loss": 0.8719,
      "grad_norm": 1.0105246305465698,
      "learning_rate": 3.598971722365039e-05,
      "epoch": 2.483018867924528,
      "step": 330
    },
    {
      "loss": 0.8846,
      "grad_norm": 0.9926725625991821,
      "learning_rate": 3.547557840616967e-05,
      "epoch": 2.490566037735849,
      "step": 331
    },
    {
      "loss": 0.8503,
      "grad_norm": 0.9875613451004028,
      "learning_rate": 3.496143958868895e-05,
      "epoch": 2.4981132075471697,
      "step": 332
    },
    {
      "loss": 0.8793,
      "grad_norm": 1.031856894493103,
      "learning_rate": 3.444730077120823e-05,
      "epoch": 2.5056603773584905,
      "step": 333
    },
    {
      "loss": 0.8829,
      "grad_norm": 1.081925392150879,
      "learning_rate": 3.393316195372751e-05,
      "epoch": 2.513207547169811,
      "step": 334
    },
    {
      "loss": 0.8328,
      "grad_norm": 1.0079116821289062,
      "learning_rate": 3.341902313624679e-05,
      "epoch": 2.520754716981132,
      "step": 335
    },
    {
      "loss": 0.891,
      "grad_norm": 1.0031927824020386,
      "learning_rate": 3.2904884318766064e-05,
      "epoch": 2.5283018867924527,
      "step": 336
    },
    {
      "loss": 0.8519,
      "grad_norm": 0.9894348978996277,
      "learning_rate": 3.2390745501285345e-05,
      "epoch": 2.5358490566037735,
      "step": 337
    },
    {
      "loss": 0.8547,
      "grad_norm": 1.0422402620315552,
      "learning_rate": 3.1876606683804625e-05,
      "epoch": 2.543396226415094,
      "step": 338
    },
    {
      "loss": 0.8812,
      "grad_norm": 0.9974081516265869,
      "learning_rate": 3.1362467866323906e-05,
      "epoch": 2.550943396226415,
      "step": 339
    },
    {
      "loss": 0.8431,
      "grad_norm": 0.9896559119224548,
      "learning_rate": 3.0848329048843186e-05,
      "epoch": 2.5584905660377357,
      "step": 340
    },
    {
      "loss": 0.822,
      "grad_norm": 0.9622916579246521,
      "learning_rate": 3.033419023136247e-05,
      "epoch": 2.5660377358490565,
      "step": 341
    },
    {
      "loss": 0.9269,
      "grad_norm": 1.0591844320297241,
      "learning_rate": 2.982005141388175e-05,
      "epoch": 2.5735849056603772,
      "step": 342
    },
    {
      "loss": 0.8751,
      "grad_norm": 1.0914266109466553,
      "learning_rate": 2.930591259640103e-05,
      "epoch": 2.581132075471698,
      "step": 343
    },
    {
      "loss": 0.8338,
      "grad_norm": 0.9895662665367126,
      "learning_rate": 2.879177377892031e-05,
      "epoch": 2.5886792452830187,
      "step": 344
    },
    {
      "loss": 0.9231,
      "grad_norm": 1.0563912391662598,
      "learning_rate": 2.827763496143959e-05,
      "epoch": 2.5962264150943395,
      "step": 345
    },
    {
      "loss": 0.8897,
      "grad_norm": 1.0386816263198853,
      "learning_rate": 2.7763496143958872e-05,
      "epoch": 2.6037735849056602,
      "step": 346
    },
    {
      "loss": 0.9552,
      "grad_norm": 1.0607317686080933,
      "learning_rate": 2.724935732647815e-05,
      "epoch": 2.611320754716981,
      "step": 347
    },
    {
      "loss": 0.886,
      "grad_norm": 1.0407661199569702,
      "learning_rate": 2.673521850899743e-05,
      "epoch": 2.6188679245283017,
      "step": 348
    },
    {
      "loss": 0.8758,
      "grad_norm": 1.0082001686096191,
      "learning_rate": 2.622107969151671e-05,
      "epoch": 2.6264150943396225,
      "step": 349
    },
    {
      "loss": 0.8344,
      "grad_norm": 0.9723528623580933,
      "learning_rate": 2.570694087403599e-05,
      "epoch": 2.6339622641509433,
      "step": 350
    },
    {
      "loss": 0.851,
      "grad_norm": 0.9930632710456848,
      "learning_rate": 2.519280205655527e-05,
      "epoch": 2.641509433962264,
      "step": 351
    },
    {
      "loss": 0.8484,
      "grad_norm": 1.0042407512664795,
      "learning_rate": 2.467866323907455e-05,
      "epoch": 2.6490566037735848,
      "step": 352
    },
    {
      "loss": 0.8346,
      "grad_norm": 0.9726488590240479,
      "learning_rate": 2.4164524421593832e-05,
      "epoch": 2.6566037735849055,
      "step": 353
    },
    {
      "loss": 0.907,
      "grad_norm": 1.059603214263916,
      "learning_rate": 2.3650385604113112e-05,
      "epoch": 2.6641509433962263,
      "step": 354
    },
    {
      "loss": 0.8528,
      "grad_norm": 1.0525922775268555,
      "learning_rate": 2.3136246786632393e-05,
      "epoch": 2.671698113207547,
      "step": 355
    },
    {
      "loss": 0.8231,
      "grad_norm": 1.0364954471588135,
      "learning_rate": 2.262210796915167e-05,
      "epoch": 2.6792452830188678,
      "step": 356
    },
    {
      "loss": 0.8363,
      "grad_norm": 1.0151582956314087,
      "learning_rate": 2.210796915167095e-05,
      "epoch": 2.6867924528301885,
      "step": 357
    },
    {
      "loss": 0.8831,
      "grad_norm": 1.078750491142273,
      "learning_rate": 2.159383033419023e-05,
      "epoch": 2.6943396226415093,
      "step": 358
    },
    {
      "loss": 0.8366,
      "grad_norm": 0.9943016767501831,
      "learning_rate": 2.107969151670951e-05,
      "epoch": 2.70188679245283,
      "step": 359
    },
    {
      "loss": 0.8791,
      "grad_norm": 1.0108182430267334,
      "learning_rate": 2.0565552699228792e-05,
      "epoch": 2.709433962264151,
      "step": 360
    },
    {
      "loss": 0.8092,
      "grad_norm": 1.0056601762771606,
      "learning_rate": 2.0051413881748076e-05,
      "epoch": 2.7169811320754715,
      "step": 361
    },
    {
      "loss": 0.9009,
      "grad_norm": 1.0623764991760254,
      "learning_rate": 1.9537275064267353e-05,
      "epoch": 2.7245283018867923,
      "step": 362
    },
    {
      "loss": 0.8509,
      "grad_norm": 1.001773476600647,
      "learning_rate": 1.9023136246786633e-05,
      "epoch": 2.732075471698113,
      "step": 363
    },
    {
      "loss": 0.8204,
      "grad_norm": 0.9813732504844666,
      "learning_rate": 1.8508997429305914e-05,
      "epoch": 2.739622641509434,
      "step": 364
    },
    {
      "loss": 0.8435,
      "grad_norm": 1.014281988143921,
      "learning_rate": 1.7994858611825194e-05,
      "epoch": 2.7471698113207546,
      "step": 365
    },
    {
      "loss": 0.8867,
      "grad_norm": 1.0326950550079346,
      "learning_rate": 1.7480719794344475e-05,
      "epoch": 2.7547169811320753,
      "step": 366
    },
    {
      "loss": 0.8597,
      "grad_norm": 1.0372090339660645,
      "learning_rate": 1.6966580976863755e-05,
      "epoch": 2.7622641509433965,
      "step": 367
    },
    {
      "loss": 0.823,
      "grad_norm": 1.0244845151901245,
      "learning_rate": 1.6452442159383032e-05,
      "epoch": 2.769811320754717,
      "step": 368
    },
    {
      "loss": 0.8108,
      "grad_norm": 1.0236420631408691,
      "learning_rate": 1.5938303341902313e-05,
      "epoch": 2.777358490566038,
      "step": 369
    },
    {
      "loss": 0.8792,
      "grad_norm": 1.0880770683288574,
      "learning_rate": 1.5424164524421593e-05,
      "epoch": 2.7849056603773583,
      "step": 370
    },
    {
      "loss": 0.8634,
      "grad_norm": 1.0472917556762695,
      "learning_rate": 1.4910025706940875e-05,
      "epoch": 2.7924528301886795,
      "step": 371
    },
    {
      "loss": 0.8929,
      "grad_norm": 1.092628836631775,
      "learning_rate": 1.4395886889460156e-05,
      "epoch": 2.8,
      "step": 372
    },
    {
      "loss": 0.8567,
      "grad_norm": 1.0746216773986816,
      "learning_rate": 1.3881748071979436e-05,
      "epoch": 2.807547169811321,
      "step": 373
    },
    {
      "loss": 0.8585,
      "grad_norm": 1.0652761459350586,
      "learning_rate": 1.3367609254498715e-05,
      "epoch": 2.8150943396226413,
      "step": 374
    },
    {
      "loss": 0.8402,
      "grad_norm": 0.9909979701042175,
      "learning_rate": 1.2853470437017995e-05,
      "epoch": 2.8226415094339625,
      "step": 375
    },
    {
      "loss": 0.8375,
      "grad_norm": 1.0015003681182861,
      "learning_rate": 1.2339331619537276e-05,
      "epoch": 2.830188679245283,
      "step": 376
    },
    {
      "loss": 0.8351,
      "grad_norm": 0.99838787317276,
      "learning_rate": 1.1825192802056556e-05,
      "epoch": 2.837735849056604,
      "step": 377
    },
    {
      "loss": 0.8645,
      "grad_norm": 1.0588079690933228,
      "learning_rate": 1.1311053984575835e-05,
      "epoch": 2.8452830188679243,
      "step": 378
    },
    {
      "loss": 0.8477,
      "grad_norm": 1.078608751296997,
      "learning_rate": 1.0796915167095115e-05,
      "epoch": 2.8528301886792455,
      "step": 379
    },
    {
      "loss": 0.8784,
      "grad_norm": 1.008252739906311,
      "learning_rate": 1.0282776349614396e-05,
      "epoch": 2.860377358490566,
      "step": 380
    },
    {
      "loss": 0.8486,
      "grad_norm": 1.0329853296279907,
      "learning_rate": 9.768637532133676e-06,
      "epoch": 2.867924528301887,
      "step": 381
    },
    {
      "loss": 0.8249,
      "grad_norm": 1.009016990661621,
      "learning_rate": 9.254498714652957e-06,
      "epoch": 2.8754716981132074,
      "step": 382
    },
    {
      "loss": 0.8623,
      "grad_norm": 1.0312122106552124,
      "learning_rate": 8.740359897172237e-06,
      "epoch": 2.8830188679245285,
      "step": 383
    },
    {
      "loss": 0.8158,
      "grad_norm": 0.9810566902160645,
      "learning_rate": 8.226221079691516e-06,
      "epoch": 2.890566037735849,
      "step": 384
    },
    {
      "loss": 0.8189,
      "grad_norm": 1.0015496015548706,
      "learning_rate": 7.712082262210796e-06,
      "epoch": 2.89811320754717,
      "step": 385
    },
    {
      "loss": 0.8623,
      "grad_norm": 1.0589288473129272,
      "learning_rate": 7.197943444730078e-06,
      "epoch": 2.9056603773584904,
      "step": 386
    },
    {
      "loss": 0.8587,
      "grad_norm": 0.9943639039993286,
      "learning_rate": 6.683804627249357e-06,
      "epoch": 2.9132075471698116,
      "step": 387
    },
    {
      "loss": 0.8557,
      "grad_norm": 1.015722393989563,
      "learning_rate": 6.169665809768638e-06,
      "epoch": 2.920754716981132,
      "step": 388
    },
    {
      "loss": 0.9225,
      "grad_norm": 1.0557194948196411,
      "learning_rate": 5.6555269922879175e-06,
      "epoch": 2.928301886792453,
      "step": 389
    },
    {
      "loss": 0.8452,
      "grad_norm": 1.0480408668518066,
      "learning_rate": 5.141388174807198e-06,
      "epoch": 2.9358490566037734,
      "step": 390
    },
    {
      "loss": 0.8488,
      "grad_norm": 1.0437753200531006,
      "learning_rate": 4.627249357326478e-06,
      "epoch": 2.9433962264150946,
      "step": 391
    },
    {
      "loss": 0.8589,
      "grad_norm": 0.9933955073356628,
      "learning_rate": 4.113110539845758e-06,
      "epoch": 2.950943396226415,
      "step": 392
    },
    {
      "loss": 0.8659,
      "grad_norm": 1.0570255517959595,
      "learning_rate": 3.598971722365039e-06,
      "epoch": 2.958490566037736,
      "step": 393
    },
    {
      "loss": 0.909,
      "grad_norm": 1.065040946006775,
      "learning_rate": 3.084832904884319e-06,
      "epoch": 2.9660377358490564,
      "step": 394
    },
    {
      "loss": 0.8733,
      "grad_norm": 1.0171884298324585,
      "learning_rate": 2.570694087403599e-06,
      "epoch": 2.9735849056603776,
      "step": 395
    },
    {
      "loss": 0.8417,
      "grad_norm": 1.0285059213638306,
      "learning_rate": 2.056555269922879e-06,
      "epoch": 2.981132075471698,
      "step": 396
    },
    {
      "loss": 0.8641,
      "grad_norm": 1.0719542503356934,
      "learning_rate": 1.5424164524421595e-06,
      "epoch": 2.988679245283019,
      "step": 397
    },
    {
      "loss": 0.8596,
      "grad_norm": 1.0382033586502075,
      "learning_rate": 1.0282776349614395e-06,
      "epoch": 2.9962264150943394,
      "step": 398
    },
    {
      "loss": 0.8578,
      "grad_norm": 1.4669685363769531,
      "learning_rate": 5.141388174807198e-07,
      "epoch": 3.0,
      "step": 399
    },
    {
      "eval_loss": 1.1499392986297607,
      "eval_runtime": 9.9281,
      "eval_samples_per_second": 53.384,
      "eval_steps_per_second": 6.749,
      "epoch": 3.0,
      "step": 399
    },
    {
      "train_runtime": 600.0073,
      "train_samples_per_second": 21.2,
      "train_steps_per_second": 0.665,
      "total_flos": 1.2987481762824192e+17,
      "train_loss": 1.1294481911157306,
      "epoch": 3.0,
      "step": 399
    }
  ],
  "dataset_info": {
    "train_size": 4240,
    "val_size": 530,
    "total_size": 4770,
    "train_val_split": "4240/530",
    "data_dir": "./datasets/datasets_v4",
    "sample_train_example": {
      "text_length": 208,
      "keys": [
        "text",
        "labels"
      ]
    }
  },
  "model_stats": {
    "trainable_params_detail": [
      {
        "name": "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight",
        "shape": [
          16,
          4096
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight",
        "shape": [
          4096,
          16
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight",
        "shape": [
          16,
          4096
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight",
        "shape": [
          4096,
          16
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      },
      {
        "name": "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight",
        "shape": [
          16,
          4096
        ],
        "dtype": "torch.float32",
        "requires_grad": true,
        "numel": 65536
      }
    ],
    "total_trainable_params_verified": 448
  },
  "training_start_time": "2025-07-12T14:56:43.703932",
  "training_end_time": "2025-07-12T15:06:43.999281",
  "total_training_duration": "0:10:00.295349"
}